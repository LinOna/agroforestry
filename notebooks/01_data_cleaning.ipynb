{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaHcyXl1+wkUtQmRAyH7yQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinOna/agroforestry/blob/main/notebooks/01_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: parse timestamp, standardize node, add station_type**\n",
        "- Reads RawData180925.csv,\n",
        "\n",
        "- Parses timestamp into a pandas datetime without timezone (naive),\n",
        "\n",
        "- Keeps your node_id as-is and makes a lowercase copy node for grouping,\n",
        "\n",
        "- Maps station_type exactly as specified (node_176 → open_field, node_189 → reference, else agroforestry),\n",
        "\n",
        "- Writes versioned outputs (no overwrite),\n",
        "\n",
        "- Prints concise diagnostics."
      ],
      "metadata": {
        "id": "4fJIkcPtSdXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1: parse timestamp, standardize node, add station_type\n",
        "# GUARANTEES:\n",
        "#  - No row drops. Adds a stable row_id for invariants.\n",
        "#  - Timestamp fixed ONCE here; later stages should NOT re-parse free text.\n",
        "#  - Writes both a canonical ISO string and an integer ts_ns for lossless reloads.\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Config: exact column names (no autodetection), defines hard-coded names for the input file and\n",
        "# two required columns ---\n",
        "RAW_CSV_PATH  = \"RawData180925.csv\"   # adjust if your path differs\n",
        "TIMESTAMP_COL = \"timestamp\"           # REQUIRED\n",
        "NODE_ID_COL   = \"node_id\"             # REQUIRED\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\") # creates a versioned working directory (af_clean_v01/) for all putputs\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# finds the next unused file name like name, without overwriting existing files\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    \"\"\"Create <stem>_v001.csv, <stem>_v002.csv, ... without overwriting.\"\"\"\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "# --- Load raw into Pandas Dataframe (unaltered) ---\n",
        "df_raw = pd.read_csv(RAW_CSV_PATH, low_memory=False)\n",
        "\n",
        "# Invariant 0: required columns present\n",
        "missing_required = [c for c in (TIMESTAMP_COL, NODE_ID_COL) if c not in df_raw.columns]\n",
        "if missing_required:\n",
        "    raise ValueError(f\"Missing required columns: {missing_required}. \"\n",
        "                     f\"Available columns: {list(df_raw.columns)}\")\n",
        "\n",
        "# Save a small head preview (versioned, no overwrite, writes the first 20 rows of the raw data to a versioned csv\n",
        "# in af_clean_v01/, then prints the path, for quick sanity checks\n",
        "head_path = next_versioned_csv(OUT_DIR, \"stage1_head\")\n",
        "df_raw.head(20).to_csv(head_path, index=False)\n",
        "print(\"Wrote preview:\", head_path)\n",
        "\n",
        "# -Work on a copy, creates a seperate DataFrame df to perfom all subsequent transformations, keeping\n",
        "# dr_raw untouched---\n",
        "df = df_raw.copy()\n",
        "\n",
        "# ---Step 1A: Add stable row_id (for invariants across all stages) ----------\n",
        "df.insert(0, \"row_id\", np.arange(len(df), dtype=np.int64))\n",
        "\n",
        "# ---Step 1B: Parse timestamp ONCE (keep NAIVE; no timezone) ----------\n",
        "# Keep original for provenance\n",
        "df[\"timestamp_raw\"] = df[TIMESTAMP_COL].astype(str)\n",
        "\n",
        "# Parse; if you know the exact format, you could pass format=\"%Y-%m-%d %H:%M:%S\"\n",
        "# Parses the string column timestamp_raw into a pandas datetime (datetime64[ns])\n",
        "# It won’t convert or attach a UTC timezone; the result is intended to be timezone-naive.\n",
        "ts = pd.to_datetime(df[\"timestamp_raw\"], errors=\"coerce\", utc=False)\n",
        "\n",
        "# Summarizes parse success: total rows vs. how many became NaT.\n",
        "n_total = len(df)\n",
        "n_nat = int(ts.isna().sum())\n",
        "print(f\"Timestamp parsed: {n_total - n_nat}/{n_total} ({(1 - n_nat/n_total)*100:.2f}%).\")\n",
        "\n",
        "# timestamps are fixed once\n",
        "if n_nat > 0:\n",
        "    # Drop a small sample for inspection and STOP — timestamps must be fixed here.\n",
        "    miss_ts_report = next_versioned_csv(OUT_DIR, \"stage1_missing_timestamp_rows\")\n",
        "    df.loc[ts.isna()].head(200).to_csv(miss_ts_report, index=False)\n",
        "    raise ValueError(f\"{n_nat} timestamps failed to parse. Sample saved to {miss_ts_report}. \"\n",
        "                     \"Fix timestamp issues in Stage 1 before proceeding.\")\n",
        "\n",
        "# Canonicalize: make an ISO-like string and a numeric epoch (ns) column\n",
        "# Use second precision in ISO to keep files compact but keep nanoseconds in ts_ns for lossless reloads.\n",
        "df[\"timestamp_iso\"] = ts.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "df[\"ts_ns\"] = ts.view(\"int64\")  # nanoseconds since epoch (int64)\n",
        "# All downstream joins, resamples, and deduplication can safely key on ts_ns.\n",
        "# timestamp_iso is for files humans read; ts_ns is for machines\n",
        "\n",
        "\n",
        "# For convenience within the in-memory DataFrame, keep a datetime column named 'timestamp'\n",
        "# Adds a working datetime column for in-process operations (sorting, grouping, resampling)\n",
        "# without re-parsing strings each time.\n",
        "df[\"timestamp\"] = ts  # datetime64[ns], NAIVE\n",
        "\n",
        "# NOTE FOR LATER STAGES:\n",
        "# - Do NOT call pd.to_datetime on free text again.\n",
        "# - If you need datetime after reloading a CSV, reconstruct via:\n",
        "#     ts = pd.to_datetime(df[\"ts_ns\"].astype(\"int64\"))\n",
        "\n",
        "# ---- Step 1C: Standardize station ID -> 'node' (simple, explicit) ----\n",
        "df[\"node\"] = (\n",
        "    df[NODE_ID_COL]\n",
        "      .astype(\"string\") # forsinc string dtype\n",
        "      .str.strip() # trimming white space\n",
        "      .str.lower() # lower casing all\n",
        ")\n",
        "\n",
        "# ----- Step 1D: station_type mapping (explicit rules), creates new column \"station_type\" ----\n",
        "# be aware of this definitions when possibly adding more nodes\n",
        "def map_station_type(node_label) -> object:\n",
        "    if pd.isna(node_label):\n",
        "        return pd.NA\n",
        "    s = str(node_label)\n",
        "    if s == \"node_176\":\n",
        "        return \"open_field\"\n",
        "    if s == \"node_189\":\n",
        "        return \"reference\"\n",
        "    return \"agroforestry\"\n",
        "\n",
        "df[\"station_type\"] = df[\"node\"].apply(map_station_type)\n",
        "\n",
        "# ------ Step 1E: Deterministic ordering ---\n",
        "# Sorts rows first by station (node), then by time (timestamp), then by tie-breaker (row_id).\n",
        "df.sort_values([\"node\", \"timestamp\", \"row_id\"], inplace=True, ignore_index=True)\n",
        "\n",
        "# ------- Invariants & hygiene (no row loss; report duplicates, don’t drop) ----------\n",
        "# Invariant 1: row count preserved\n",
        "# Hard check that Stage 1 didn’t add/drop rows. If counts differ, the script stops here.\n",
        "assert len(df) == len(df_raw), \"Row count changed in Stage 1 — this should never happen.\"\n",
        "\n",
        "# Hygiene: check duplicate keys (node, timestamp_iso) — just report, do not alter\n",
        "dup_mask = df.duplicated(subset=[\"node\", \"timestamp_iso\"], keep=False)\n",
        "n_dups = int(dup_mask.sum())\n",
        "if n_dups > 0:\n",
        "    dup_report = next_versioned_csv(OUT_DIR, \"stage1_duplicate_node_timestamp_rows\")\n",
        "    df.loc[dup_mask, [\"node\", \"timestamp_iso\", \"row_id\"]].to_csv(dup_report, index=False)\n",
        "    print(f\"NOTE: Found {n_dups} rows sharing the same (node, timestamp_iso). \"\n",
        "          f\"Listed in: {dup_report}. No rows were dropped.\")\n",
        "\n",
        "# ---------- Save Stage 1 (versioned, no overwrite) ----------\n",
        "stage1_path = next_versioned_csv(OUT_DIR, \"stage1_parsed\")\n",
        "df.to_csv(stage1_path, index=False)\n",
        "print(\"Wrote Stage 1:\", stage1_path)\n",
        "\n",
        "# ---------- Minimal diagnostics ----------\n",
        "print(\"\\nSummary\")\n",
        "print(\"  rows:\", len(df)) # report total row count\n",
        "print(\"  unique nodes:\", sorted(df['node'].dropna().unique().tolist())) # lists the distinct node labels\n",
        "print(\"  station_type counts:\\n\", df[\"station_type\"].value_counts(dropna=False)) # counts per station_type,incl. miss.\n",
        "\n",
        "# Dataset-wide time span (authoritative), shows earliest and latest parsed timestamps\n",
        "print(\"  timestamp range:\", df[\"timestamp\"].min(), \"→\", df[\"timestamp\"].max())\n",
        "\n",
        "# Columns you’ll carry forward (documented)\n",
        "carry = [\"row_id\", \"timestamp_iso\", \"ts_ns\", \"timestamp\", \"node_id\", \"node\", \"station_type\"]\n",
        "print(\"\\nCarry-forward columns:\", carry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psJkwGt3pDK_",
        "outputId": "035eb9ca-c49d-4e3d-9f68-dbe450fc2c71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote preview: af_clean_v01/stage1_head_v001.csv\n",
            "Timestamp parsed: 483787/483787 (100.00%).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2066309695.py:78: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
            "  df[\"ts_ns\"] = ts.view(\"int64\")  # nanoseconds since epoch (int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOTE: Found 2 rows sharing the same (node, timestamp_iso). Listed in: af_clean_v01/stage1_duplicate_node_timestamp_rows_v001.csv. No rows were dropped.\n",
            "Wrote Stage 1: af_clean_v01/stage1_parsed_v001.csv\n",
            "\n",
            "Summary\n",
            "  rows: 483787\n",
            "  unique nodes: ['node_176', 'node_179', 'node_181', 'node_182', 'node_183', 'node_184', 'node_186', 'node_187', 'node_189']\n",
            "  station_type counts:\n",
            " station_type\n",
            "agroforestry    369066\n",
            "reference        69759\n",
            "open_field       44962\n",
            "Name: count, dtype: int64\n",
            "  timestamp range: 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "\n",
            "Carry-forward columns: ['row_id', 'timestamp_iso', 'ts_ns', 'timestamp', 'node_id', 'node', 'station_type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How/When to use which timestamp:\n",
        "\n",
        "ts_ns (int64, nanoseconds since epoch) → the authoritative time. Use it for joins/merges, deduplication, exact equality checks, and persistent IDs in files. It’s lossless and immune to locale/format shenanigans.\n",
        "\n",
        "timestamp (datetime64[ns], tz-naive) → the working time for analysis inside Python: sorting, rolling windows, resampling, plotting. It’s convenient and fast, but don’t rely on its string form for persistence.\n",
        "\n",
        "timestamp_iso (string, to the second) → human-readable/export only (logs, previews). Don’t analyze with it; it drops sub-second precision.\n",
        "\n",
        "Short version: compute with timestamp, key and persist with ts_ns, display with timestamp_iso."
      ],
      "metadata": {
        "id": "cuhVwNA6x6Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Soil moisture Range QC** for ground_humidity_0 and ground_humidity_1\n",
        "* Loads the latest Stage 1 file.\n",
        "* Checks required columns exist (timestamp, node_id, node, station_type, ground_humidity_0, ground_humidity_1).\n",
        "* Classifies each moisture value as: OK (within 0–100),WARN_boundary (tiny tolerance outside bounds: −0.5…0 or 100…100.5), CRIT_out_of_range (anything <−0.5 or >100.5).\n",
        "* Builds rollups (qc_sm_anycrit_*, qc_sm_anywarn_*).\n",
        "* Creates clean columns: *_clean (CRIT → NaN; WARN at bounds → clamped to 0 or 100; OK → unchanged).\n",
        "* Saves a versioned Stage 2 file and a small summary CSV."
      ],
      "metadata": {
        "id": "tNYSxPLKSk_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2: Soil moisture Range QC (no unit inference)\n",
        "# GUARANTEES:\n",
        "#  - Reads the latest Stage 1 only\n",
        "#  - Reconstructs timestamp from ts_ns (no free-text reparse)\n",
        "#  - No row loss; preserves row_id\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "\n",
        "# ---- Config (fixed column names; no autodetection), creates/ensures the working output directory ----\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Defines the canonical column names expected from Stage 1.\n",
        "# These constants are used to avoid typos and to make the schema explicit.\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\" # authoritative time key (used to rebuild timestamp).\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"      # in-memory datetime64[ns], reconstructed from ts_ns\n",
        "NODE_ID_COL   = \"node_id\"\n",
        "NODE_COL      = \"node\"\n",
        "STATION_TYPE  = \"station_type\"\n",
        "\n",
        "SM_COLS = [\"ground_humidity_0\", \"ground_humidity_1\"]  # soil moisture probes (VWC %)\n",
        "\n",
        "# Range thresholds\n",
        "LOWER_OK, UPPER_OK = 0.0, 100.0\n",
        "LOWER_TOL, UPPER_TOL = -0.5, 100.5   # tolerance band treated as WARN_boundary\n",
        "\n",
        "# versioned file name helper\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    \"\"\"Create <stem>_v001.csv, <stem>_v002.csv, ... without overwriting.\"\"\"\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "# ---- Load input: latest Stage 1 file (only) ----\n",
        "stage1_files = sorted(glob(str(OUT_DIR / \"stage1_parsed_v*.csv\"))) # lists all files matching, and sorts\n",
        "assert stage1_files, \"No Stage 1 file found (expected: af_clean_v01/stage1_parsed_v*.csv).\"\n",
        "in_path = stage1_files[-1] # picks the latest version (highest vNNN)\n",
        "print(\"Reading:\", in_path)\n",
        "\n",
        "# loads the chosen step-1 file into a new DataFrame named df\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "\n",
        "# ---- Required columns must be present (including invariants from Stage 1) ----\n",
        "required = {\n",
        "    ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_ID_COL, NODE_COL, STATION_TYPE, *SM_COLS\n",
        "}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns from Stage 1: {sorted(missing)}\")\n",
        "\n",
        "# ---- Reconstruct timestamp from ts_ns (authoritative); DO NOT parse free text ----\n",
        "# Keep an in-memory datetime column for convenience; do not overwrite ts_ns or timestamp_iso.\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "\n",
        "# ---- Invariant: no row loss relative to Stage 1 input ----\n",
        "n_in = len(df)\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id should be unique from Stage 1.\"\n",
        "# (Since we load Stage 1 directly here, this mainly asserts uniqueness.)\n",
        "\n",
        "# Ensure numeric for moisture columns (raw columns remain unchanged)\n",
        "for c in SM_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---- Helper to classify and build clean series for one probe ----\n",
        "# qc_sm_anycrit_<k>: boolean (True if that row is CRIT for this probe).\n",
        "# qc_sm_anywarn_<k>: boolean (True if that row is WARN for this probe).\n",
        "\n",
        "\n",
        "def range_qc_for_probe(df: pd.DataFrame, col: str):\n",
        "    \"\"\"\n",
        "    Builds:\n",
        "      - qc_sm_range_<k>   : 'OK' / 'WARN_boundary' / 'CRIT_out_of_range'\n",
        "      - qc_sm_anycrit_<k> : boolean\n",
        "      - qc_sm_anywarn_<k> : boolean\n",
        "      - <col>_clean       : CRIT->NaN, WARN_boundary clamped to bounds, OK unchanged\n",
        "    \"\"\"\n",
        "    # Suffix k is '0' or '1' inferred from the column name\n",
        "    if col.endswith(\"_0\"):\n",
        "        k = \"0\"\n",
        "    elif col.endswith(\"_1\"):\n",
        "        k = \"1\"\n",
        "    else:\n",
        "        k = col  # fallback\n",
        "\n",
        "    val = df[col]\n",
        "\n",
        "    # Start with all OK; NaNs remain OK here (they'll stay NaN in _clean)\n",
        "    qc = pd.Series(\"OK\", index=val.index, dtype=\"object\")\n",
        "\n",
        "    # Boundary tolerance (warn)\n",
        "    warn_low  = val.ge(LOWER_TOL) & val.lt(LOWER_OK)   # [-0.5, 0)\n",
        "    warn_high = val.gt(UPPER_OK) & val.le(UPPER_TOL)   # (100, 100.5]\n",
        "    warn_mask = warn_low | warn_high\n",
        "\n",
        "    # Critical (beyond tolerance)\n",
        "    crit_low  = val.lt(LOWER_TOL)\n",
        "    crit_high = val.gt(UPPER_TOL)\n",
        "    crit_mask = crit_low | crit_high\n",
        "\n",
        "    # Assign flags\n",
        "    qc.loc[warn_mask] = \"WARN_boundary\"\n",
        "    qc.loc[crit_mask] = \"CRIT_out_of_range\"\n",
        "\n",
        "    # Build clean series\n",
        "    clean = val.copy()\n",
        "    clean.loc[crit_mask] = np.nan           # CRIT -> NaN\n",
        "    clean.loc[warn_low]  = LOWER_OK         # WARN at low edge -> clamp\n",
        "    clean.loc[warn_high] = UPPER_OK         # WARN at high edge -> clamp\n",
        "\n",
        "    # Rollups (booleans)\n",
        "    anycrit = qc.eq(\"CRIT_out_of_range\")\n",
        "    anywarn = qc.eq(\"WARN_boundary\")\n",
        "\n",
        "    # Attach to df, creating new columns\n",
        "    df[f\"qc_sm_range_{k}\"]   = qc\n",
        "    df[f\"qc_sm_anycrit_{k}\"] = anycrit\n",
        "    df[f\"qc_sm_anywarn_{k}\"] = anywarn\n",
        "    df[f\"{col}_clean\"]       = clean\n",
        "\n",
        "    # Return some quick counts for the console\n",
        "    return {\n",
        "        \"probe\": col,\n",
        "        \"n_OK\": int(qc.eq(\"OK\").sum()),\n",
        "        \"n_WARN_boundary\": int(anywarn.sum()),\n",
        "        \"n_CRIT_out_of_range\": int(anycrit.sum()),\n",
        "        \"n_clamped_warn\": int(warn_mask.sum()),\n",
        "        \"n_masked_crit\": int(crit_mask.sum()),\n",
        "    }\n",
        "\n",
        "# ---- Apply QC to both soil moisture probes ----\n",
        "summaries = []\n",
        "for col in SM_COLS:\n",
        "    summaries.append(range_qc_for_probe(df, col))\n",
        "\n",
        "# Deterministic order\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "\n",
        "# ---- Save outputs, writes the full DataFrame with new QC labels and\n",
        "# <col>_clean columns to a versioned csv, no overwrite ----\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage2_soilmoisture_range\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# ---- Small summary table overall and per node/probe ----\n",
        "summary_df = pd.DataFrame(summaries)\n",
        "summary_path = next_versioned_csv(OUT_DIR, \"stage2_sm_range_counts_overall\")\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(\"Wrote overall counts:\", summary_path)\n",
        "\n",
        "# Per-node flag counts (useful to spot troublesome stations), produces the table\n",
        "pernode = (\n",
        "    df.melt(id_vars=[NODE_COL, TIMESTAMP_COL], value_vars=[f\"qc_sm_range_0\", f\"qc_sm_range_1\"],\n",
        "            var_name=\"which\", value_name=\"flag\")\n",
        "      .groupby([NODE_COL, \"which\", \"flag\"]).size().rename(\"n\").reset_index()\n",
        "      .sort_values([NODE_COL, \"which\", \"flag\"])\n",
        ")\n",
        "pernode_path = next_versioned_csv(OUT_DIR, \"stage2_sm_range_counts_pernode\")\n",
        "pernode.to_csv(pernode_path, index=False)\n",
        "print(\"Wrote per-node counts:\", pernode_path)\n",
        "\n",
        "# ---- Minimal diagnostics to the console, prints the table ----\n",
        "print(\"\\nSummary (overall):\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Dataset-wide time span (authoritative, from ts_ns)\n",
        "tmin = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).min()\n",
        "tmax = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).max()\n",
        "print(\"\\nTime span (dataset, from ts_ns):\", tmin, \"→\", tmax)\n",
        "\n",
        "# Invariant echo, row preservation, repeats the row-count check\n",
        "print(\"Rows in/out (should match Stage 1):\", n_in, \"→\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjpEgzehqane",
        "outputId": "377b35c0-1a63-4a65-ae37-133f5cb1c550"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: af_clean_v01/stage1_parsed_v001.csv\n",
            "Wrote: af_clean_v01/stage2_soilmoisture_range_v001.csv\n",
            "Wrote overall counts: af_clean_v01/stage2_sm_range_counts_overall_v001.csv\n",
            "Wrote per-node counts: af_clean_v01/stage2_sm_range_counts_pernode_v001.csv\n",
            "\n",
            "Summary (overall):\n",
            "            probe   n_OK  n_WARN_boundary  n_CRIT_out_of_range  n_clamped_warn  n_masked_crit\n",
            "ground_humidity_0 483787                0                    0               0              0\n",
            "ground_humidity_1 483787                0                    0               0              0\n",
            "\n",
            "Time span (dataset, from ts_ns): 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "Rows in/out (should match Stage 1): 483787 → 483787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2b — Soil moisture diagnostics (read-only)**\n",
        "* Loads the latest Stage 2 (range-QC) table and parses timestamp; works with node, ground_humidity_0/1 raw and their *_clean versions.\n",
        "* Ensures numeric types for the soil-moisture columns (raw & clean).\n",
        "* Computes median sampling interval per node (minutes) to understand cadence and to scale later duration reasoning.\n",
        "* Per node × probe (0 / 1), it summarizes:\n",
        "* Availability: row count, % missing in raw and in clean.\n",
        "* Distribution (clean): min, Q1, median, Q3, max, IQR, std.\n",
        "* Edge occupancy (clean): % of values at/near 0% and 100% VWC (saturation/near-empty behavior).\n",
        "* Step behavior (clean): median absolute step |Δ|, 95th percentile of |Δ|, and fraction of zero steps (exact repeats) as a quick flatness indicator.\n",
        "* Daily structure: for each day, median moisture → std of daily medians (overall variability) and fraction of “low-change” day-to-day steps (simple stability proxy).\n",
        "* Pair consistency within each node (probe 0 vs 1):\n",
        "* Aligns timestamps, compares *_clean series.\n",
        "* Reports median absolute difference, 95th percentile difference, fraction of large gaps (>|15%|), and Pearson correlation between the two probes.\n",
        "* Outputs only summary tables (CSV):\n",
        "* A per-node×probe diagnostic table (availability, distributions, step stats, daily stats).\n",
        "* A per-node pair table (0 vs 1 comparison metrics).\n",
        "* No masking, no flags changed: this stage is strictly observational to spot suspicious nodes/probes before applying flatline or step QC thresholds later."
      ],
      "metadata": {
        "id": "WvkwvlS2K1ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2b — Soil moisture diagnostics (read-only)\n",
        "# Identifies nodes that look suspicious BEFORE flatline/step flags.\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Inputs ----\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"     # in-memory only (reconstructed)\n",
        "NODE_COL      = \"node\"\n",
        "\n",
        "PROBES  = [\"ground_humidity_0\", \"ground_humidity_1\"]           # raw\n",
        "CLEAN   = [f\"{c}_clean\" for c in PROBES]                        # clean (after Stage 2)\n",
        "\n",
        "# ---- Locate latest Stage 2 file (range QC output) ----\n",
        "stage2_files = sorted(glob(str(OUT_DIR / \"stage2_soilmoisture_range_v*.csv\")))\n",
        "assert stage2_files, \"Run Stage 2 (range QC) first (expected: af_clean_v01/stage2_soilmoisture_range_v*.csv).\"\n",
        "in_path = stage2_files[-1]\n",
        "print(\"Reading:\", in_path)\n",
        "\n",
        "# ---- Load & reconstruct timestamp from ts_ns (authoritative) ----\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "required_cols = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL, *PROBES, *CLEAN}\n",
        "missing = required_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns in Stage 2 file: {sorted(missing)}\")\n",
        "\n",
        "# Reconstruct 'timestamp' (do NOT parse free-text)\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "\n",
        "# Echo invariants\n",
        "print(\"Rows:\", len(df), \"| Nodes:\", df[NODE_COL].nunique())\n",
        "print(\"Dataset time span:\", df[TIMESTAMP_COL].min(), \"→\", df[TIMESTAMP_COL].max())\n",
        "\n",
        "# Ensure numeric for probe columns\n",
        "for c in PROBES + CLEAN:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---- Helper: median sampling interval (minutes) per node ----\n",
        "def median_dt_minutes(s: pd.Series) -> float:\n",
        "    if s.size < 2:\n",
        "        return np.nan\n",
        "    # s is datetime64[ns]; use numpy timedeltas\n",
        "    d = (s.values[1:] - s.values[:-1]).astype(\"timedelta64[s]\").astype(float)\n",
        "    good = d[d > 0]\n",
        "    return float(np.nanmedian(good) / 60.0) if good.size else np.nan\n",
        "\n",
        "dt_per_node = (\n",
        "    df.sort_values([NODE_COL, TIMESTAMP_COL])\n",
        "      .groupby(NODE_COL, as_index=False)[TIMESTAMP_COL]\n",
        "      .apply(lambda s: median_dt_minutes(s.reset_index(drop=True)))\n",
        "      .rename(columns={TIMESTAMP_COL: \"median_dt_min\"})\n",
        ")\n",
        "\n",
        "# ---- Daily variability helper on a clean column ----\n",
        "def daily_variability_metrics(g: pd.DataFrame, clean_col: str, std_thresh: float = 0.05):\n",
        "    \"\"\"Returns daily median std and fraction of small day-to-day change.\n",
        "       std_thresh in % VWC (absolute).\"\"\"\n",
        "    s = g[[TIMESTAMP_COL, clean_col]].dropna().copy()\n",
        "    if s.empty:\n",
        "        return pd.Series({\"daily_std\": np.nan, \"frac_days_lowvar\": np.nan})\n",
        "    s[\"date\"] = s[TIMESTAMP_COL].dt.floor(\"D\")\n",
        "    per_day = s.groupby(\"date\")[clean_col].median()\n",
        "    if per_day.size < 2:\n",
        "        return pd.Series({\"daily_std\": np.nan, \"frac_days_lowvar\": np.nan})\n",
        "    daily_std = float(per_day.std())\n",
        "    # fraction of adjacent days with small change\n",
        "    daydiff = per_day.diff().abs().fillna(0)\n",
        "    frac_low = float((daydiff <= std_thresh).mean())\n",
        "    return pd.Series({\"daily_std\": daily_std, \"frac_days_lowvar\": frac_low})\n",
        "\n",
        "# ---- Per-node, per-probe summary (availability & variability on CLEAN) ----\n",
        "rows = []\n",
        "df_sorted = df.sort_values([NODE_COL, TIMESTAMP_COL])\n",
        "\n",
        "for node_id, g in df_sorted.groupby(NODE_COL, dropna=False):\n",
        "    # median dt for this node\n",
        "    med_dt = float(dt_per_node.loc[dt_per_node[NODE_COL] == node_id, \"median_dt_min\"].values[0]\n",
        "                   ) if (dt_per_node[NODE_COL] == node_id).any() else np.nan\n",
        "\n",
        "    for raw_col, clean_col in zip(PROBES, CLEAN):\n",
        "        s_raw   = g[raw_col]\n",
        "        s_clean = g[clean_col]\n",
        "\n",
        "        n = int(len(g))\n",
        "        n_nan_raw   = int(s_raw.isna().sum())\n",
        "        n_nan_clean = int(s_clean.isna().sum())\n",
        "\n",
        "        # Clean value distribution\n",
        "        v = s_clean.dropna()\n",
        "        if not v.empty:\n",
        "            q = v.quantile([0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "            v_min, v_q1, v_med, v_q3, v_max = [float(q.loc[x]) for x in [0.0, 0.25, 0.5, 0.75, 1.0]]\n",
        "            v_iqr = float(v_q3 - v_q1)\n",
        "            v_std = float(v.std())\n",
        "            edge_lo = float((v <= 0.1).mean())\n",
        "            edge_hi = float((v >= 99.9).mean())\n",
        "        else:\n",
        "            v_min=v_q1=v_med=v_q3=v_max=v_iqr=v_std=edge_lo=edge_hi=np.nan\n",
        "\n",
        "        # Absolute step stats on CLEAN (not rate, just |Δ|)\n",
        "        v_with_ts = g[[TIMESTAMP_COL]].copy()\n",
        "        v_with_ts[\"val\"] = s_clean.values\n",
        "        v_with_ts = v_with_ts.dropna().sort_values(TIMESTAMP_COL)\n",
        "        if len(v_with_ts) >= 2:\n",
        "            dv = v_with_ts[\"val\"].diff().abs().dropna()\n",
        "            d_med = float(dv.median())\n",
        "            d_p95 = float(dv.quantile(0.95))\n",
        "            d_zero_frac = float((dv == 0).mean())\n",
        "        else:\n",
        "            d_med = d_p95 = d_zero_frac = np.nan\n",
        "\n",
        "        # Daily stability metrics\n",
        "        daily = daily_variability_metrics(g, clean_col, std_thresh=0.05)\n",
        "\n",
        "        rows.append({\n",
        "            \"node\": node_id,\n",
        "            \"probe\": raw_col,\n",
        "            \"n_rows\": n,\n",
        "            \"pct_missing_raw\": round(100*n_nan_raw / n, 2) if n else np.nan,\n",
        "            \"pct_missing_clean\": round(100*n_nan_clean / n, 2) if n else np.nan,\n",
        "            \"median_dt_min\": med_dt,\n",
        "            \"min\": v_min, \"median\": v_med, \"max\": v_max,\n",
        "            \"IQR\": v_iqr, \"std\": v_std,\n",
        "            \"edge_lo_pct\": round(100*edge_lo, 2) if pd.notna(edge_lo) else np.nan,\n",
        "            \"edge_hi_pct\": round(100*edge_hi, 2) if pd.notna(edge_hi) else np.nan,\n",
        "            \"d_abs_median\": d_med, \"d_abs_p95\": d_p95, \"d_zero_frac\": d_zero_frac,\n",
        "            \"daily_std\": daily[\"daily_std\"],\n",
        "            \"frac_days_lowvar\": daily[\"frac_days_lowvar\"],\n",
        "        })\n",
        "\n",
        "summary_node_probe = pd.DataFrame(rows)\n",
        "\n",
        "# ---- Optional pair summary (0 vs 1), purely diagnostic ----\n",
        "pairs = []\n",
        "for node_id, g in df_sorted.groupby(NODE_COL, dropna=False):\n",
        "    cols_needed = [TIMESTAMP_COL, CLEAN[0], CLEAN[1]]\n",
        "    gg = g[cols_needed].dropna().sort_values(TIMESTAMP_COL)\n",
        "    if len(gg) >= 3:\n",
        "        diff = (gg[CLEAN[0]] - gg[CLEAN[1]]).abs()\n",
        "        med_diff = float(diff.median())\n",
        "        p95_diff = float(diff.quantile(0.95))\n",
        "        big_gap_frac = float((diff > 15.0).mean())  # >15% absolute gap\n",
        "        # Pearson correlation (guard against constant series)\n",
        "        x = gg[CLEAN[0]].to_numpy()\n",
        "        y = gg[CLEAN[1]].to_numpy()\n",
        "        if np.isfinite(x).any() and np.isfinite(y).any() and (np.std(x) > 0) and (np.std(y) > 0):\n",
        "            r = float(pd.Series(x).corr(pd.Series(y)))\n",
        "        else:\n",
        "            r = np.nan\n",
        "        pairs.append({\n",
        "            \"node\": node_id,\n",
        "            \"n_aligned\": int(len(gg)),\n",
        "            \"median_abs_diff\": med_diff,\n",
        "            \"p95_abs_diff\": p95_diff,\n",
        "            \"frac_diff_gt_15pct\": big_gap_frac,\n",
        "            \"corr_clean_0_vs_1\": r,\n",
        "        })\n",
        "pair_summary = pd.DataFrame(pairs)\n",
        "\n",
        "# ---- Save diagnostics (versioned, no overwrite) ----\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "nodeprobe_path = next_versioned_csv(OUT_DIR, \"stage2_sm_diagnostics_nodeprobe\")\n",
        "summary_node_probe.to_csv(nodeprobe_path, index=False)\n",
        "print(\"Wrote per-node/probe diagnostics:\", nodeprobe_path)\n",
        "\n",
        "if not pair_summary.empty:\n",
        "    pair_path = next_versioned_csv(OUT_DIR, \"stage2_sm_diagnostics_pairs\")\n",
        "    pair_summary.to_csv(pair_path, index=False)\n",
        "    print(\"Wrote pair diagnostics:\", pair_path)\n",
        "else:\n",
        "    print(\"Pair diagnostics: not enough aligned data to compute.\")\n",
        "\n",
        "# ---- Quick console overview ----\n",
        "print(\"\\nTop nodes by % missing (clean):\")\n",
        "print(summary_node_probe.sort_values(\"pct_missing_clean\", ascending=False)\n",
        "      .head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nNodes with large p95 |Δ| on clean (possible jumpiness):\")\n",
        "print(summary_node_probe.sort_values(\"d_abs_p95\", ascending=False)\n",
        "      .head(10).to_string(index=False))\n",
        "\n",
        "if not pair_summary.empty:\n",
        "    print(\"\\nPair summary (largest median_abs_diff):\")\n",
        "    print(pair_summary.sort_values(\"median_abs_diff\", ascending=False)\n",
        "          .head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdoyr68Wqoju",
        "outputId": "efc200db-259b-41f1-c22b-41a49a34ebb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: af_clean_v01/stage2_soilmoisture_range_v001.csv\n",
            "Rows: 483787 | Nodes: 9\n",
            "Dataset time span: 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "Wrote per-node/probe diagnostics: af_clean_v01/stage2_sm_diagnostics_nodeprobe_v001.csv\n",
            "Wrote pair diagnostics: af_clean_v01/stage2_sm_diagnostics_pairs_v001.csv\n",
            "\n",
            "Top nodes by % missing (clean):\n",
            "    node             probe  n_rows  pct_missing_raw  pct_missing_clean  median_dt_min  min  median   max    IQR       std  edge_lo_pct  edge_hi_pct  d_abs_median  d_abs_p95  d_zero_frac  daily_std  frac_days_lowvar\n",
            "node_176 ground_humidity_0   44962              0.0                0.0      15.483333 1.87    4.19  8.56  1.610  1.282132         0.00          0.0          0.04       0.16     0.088988   1.274833          0.527307\n",
            "node_176 ground_humidity_1   44962              0.0                0.0      15.483333 3.59    7.33 25.90  1.190  2.035530         0.00          0.0          0.04       0.16     0.127889   1.925398          0.391714\n",
            "node_179 ground_humidity_0   54279              0.0                0.0      15.466667 0.00    7.72 76.30  2.295  3.629296         1.38          0.0          0.03       0.16     0.134953   3.097135          0.449180\n",
            "node_179 ground_humidity_1   54279              0.0                0.0      15.466667 0.85    5.10 25.50  4.540  2.641954         0.00          0.0          0.03       0.12     0.129997   2.624022          0.539344\n",
            "node_181 ground_humidity_0   53799              0.0                0.0      15.800000 9.14   20.30 50.60  5.000  8.437653         0.00          0.0          0.00       0.20     0.572549   8.296266          0.286408\n",
            "node_181 ground_humidity_1   53799              0.0                0.0      15.800000 0.00   15.00 50.60 15.735 10.713372         0.01          0.0          0.03       0.20     0.387524  10.550042          0.286408\n",
            "node_182 ground_humidity_0   50427              0.0                0.0      15.783333 0.67    4.86 12.70  4.470  2.358958         0.00          0.0          0.03       0.16     0.113096   2.339817          0.526678\n",
            "node_182 ground_humidity_1   50427              0.0                0.0      15.783333 1.61    4.43 16.80  2.280  3.050749         0.00          0.0          0.03       0.16     0.178420   3.001648          0.449225\n",
            "node_183 ground_humidity_0   52832              0.0                0.0      15.800000 2.61    5.87  9.64  2.500  1.748002         0.00          0.0          0.03       0.16     0.103273   1.752903          0.543372\n",
            "node_183 ground_humidity_1   52832              0.0                0.0      15.800000 7.26   12.60 30.10  5.480  3.241677         0.00          0.0          0.01       0.16     0.472526   3.245199          0.319149\n",
            "\n",
            "Nodes with large p95 |Δ| on clean (possible jumpiness):\n",
            "    node             probe  n_rows  pct_missing_raw  pct_missing_clean  median_dt_min  min  median   max    IQR       std  edge_lo_pct  edge_hi_pct  d_abs_median  d_abs_p95  d_zero_frac  daily_std  frac_days_lowvar\n",
            "node_187 ground_humidity_0   56339              0.0                0.0      15.483333 0.78    5.15 83.30  2.510 21.318982         0.00          0.0          0.04       1.20     0.110867  20.134884          0.460317\n",
            "node_189 ground_humidity_1   69759              0.0                0.0       0.783333 2.85    5.87 27.80  2.930  3.492496         0.00          0.0          0.08       0.58     0.167164   4.106221          0.328395\n",
            "node_189 ground_humidity_0   69759              0.0                0.0       0.783333 3.67    7.33 34.30  4.290  3.055802         0.00          0.0          0.10       0.57     0.270636   3.364664          0.320988\n",
            "node_186 ground_humidity_1   52294              0.0                0.0      15.283333 0.00    5.39 22.10  2.020  2.655930        10.90          0.0          0.04       0.49     0.130801   2.537624          0.511073\n",
            "node_186 ground_humidity_0   52294              0.0                0.0      15.283333 1.87    6.09 24.80  2.630  2.208058         0.00          0.0          0.05       0.41     0.116574   2.089699          0.359455\n",
            "node_184 ground_humidity_0   49096              0.0                0.0      15.616667 4.33   17.10 48.90 15.000  7.759703         0.00          0.0          0.05       0.40     0.389714   7.764673          0.209220\n",
            "node_181 ground_humidity_1   53799              0.0                0.0      15.800000 0.00   15.00 50.60 15.735 10.713372         0.01          0.0          0.03       0.20     0.387524  10.550042          0.286408\n",
            "node_181 ground_humidity_0   53799              0.0                0.0      15.800000 9.14   20.30 50.60  5.000  8.437653         0.00          0.0          0.00       0.20     0.572549   8.296266          0.286408\n",
            "node_176 ground_humidity_1   44962              0.0                0.0      15.483333 3.59    7.33 25.90  1.190  2.035530         0.00          0.0          0.04       0.16     0.127889   1.925398          0.391714\n",
            "node_176 ground_humidity_0   44962              0.0                0.0      15.483333 1.87    4.19  8.56  1.610  1.282132         0.00          0.0          0.04       0.16     0.088988   1.274833          0.527307\n",
            "\n",
            "Pair summary (largest median_abs_diff):\n",
            "    node  n_aligned  median_abs_diff  p95_abs_diff  frac_diff_gt_15pct  corr_clean_0_vs_1\n",
            "node_184      49096            12.33         21.53            0.275053           0.021185\n",
            "node_183      52832             6.81         10.10            0.001022           0.799055\n",
            "node_181      53799             5.00         11.88            0.000613           0.892852\n",
            "node_182      50427             2.55          6.45            0.000000           0.374669\n",
            "node_176      44962             2.50          6.04            0.001157           0.418395\n",
            "node_189      69759             1.63          4.02            0.000143           0.861690\n",
            "node_186      52294             1.47          7.88            0.000650           0.028087\n",
            "node_179      54279             1.18          6.02            0.009599           0.326714\n",
            "node_187      56339             0.58         71.61            0.201938           0.615611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 — Soil moisture: known broken probes**\n",
        "- Load your latest Stage 2 output.\n",
        "\n",
        "- Add qc_sm_known_broken_0 and qc_sm_known_broken_1:\n",
        "\n",
        "- From 2025-04-04 onward:\n",
        "\n",
        "- node_184 → probe _0 is CRIT_known_broken\n",
        "- node_186 → probe _1 is CRIT_known_broken\n",
        "- node_187 → probe _0 is CRIT_known_broken\n",
        "- Elsewhere: OK\n",
        "\n",
        "- Update rollups (qc_sm_anycrit_*) by OR-ing with these new CRIT flags.\n",
        "\n",
        "- Further mask ground_humidity_*_clean to NaN where known-broken is CRIT.\n",
        "\n",
        "- Save a versioned Stage 3 file and small summary files."
      ],
      "metadata": {
        "id": "Rv6ZsCgcZdK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 3: Known-broken probes (CRIT from FAIL_START onward)\n",
        "# - Originally SM-only; now also propagates to soil temperature.\n",
        "# - Keeps filename pattern: stage3_sm_knownbroken_v*.csv\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core columns\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"\n",
        "NODE_COL      = \"node\"\n",
        "\n",
        "# Ground sensor variables\n",
        "SM0, SM1 = \"ground_humidity_0\", \"ground_humidity_1\"\n",
        "ST0, ST1 = \"ground_temp_0\", \"ground_temp_1\"\n",
        "\n",
        "SM0_CLEAN, SM1_CLEAN = f\"{SM0}_clean\", f\"{SM1}_clean\"\n",
        "ST0_CLEAN, ST1_CLEAN = f\"{ST0}_clean\", f\"{ST1}_clean\"\n",
        "\n",
        "# Rollups\n",
        "QC_SM_ANYCRIT_0, QC_SM_ANYCRIT_1 = \"qc_sm_anycrit_0\", \"qc_sm_anycrit_1\"\n",
        "QC_ST_ANYCRIT_0, QC_ST_ANYCRIT_1 = \"qc_st_anycrit_0\", \"qc_st_anycrit_1\"\n",
        "\n",
        "# Known failure start (inclusive)\n",
        "FAIL_START = pd.Timestamp(\"2025-04-04\")\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "# ---- Load latest Stage 2 output (or later if you’ve chained steps) ----\n",
        "stage2_files = sorted(glob(str(OUT_DIR / \"stage2_soilmoisture_range_v*.csv\")))\n",
        "assert stage2_files, \"Run Stage 2 first (expected: af_clean_v01/stage2_soilmoisture_range_v*.csv).\"\n",
        "in_path = stage2_files[-1]\n",
        "print(\"Reading:\", in_path)\n",
        "\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "\n",
        "# ---- Required base columns ----\n",
        "required = {\n",
        "    ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL,\n",
        "    SM0, SM1, SM0_CLEAN, SM1_CLEAN,\n",
        "    \"qc_sm_range_0\", QC_SM_ANYCRIT_0,\n",
        "    \"qc_sm_range_1\", QC_SM_ANYCRIT_1,\n",
        "    # temperature raws should be present from Stage 1; clean may not exist yet\n",
        "    ST0, ST1,\n",
        "}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns for Step 3: {sorted(missing)}\")\n",
        "\n",
        "# Authoritative timestamp\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "\n",
        "# Invariants\n",
        "n_in = len(df)\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id must be unique (from Stage 1).\"\n",
        "\n",
        "# Ensure numeric clean columns exist for temperature (pass-through from raw if absent)\n",
        "if ST0_CLEAN not in df.columns:\n",
        "    df[ST0_CLEAN] = pd.to_numeric(df[ST0], errors=\"coerce\")\n",
        "if ST1_CLEAN not in df.columns:\n",
        "    df[ST1_CLEAN] = pd.to_numeric(df[ST1], errors=\"coerce\")\n",
        "\n",
        "# Ensure ST rollups exist (bool)\n",
        "if QC_ST_ANYCRIT_0 not in df.columns:\n",
        "    df[QC_ST_ANYCRIT_0] = False\n",
        "if QC_ST_ANYCRIT_1 not in df.columns:\n",
        "    df[QC_ST_ANYCRIT_1] = False\n",
        "\n",
        "# Keep previous anycrit booleans for delta reporting\n",
        "prev_sm_anycrit_0 = df[QC_SM_ANYCRIT_0].astype(bool).copy()\n",
        "prev_sm_anycrit_1 = df[QC_SM_ANYCRIT_1].astype(bool).copy()\n",
        "prev_st_anycrit_0 = df[QC_ST_ANYCRIT_0].astype(bool).copy()\n",
        "prev_st_anycrit_1 = df[QC_ST_ANYCRIT_1].astype(bool).copy()\n",
        "\n",
        "# Initialize known-broken flags (SM + ST)\n",
        "df[\"qc_sm_known_broken_0\"] = \"OK\"\n",
        "df[\"qc_sm_known_broken_1\"] = \"OK\"\n",
        "df[\"qc_st_known_broken_0\"] = \"OK\"\n",
        "df[\"qc_st_known_broken_1\"] = \"OK\"\n",
        "\n",
        "# ---- Rules ----\n",
        "# Existing:\n",
        "mask_184_p0 = (df[NODE_COL] == \"node_184\") & (df[TIMESTAMP_COL] >= FAIL_START)  # probe 0\n",
        "mask_186_p1 = (df[NODE_COL] == \"node_186\") & (df[TIMESTAMP_COL] >= FAIL_START)  # probe 1\n",
        "# New request: node_187 probe 0 (both SM0 and ST0)\n",
        "mask_187_p0 = (df[NODE_COL] == \"node_187\") & (df[TIMESTAMP_COL] >= FAIL_START) # probe 0\n",
        "\n",
        "# Apply to SM flags\n",
        "df.loc[mask_184_p0, \"qc_sm_known_broken_0\"] = \"CRIT_known_broken\"\n",
        "df.loc[mask_186_p1, \"qc_sm_known_broken_1\"] = \"CRIT_known_broken\"\n",
        "df.loc[mask_187_p0, \"qc_sm_known_broken_0\"] = \"CRIT_known_broken\"\n",
        "\n",
        "# Apply to ST flags\n",
        "df.loc[mask_184_p0, \"qc_st_known_broken_0\"] = \"CRIT_known_broken\"\n",
        "df.loc[mask_186_p1, \"qc_st_known_broken_1\"] = \"CRIT_known_broken\"\n",
        "df.loc[mask_187_p0, \"qc_st_known_broken_0\"] = \"CRIT_known_broken\"\n",
        "\n",
        "# ---- Update rollups ----\n",
        "kb_sm0 = df[\"qc_sm_known_broken_0\"].eq(\"CRIT_known_broken\")\n",
        "kb_sm1 = df[\"qc_sm_known_broken_1\"].eq(\"CRIT_known_broken\")\n",
        "df[QC_SM_ANYCRIT_0] = prev_sm_anycrit_0 | kb_sm0\n",
        "df[QC_SM_ANYCRIT_1] = prev_sm_anycrit_1 | kb_sm1\n",
        "\n",
        "kb_st0 = df[\"qc_st_known_broken_0\"].eq(\"CRIT_known_broken\")\n",
        "kb_st1 = df[\"qc_st_known_broken_1\"].eq(\"CRIT_known_broken\")\n",
        "df[QC_ST_ANYCRIT_0] = prev_st_anycrit_0 | kb_st0\n",
        "df[QC_ST_ANYCRIT_1] = prev_st_anycrit_1 | kb_st1\n",
        "\n",
        "# ---- Mask clean series on CRIT known-broken ----\n",
        "# Soil moisture\n",
        "df.loc[kb_sm0, SM0_CLEAN] = np.nan\n",
        "df.loc[kb_sm1, SM1_CLEAN] = np.nan\n",
        "# Soil temperature\n",
        "df.loc[kb_st0, ST0_CLEAN] = np.nan\n",
        "df.loc[kb_st1, ST1_CLEAN] = np.nan\n",
        "\n",
        "# ---- Save with same Stage 3 name for downstream compatibility ----\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage3_sm_knownbroken\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# ---- Summaries ----\n",
        "summary = pd.DataFrame([\n",
        "    {\"family\":\"SM\", \"probe\":\"0\",\n",
        "     \"n_CRIT_known_broken\": int(kb_sm0.sum()),\n",
        "     \"new_anycrit_added\":   int((~prev_sm_anycrit_0 & kb_sm0).sum())},\n",
        "    {\"family\":\"SM\", \"probe\":\"1\",\n",
        "     \"n_CRIT_known_broken\": int(kb_sm1.sum()),\n",
        "     \"new_anycrit_added\":   int((~prev_sm_anycrit_1 & kb_sm1).sum())},\n",
        "    {\"family\":\"ST\", \"probe\":\"0\",\n",
        "     \"n_CRIT_known_broken\": int(kb_st0.sum()),\n",
        "     \"new_anycrit_added\":   int((~prev_st_anycrit_0 & kb_st0).sum())},\n",
        "    {\"family\":\"ST\", \"probe\":\"1\",\n",
        "     \"n_CRIT_known_broken\": int(kb_st1.sum()),\n",
        "     \"new_anycrit_added\":   int((~prev_st_anycrit_1 & kb_st1).sum())},\n",
        "])\n",
        "sum_path = next_versioned_csv(OUT_DIR, \"stage3_knownbroken_counts_overall\")\n",
        "summary.to_csv(sum_path, index=False)\n",
        "print(\"Wrote overall counts:\", sum_path)\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# Per-node breakdown (both families)\n",
        "pernode = (\n",
        "    pd.concat([\n",
        "        df[[NODE_COL, \"qc_sm_known_broken_0\"]].rename(columns={\"qc_sm_known_broken_0\":\"flag\"}).assign(family=\"SM\", probe=\"0\"),\n",
        "        df[[NODE_COL, \"qc_sm_known_broken_1\"]].rename(columns={\"qc_sm_known_broken_1\":\"flag\"}).assign(family=\"SM\", probe=\"1\"),\n",
        "        df[[NODE_COL, \"qc_st_known_broken_0\"]].rename(columns={\"qc_st_known_broken_0\":\"flag\"}).assign(family=\"ST\", probe=\"0\"),\n",
        "        df[[NODE_COL, \"qc_st_known_broken_1\"]].rename(columns={\"qc_st_known_broken_1\":\"flag\"}).assign(family=\"ST\", probe=\"1\"),\n",
        "    ], ignore_index=True)\n",
        "    .groupby([NODE_COL, \"family\", \"probe\", \"flag\"]).size().rename(\"n\").reset_index()\n",
        "    .sort_values([NODE_COL, \"family\", \"probe\", \"flag\"])\n",
        ")\n",
        "pernode_path = next_versioned_csv(OUT_DIR, \"stage3_knownbroken_counts_pernode\")\n",
        "pernode.to_csv(pernode_path, index=False)\n",
        "print(\"Wrote per-node counts:\", pernode_path)\n",
        "\n",
        "# Breadcrumbs\n",
        "tmin = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).min()\n",
        "tmax = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).max()\n",
        "print(\"\\nTime span:\", tmin, \"→\", tmax, \"| Rows:\", n_in, \"→\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRIwsB4lt5rL",
        "outputId": "7c0d284a-2be2-428d-97c5-656675245283"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: af_clean_v01/stage2_soilmoisture_range_v001.csv\n",
            "Wrote: af_clean_v01/stage3_sm_knownbroken_v001.csv\n",
            "Wrote overall counts: af_clean_v01/stage3_knownbroken_counts_overall_v001.csv\n",
            "family probe  n_CRIT_known_broken  new_anycrit_added\n",
            "    SM     0                30230              30230\n",
            "    SM     1                15620              15620\n",
            "    ST     0                30230              30230\n",
            "    ST     1                15620              15620\n",
            "Wrote per-node counts: af_clean_v01/stage3_knownbroken_counts_pernode_v001.csv\n",
            "\n",
            "Time span: 2023-10-30 12:18:43 → 2025-09-18 10:29:42 | Rows: 483787 → 483787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New columns: qc_sm_known_broken_0, qc_sm_known_broken_1 with OK or CRIT_known_broken.\n",
        "\n",
        "Updated rollups: qc_sm_anycrit_0/1 now reflect these CRITs.\n",
        "\n",
        "ground_humidity_0_clean and ground_humidity_1_clean set to NaN during the known-broken periods.\n",
        "\n",
        "Summary CSVs with how many rows were flagged per probe and per node."
      ],
      "metadata": {
        "id": "IuANNQU5Zxjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Soil moisture — flatline detection**\n",
        "* resolution-aware exact repeats only (no arbitrary rolling std),\n",
        "* time-based thresholds (WARN ≥ 24 h, CRIT ≥ 48 h),\n",
        "* per-node sampling interval used for duration,\n",
        "* **no** boosters yet (rain responsiveness / pair-consistency) — we can add them later, behind switches."
      ],
      "metadata": {
        "id": "SEgGUmitlyDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 4: Soil moisture — flatline detection (exact-repeat, resolution-aware)\n",
        "# GUARANTEES:\n",
        "#  - Reads latest Stage 3 (else Stage 2)\n",
        "#  - Reconstructs timestamp from ts_ns (no free-text parsing)\n",
        "#  - No row loss; preserves row_id\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import math\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core columns from Stage 1\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"   # in-memory datetime64[ns]\n",
        "NODE_COL      = \"node\"\n",
        "\n",
        "# Soil moisture\n",
        "SM_COLS  = [\"ground_humidity_0\", \"ground_humidity_1\"]\n",
        "SM_CLEAN = {c: f\"{c}_clean\" for c in SM_COLS}\n",
        "\n",
        "# Thresholds (time-based)\n",
        "WARN_HOURS = 24    # identical value for ≥ 24h -> WARN\n",
        "CRIT_HOURS = 48    # identical value for ≥ 48h -> CRIT\n",
        "\n",
        "# Optional boosters (keep OFF for now)\n",
        "USE_RAIN_BOOSTER = False\n",
        "RAIN_COL = \"rain_amount\"\n",
        "RAIN_EVENT_MM = 2.0\n",
        "RAIN_WINDOW_H = 6\n",
        "NO_RESPONSE_THRESHOLD = 0.5\n",
        "\n",
        "USE_PAIR_BOOSTER = False\n",
        "\n",
        "# the previous block defines what counts as a flatline, how long it must\n",
        "# persist to be WARN vs CRIT, and prepares the columns and optional heuristics\n",
        "# the stage will use\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "# ---- Load latest prior stage (prefer Stage 3, else Stage 2) ----\n",
        "cands = sorted(glob(str(OUT_DIR / \"stage3_sm_knownbroken_v*.csv\")))\n",
        "if not cands:\n",
        "    cands = sorted(glob(str(OUT_DIR / \"stage2_soilmoisture_range_v*.csv\")))\n",
        "assert cands, \"Run Stage 2 (and Stage 3 if available) first.\"\n",
        "in_path = cands[-1]\n",
        "print(\"Reading:\", in_path)\n",
        "\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "\n",
        "# ---- Required columns must exist ----\n",
        "required = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL, *SM_COLS, *SM_CLEAN.values()}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
        "\n",
        "# ---- Reconstruct timestamp from ts_ns (authoritative) ----\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "\n",
        "# Invariants (echo)\n",
        "n_in = len(df)\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id should be unique from Stage 1.\"\n",
        "\n",
        "# Ensure numeric for moisture columns (raw; clean can be NaN already)\n",
        "for c in SM_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---- Helpers ----\n",
        "def infer_resolution(x: pd.Series) -> float:\n",
        "    \"\"\"Infer numeric resolution from unique deltas; clamp to [1e-3, 1.0].\"\"\"\n",
        "    vals = np.sort(x.dropna().unique())\n",
        "    if len(vals) < 2:\n",
        "        return 0.01\n",
        "    diffs = np.diff(vals)\n",
        "    pos = diffs[diffs > 0]\n",
        "    res = float(np.nanmin(pos)) if len(pos) else 0.01\n",
        "    return max(1e-3, min(res, 1.0))\n",
        "\n",
        "def decimals_from_resolution(res: float) -> int:\n",
        "    if res <= 0:\n",
        "        return 2\n",
        "    dec = max(0, int(round(-np.log10(res))))\n",
        "    return min(dec, 6)\n",
        "\n",
        "def median_sampling_minutes(ts: pd.Series) -> float:\n",
        "    if ts.size < 2:\n",
        "        return np.nan\n",
        "    d = (ts.values[1:] - ts.values[:-1]).astype(\"timedelta64[s]\").astype(float)\n",
        "    med = np.nanmedian(d) if d.size else np.nan\n",
        "    if not (med > 0):\n",
        "        return np.nan\n",
        "    return float(med) / 60.0\n",
        "\n",
        "def flag_flatlines_for_probe(g: pd.DataFrame, col: str) -> pd.Series:\n",
        "    \"\"\"Return per-row 'OK' / 'WARN_flatline' / 'CRIT_flatline' for one node+probe group.\"\"\"\n",
        "    # Estimate sampling interval (per node)\n",
        "    dt_min = median_sampling_minutes(g[TIMESTAMP_COL].reset_index(drop=True))\n",
        "    if not (dt_min > 0):\n",
        "        dt_min = 10.0  # fallback\n",
        "\n",
        "    warn_needed = max(2, int(math.ceil((WARN_HOURS * 60.0) / dt_min)))\n",
        "    crit_needed = max(2, int(math.ceil((CRIT_HOURS * 60.0) / dt_min)))\n",
        "\n",
        "    # Infer resolution and round series to that grid\n",
        "    res = infer_resolution(g[col])\n",
        "    dec = decimals_from_resolution(res)\n",
        "    vals = g[col].to_numpy(dtype=float)\n",
        "    v_rounded = np.round(vals / res) * res\n",
        "    v_rounded = np.round(v_rounded, dec)  # tidy FP noise\n",
        "\n",
        "    # Label runs where the rounded value is identical\n",
        "    same_as_prev = np.r_[True, v_rounded[1:] == v_rounded[:-1]]  # first row starts a run\n",
        "    run_id = np.cumsum(~same_as_prev)  # increment when value changes\n",
        "\n",
        "    # Run lengths in samples\n",
        "    counts = np.bincount(run_id)\n",
        "    run_len = counts[run_id]\n",
        "\n",
        "    # Classify\n",
        "    flag = np.full(len(g), \"OK\", dtype=object)\n",
        "    warn_mask = (run_len >= warn_needed) & (run_len < crit_needed)\n",
        "    crit_mask = (run_len >= crit_needed)\n",
        "    flag[warn_mask] = \"WARN_flatline\"\n",
        "    flag[crit_mask] = \"CRIT_flatline\"\n",
        "\n",
        "    return pd.Series(flag, index=g.index, name=f\"qc_sm_flatline_{'0' if col.endswith('_0') else '1'}\")\n",
        "\n",
        "# ---- Apply flatline detection per node for each probe ----\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True)\n",
        "for col in SM_COLS:\n",
        "    flag_name = f\"qc_sm_flatline_{'0' if col.endswith('_0') else '1'}\"\n",
        "    df[flag_name] = \"OK\"\n",
        "\n",
        "    # group by node; pass only needed columns to avoid deprecation warnings\n",
        "    flags = (\n",
        "        df.groupby(NODE_COL, group_keys=False)[[TIMESTAMP_COL, col]]\n",
        "          .apply(lambda g: flag_flatlines_for_probe(g, col))\n",
        "    )\n",
        "    df.loc[flags.index, flag_name] = flags.values\n",
        "\n",
        "# ---- Update rollups and clean series ----\n",
        "for col in SM_COLS:\n",
        "    suffix      = \"0\" if col.endswith(\"_0\") else \"1\"\n",
        "    flat_col    = f\"qc_sm_flatline_{suffix}\"\n",
        "    anycrit_col = f\"qc_sm_anycrit_{suffix}\"\n",
        "    anywarn_col = f\"qc_sm_anywarn_{suffix}\"\n",
        "    clean_col   = SM_CLEAN[col]\n",
        "\n",
        "    if anycrit_col not in df.columns:\n",
        "        df[anycrit_col] = False\n",
        "    if anywarn_col not in df.columns:\n",
        "        df[anywarn_col] = False\n",
        "\n",
        "    prev_anycrit = df[anycrit_col].astype(bool)\n",
        "    prev_anywarn = df[anywarn_col].astype(bool)\n",
        "\n",
        "    new_crit = df[flat_col].eq(\"CRIT_flatline\")\n",
        "    new_warn = df[flat_col].eq(\"WARN_flatline\")\n",
        "\n",
        "    updated_anycrit = prev_anycrit | new_crit\n",
        "    updated_anywarn = (prev_anywarn | new_warn) & (~updated_anycrit)\n",
        "\n",
        "    df[anycrit_col] = updated_anycrit\n",
        "    df[anywarn_col] = updated_anywarn\n",
        "\n",
        "    # Mask CRIT flatlines in the clean series (WARN stays visible)\n",
        "    df.loc[new_crit, clean_col] = np.nan\n",
        "\n",
        "# ---- Optional boosters (OFF) ----\n",
        "if USE_RAIN_BOOSTER:\n",
        "    print(\"RAIN booster placeholder (OFF).\")\n",
        "if USE_PAIR_BOOSTER:\n",
        "    print(\"PAIR booster placeholder (OFF).\")\n",
        "\n",
        "# ---- Deterministic ordering & save ----\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage4_sm_flatline\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# ---- Summaries ----\n",
        "def summarize_flags(df, suffix):\n",
        "    col = f\"qc_sm_flatline_{suffix}\"\n",
        "    return (df[col].value_counts(dropna=False)\n",
        "              .rename_axis(\"flag\").reset_index(name=f\"n_rows_{suffix}\"))\n",
        "\n",
        "sum0 = summarize_flags(df, \"0\")\n",
        "sum1 = summarize_flags(df, \"1\")\n",
        "summary = sum0.merge(sum1, on=\"flag\", how=\"outer\").fillna(0).sort_values(\"flag\")\n",
        "sum_path = next_versioned_csv(OUT_DIR, \"stage4_sm_flatline_counts_overall\")\n",
        "summary.to_csv(sum_path, index=False)\n",
        "print(\"Wrote overall counts:\", sum_path)\n",
        "print(\"\\nOverall flatline counts:\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# Per-node duration (approx) of CRIT/WARN (in hours), using median dt per node\n",
        "def pernode_duration_estimate(df, probe_col, flat_col):\n",
        "    med_dt = (df.groupby(NODE_COL)[TIMESTAMP_COL]\n",
        "                .apply(lambda s: median_sampling_minutes(s.sort_values()))\n",
        "                .rename(\"dt_min\"))\n",
        "    tmp = df[[NODE_COL, flat_col]].copy()\n",
        "    tmp[\"n\"] = 1\n",
        "    tmp = tmp.groupby([NODE_COL, flat_col])[\"n\"].sum().reset_index()\n",
        "    tmp = tmp.merge(med_dt, on=NODE_COL, how=\"left\")\n",
        "    tmp[\"hours\"] = (tmp[\"n\"] * tmp[\"dt_min\"]) / 60.0\n",
        "    tmp[\"probe\"] = probe_col\n",
        "    return tmp[[NODE_COL, \"probe\", flat_col, \"n\", \"hours\"]]\n",
        "\n",
        "pernode0 = pernode_duration_estimate(df, \"probe0\", \"qc_sm_flatline_0\")\n",
        "pernode1 = pernode_duration_estimate(df, \"probe1\", \"qc_sm_flatline_1\")\n",
        "pernode = pd.concat([pernode0, pernode1], ignore_index=True)\n",
        "pernode_path = next_versioned_csv(OUT_DIR, \"stage4_sm_flatline_counts_pernode\")\n",
        "pernode.to_csv(pernode_path, index=False)\n",
        "print(\"Wrote per-node duration estimates:\", pernode_path)\n",
        "\n",
        "# Minimal console diagnostics\n",
        "tmin = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).min()\n",
        "tmax = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).max()\n",
        "print(\"\\nTime span (dataset, from ts_ns):\", tmin, \"→\", tmax)\n",
        "print(\"Rows in/out (should match input):\", n_in, \"→\", len(df))\n",
        "print(\"Nodes:\", sorted(df[NODE_COL].dropna().unique().tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GdnRuinsknQ",
        "outputId": "01485a51-736a-44dc-c336-a8fa16c955d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: af_clean_v01/stage3_sm_knownbroken_v001.csv\n",
            "Wrote: af_clean_v01/stage4_sm_flatline_v001.csv\n",
            "Wrote overall counts: af_clean_v01/stage4_sm_flatline_counts_overall_v001.csv\n",
            "\n",
            "Overall flatline counts:\n",
            "         flag  n_rows_0  n_rows_1\n",
            "           OK    483690  483787.0\n",
            "WARN_flatline        97       0.0\n",
            "Wrote per-node duration estimates: af_clean_v01/stage4_sm_flatline_counts_pernode_v001.csv\n",
            "\n",
            "Time span (dataset, from ts_ns): 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "Rows in/out (should match input): 483787 → 483787\n",
            "Nodes: ['node_176', 'node_179', 'node_181', 'node_182', 'node_183', 'node_184', 'node_186', 'node_187', 'node_189']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Soil mositure**\n",
        "\n",
        "* Reads the latest ≤Stage 4 input (never Step-5 output) → idempotent reruns.\n",
        "* Parses time, coerces soil-moisture probes to numeric, sorts by node & time.\n",
        "* Flags long ramps (monotonic rise/fall after median smoothing) as WARN; no masking.\n",
        "* Flags prolonged high-volatility runaway segments (rolling STD) as CRIT candidates.\n",
        "* Applies masks conservatively: keep earlier CRIT (range/known-broken/flatline); mask runaway only for (node_187, probe 0) and force-mask after 2025-04-01.\n",
        "* Rebuilds fresh *_clean series from raw, then re-applies only the chosen CRIT masks.\n",
        "* Writes a versioned CSV in af_clean_v01/ (no overwrite).\n",
        "* Prints concise diagnostics (OK/WARN/CRIT counts per probe)."
      ],
      "metadata": {
        "id": "_63nrA1i3MYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 5: Soil moisture — conservative regime QC (ramp/runaway)\n",
        "# Idempotent: rebuilds *_clean from raw and re-applies ONLY chosen CRIT masks\n",
        "# =========================\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ROW_ID_COL, TS_NS_COL, TS_ISO_COL = \"row_id\", \"ts_ns\", \"timestamp_iso\"\n",
        "TIMESTAMP_COL, NODE_COL = \"timestamp\", \"node\"\n",
        "SM_COLS = [\"ground_humidity_0\", \"ground_humidity_1\"]\n",
        "SM_CLEAN = {c: f\"{c}_clean\" for c in SM_COLS}\n",
        "\n",
        "# ---- tuning (conservative) ----\n",
        "RAMP_MIN_AMP_VWC    = 10.0   # WARN only\n",
        "RAMP_MIN_HOURS      = 24.0\n",
        "RAMP_MIN_SIGN_SHARE = 0.85\n",
        "\n",
        "RUNAWAY_STD_WIN_H   = 12.0   # CRIT (eligible for masking)\n",
        "RUNAWAY_STD_MIN_VWC = 5.0\n",
        "RUNAWAY_MIN_HOURS   = 24.0\n",
        "\n",
        "SMOOTH_WIN_SAMPLES  = 5\n",
        "MIN_DT_MINUTES      = 5.0\n",
        "\n",
        "# mask policy: only mask runaway for (node_187, probe 0)\n",
        "ALLOW_AUTOMASK = {(\"node_187\",\"0\"): True}\n",
        "FORCE_MASK_AFTER = {(\"node_187\",\"0\"): pd.Timestamp(\"2025-04-01\")}\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists(): return p\n",
        "        i += 1\n",
        "\n",
        "def pick_latest_by_priority(patterns):\n",
        "    for pat in patterns:\n",
        "        files = sorted(glob(str(OUT_DIR / pat)))\n",
        "        if files: return files[-1]\n",
        "    return None\n",
        "\n",
        "# ---- read strictly from ≤ Stage 4 to avoid reusing Stage 5 outputs ----\n",
        "in_path = pick_latest_by_priority([\n",
        "    \"stage4_sm_flatline_v*.csv\",\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage2_soilmoisture_range_v*.csv\",\n",
        "    \"stage1_parsed_v*.csv\",\n",
        "])\n",
        "assert in_path is not None, \"No Stage 1–4 file found.\"\n",
        "print(\"Reading:\", in_path)\n",
        "\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "n_in = len(df)\n",
        "\n",
        "# basics\n",
        "req = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL, *SM_COLS}\n",
        "missing = req - set(df.columns)\n",
        "if missing: raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
        "\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id must be unique.\"\n",
        "for c in SM_COLS: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def median_sampling_minutes(ts_np: np.ndarray) -> float:\n",
        "    if ts_np.size < 2: return np.nan\n",
        "    d = (ts_np[1:] - ts_np[:-1]).astype(\"timedelta64[s]\").astype(float)\n",
        "    med = np.nanmedian(d) if d.size else np.nan\n",
        "    return float(med)/60.0 if med and med > 0 else np.nan\n",
        "\n",
        "def smooth_median(x: np.ndarray, win: int) -> np.ndarray:\n",
        "    if not (win >= 3 and win % 2 == 1): return x.astype(float)\n",
        "    n = x.size; pad = win//2\n",
        "    xp = np.pad(x.astype(float), (pad, pad), mode=\"edge\")\n",
        "    out = np.empty(n, dtype=float)\n",
        "    for i in range(n):\n",
        "        w = xp[i:i+win]; w = w[np.isfinite(w)]\n",
        "        out[i] = np.nan if w.size == 0 else np.median(w)\n",
        "    return out\n",
        "\n",
        "def runs_from_bool(mask: np.ndarray):\n",
        "    if mask.size == 0: return []\n",
        "    edges = np.diff(np.r_[False, mask.astype(bool), False])\n",
        "    s = np.where(edges == 1)[0]; e = np.where(edges == -1)[0] - 1\n",
        "    return list(zip(s, e))\n",
        "\n",
        "def detect_ramp_flags_group(x: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "    n = x.size\n",
        "    if n < 3: return np.array([\"OK\"]*n, dtype=object)\n",
        "    order = np.argsort(t); rev = np.empty_like(order); rev[order] = np.arange(n)\n",
        "    xs = smooth_median(x[order].astype(float), SMOOTH_WIN_SAMPLES)\n",
        "    ts = t[order]\n",
        "    dt_min = (ts[1:] - ts[:-1]).astype(\"timedelta64[s]\").astype(float)/60.0\n",
        "    ok_dt  = np.r_[False, dt_min >= MIN_DT_MINUTES]\n",
        "    dif    = np.full(n, np.nan); dif[1:][ok_dt[1:]] = xs[1:][ok_dt[1:]] - xs[:-1][ok_dt[1:]]\n",
        "    mono   = np.where(np.abs(dif) < 1e-6, 0.0, np.sign(dif))\n",
        "    flags_sorted = np.array([\"OK\"]*n, dtype=object)\n",
        "    change = np.r_[True, mono[1:] != mono[:-1]] | (mono == 0)\n",
        "    run_id = np.cumsum(change)\n",
        "    for rid in np.unique(run_id[mono != 0]):\n",
        "        mask = (run_id == rid) & (mono != 0)\n",
        "        idx = np.where(mask)[0]\n",
        "        if idx.size == 0: continue\n",
        "        i0, i1 = idx.min(), idx.max()\n",
        "        dur_min = np.nansum(dt_min[i0:i1]) if i1 > i0 else 0.0\n",
        "        amp = abs(xs[i1] - xs[i0]) if np.isfinite(xs[i0]) and np.isfinite(xs[i1]) else np.nan\n",
        "        nz = np.isfinite(dif[i0:i1+1]) & (np.abs(dif[i0:i1+1]) >= 1e-6)\n",
        "        if nz.any():\n",
        "            sign_share = np.mean(np.sign(dif[i0:i1+1][nz]) == np.sign(np.nansum(dif[i0:i1+1][nz])))\n",
        "        else:\n",
        "            sign_share = 0.0\n",
        "        if (dur_min >= RAMP_MIN_HOURS*60.0) and (amp >= RAMP_MIN_AMP_VWC) and (sign_share >= RAMP_MIN_SIGN_SHARE):\n",
        "            flags_sorted[i0:i1+1] = \"WARN_ramp\"\n",
        "    return flags_sorted[rev]\n",
        "\n",
        "def detect_runaway_flags_group(x: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "    n = x.size\n",
        "    if n < 3: return np.array([\"OK\"]*n, dtype=object)\n",
        "    order = np.argsort(t); rev = np.empty_like(order); rev[order] = np.arange(n)\n",
        "    xs = smooth_median(x[order].astype(float), SMOOTH_WIN_SAMPLES)\n",
        "    ts = t[order]\n",
        "    dt = median_sampling_minutes(ts); dt = 10.0 if not (dt > 0) else dt\n",
        "    win = max(5, int(round((RUNAWAY_STD_WIN_H*60.0)/dt)))\n",
        "    rs = pd.Series(xs).rolling(win, min_periods=win//2).std().to_numpy()\n",
        "    high_std = np.isfinite(rs) & (rs >= RUNAWAY_STD_MIN_VWC)\n",
        "    flags_sorted = np.array([\"OK\"]*n, dtype=object)\n",
        "    for s, e in runs_from_bool(high_std):\n",
        "        dur_h = (ts[e] - ts[s]).astype(\"timedelta64[s]\").astype(float) / 3600.0\n",
        "        if dur_h >= RUNAWAY_MIN_HOURS:\n",
        "            flags_sorted[s:e+1] = \"CRIT_runaway\"\n",
        "    return flags_sorted[rev]\n",
        "\n",
        "# ---------- initialize outputs ----------\n",
        "for k in (\"0\",\"1\"):\n",
        "    df[f\"qc_sm_regime_{k}\"] = \"OK\"\n",
        "    df[f\"qc_sm_anycrit_{k}\"] = False\n",
        "    df[f\"qc_sm_anywarn_{k}\"] = False\n",
        "\n",
        "# ---------- per-node loop (no groupby.apply) ----------\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True)\n",
        "for col in SM_COLS:\n",
        "    k = \"0\" if col.endswith(\"_0\") else \"1\"\n",
        "    regime_all = np.full(len(df), \"OK\", dtype=object)\n",
        "    for node, g in df[[NODE_COL, TIMESTAMP_COL, col]].groupby(NODE_COL, sort=False):\n",
        "        idx = g.index.values\n",
        "        x = g[col].to_numpy(dtype=float); t = g[TIMESTAMP_COL].to_numpy()\n",
        "        good = np.isfinite(x)\n",
        "        if not good.any(): continue\n",
        "        warn = detect_ramp_flags_group(x[good], t[good])\n",
        "        crit = detect_runaway_flags_group(x[good], t[good])\n",
        "        combined = np.where(crit == \"CRIT_runaway\", \"CRIT_runaway\",\n",
        "                    np.where(warn == \"WARN_ramp\", \"WARN_ramp\", \"OK\"))\n",
        "        regime_all[idx[good]] = combined\n",
        "    df[f\"qc_sm_regime_{k}\"] = regime_all\n",
        "\n",
        "# explicit force window(s)\n",
        "for (node_id, k), tcut in FORCE_MASK_AFTER.items():\n",
        "    ix = (df[NODE_COL] == node_id) & (df[TIMESTAMP_COL] >= pd.Timestamp(tcut))\n",
        "    df.loc[ix, f\"qc_sm_regime_{k}\"] = \"CRIT_runaway\"\n",
        "\n",
        "# ---------- REBUILD *_clean FROM RAW and re-apply ONLY desired CRIT masks ----------\n",
        "for col in SM_COLS:\n",
        "    k = \"0\" if col.endswith(\"_0\") else \"1\"\n",
        "    clean = df[col].astype(float).copy()\n",
        "\n",
        "    def is_crit(colname: str, value_prefix=\"CRIT\"):\n",
        "        return df[colname].astype(str).str.startswith(value_prefix) if colname in df.columns else pd.Series(False, index=df.index)\n",
        "\n",
        "    # keep Stage 2/3/4 CRIT masks\n",
        "    crit_range = is_crit(f\"qc_sm_range_{k}\")                 # CRIT_out_of_range\n",
        "    crit_kb    = df.get(f\"qc_sm_known_broken_{k}\", pd.Series(\"OK\", index=df.index)).eq(\"CRIT_known_broken\")\n",
        "    crit_flat  = is_crit(f\"qc_sm_flatline_{k}\")              # CRIT_flatline\n",
        "\n",
        "    # Step 5R runaway (but mask only for allowed node/probe)\n",
        "    crit_run = df[f\"qc_sm_regime_{k}\"].eq(\"CRIT_runaway\")\n",
        "    may_mask = df[NODE_COL].map(lambda n: ALLOW_AUTOMASK.get((n, k), False)).fillna(False)\n",
        "    crit_run = crit_run & may_mask\n",
        "\n",
        "    mask_any = crit_range | crit_kb | crit_flat | crit_run\n",
        "    clean[mask_any] = np.nan\n",
        "    df[SM_CLEAN[col]] = clean\n",
        "\n",
        "    # rollups (WARN from regime only; others are CRIT-only stages here)\n",
        "    df[f\"qc_sm_anycrit_{k}\"] = mask_any\n",
        "    df[f\"qc_sm_anywarn_{k}\"] = (df[f\"qc_sm_regime_{k}\"] == \"WARN_ramp\") & (~df[f\"qc_sm_anycrit_{k}\"])\n",
        "\n",
        "# ---------- save ----------\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage5r_sm_regimes\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# quick summary\n",
        "for k in (\"0\",\"1\"):\n",
        "    s = df[f\"qc_sm_regime_{k}\"].value_counts(dropna=False)\n",
        "    print(f\"Probe {k} regimes:\", dict(s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aM7t33HM13Hn",
        "outputId": "d40a528d-9047-4fd9-96ee-0a660ca62b93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "No Stage 1–4 file found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1114897438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m\"stage1_parsed_v*.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m ])\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0min_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No Stage 1–4 file found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: No Stage 1–4 file found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following neighbors_manual defines who is neighbouring node (in the physical setting of the climate stations), to be able to use the neighbouring node as comparison for sanity checks."
      ],
      "metadata": {
        "id": "ZQ-jop5J2wh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write neighbors_manual.csv from your mapping (run once)\n",
        "import pandas as pd\n",
        "\n",
        "rows = [\n",
        "    # node_id, sm0 list,                         sm1 list\n",
        "    (\"node_176\", [\"node_176_1\"],                 [\"node_176_0\"]),\n",
        "    (\"node_182\", [\"node_182_1\",\"node_187_0\",\"node_187_1\"], [\"node_182_0\",\"node_187_0\",\"node_187_1\"]),\n",
        "    (\"node_187\", [\"node_187_1\",\"node_182_1\",\"node_182_0\"], [\"node_187_0\",\"node_179_0\",\"node_179_1\"]),\n",
        "    (\"node_179\", [\"node_179_1\",\"node_187_1\",\"node_187_0\"], [\"node_179_0\",\"node_187_1\",\"node_187_0\"]),  # fixed 187_187_0→187_0\n",
        "    (\"node_183\", [\"node_183_1\",\"node_179_1\",\"node_181_0\"], [\"node_183_0\",\"node_181_0\",\"node_181_1\"]),\n",
        "    (\"node_181\", [\"node_181_1\",\"node_186_0\",\"node_186_1\"], [\"node_181_0\",\"node_186_0\",\"node_186_1\"]),\n",
        "    (\"node_186\", [\"node_186_1\",\"node_181_1\",\"node_181_0\"], []),  # no sm1 list provided\n",
        "    (\"node_184\", [\"node_184_1\",\"node_186_1\",\"node_186_0\"], []),  # no sm1 list provided\n",
        "    (\"node_189\", [\"node_189_1\"],                 [\"node_189_0\"]),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"node_id\": [n for n,_,_ in rows],\n",
        "    \"sm0_neighbors\": [\"|\".join(lst) for _,lst,_ in rows],\n",
        "    \"sm1_neighbors\": [\"|\".join(lst) for _,_,lst in rows],\n",
        "})\n",
        "\n",
        "df.to_csv(\"neighbors_manual.csv\", index=False)\n",
        "print(\"Wrote neighbors_manual.csv:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4NYl_ESZKX4",
        "outputId": "4a90ecc9-7b27-47f4-943b-54a8b1374e8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote neighbors_manual.csv:\n",
            "    node_id                     sm0_neighbors  \\\n",
            "0  node_176                        node_176_1   \n",
            "1  node_182  node_182_1|node_187_0|node_187_1   \n",
            "2  node_187  node_187_1|node_182_1|node_182_0   \n",
            "3  node_179  node_179_1|node_187_1|node_187_0   \n",
            "4  node_183  node_183_1|node_179_1|node_181_0   \n",
            "5  node_181  node_181_1|node_186_0|node_186_1   \n",
            "6  node_186  node_186_1|node_181_1|node_181_0   \n",
            "7  node_184  node_184_1|node_186_1|node_186_0   \n",
            "8  node_189                        node_189_1   \n",
            "\n",
            "                      sm1_neighbors  \n",
            "0                        node_176_0  \n",
            "1  node_182_0|node_187_0|node_187_1  \n",
            "2  node_187_0|node_179_0|node_179_1  \n",
            "3  node_179_0|node_187_1|node_187_0  \n",
            "4  node_183_0|node_181_0|node_181_1  \n",
            "5  node_181_0|node_186_0|node_186_1  \n",
            "6                                    \n",
            "7                                    \n",
            "8                        node_189_0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual neighbor composite\n",
        "\n",
        "Purpose: Build a reference soil-moisture time series for a target node/probe from manually listed neighbor nodes—no GPS needed.\n",
        "\n",
        "Inputs: Main dataframe with node, __t (UTC datetime), ground_humidity_0/1 (optionally …_clean), plus a neighbors_manual.csv with node_id, sm0_neighbors, sm1_neighbors.\n",
        "\n",
        "Process (robust):\n",
        "* Load neighbor IDs for the target node/probe.\n",
        "* Extract duplicate-safe time series for target & neighbors (median if duplicate timestamps).\n",
        "* Robust-scale each series using recent median & IQR (handles offsets/drift).\n",
        "* Screen neighbors by Spearman correlation to the target (keeps those that co-move).\n",
        "* Combine neighbors via median of z-scores, then map back to target units.\n",
        "* Quality flag: Compute rolling correlation target vs. reference → label confidence “med” or “low.”\n",
        "* Outputs: The composite reference series (same units as target) + metadata (source, confidence, number of neighbors, correlation).\n",
        "* Why this way: Median/IQR + Spearman + median combine = outlier-resistant, works with baseline differences, and is idempotent (safe to rerun)."
      ],
      "metadata": {
        "id": "h3wdsHOi9iPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Manual neighbor composite (no lat/lon needed) ----\n",
        "# Requires df with columns: node_col, \"__t\" (UTC datetime), ground_humidity_{0,1}, and *optionally* *_clean\n",
        "import re\n",
        "\n",
        "NEIGH_PATH = \"neighbors_manual.csv\"  # change if needed\n",
        "\n",
        "def _load_manual_neighbors(path=NEIGH_PATH):\n",
        "    try:\n",
        "        nm = pd.read_csv(path)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "    # normalize columns\n",
        "    nm.columns = [c.strip().lower() for c in nm.columns]\n",
        "    need = {\"node_id\",\"sm0_neighbors\",\"sm1_neighbors\"}\n",
        "    if not need.issubset(set(nm.columns)):\n",
        "        raise ValueError(f\"neighbors_manual.csv is missing columns: {sorted(need - set(nm.columns))}\")\n",
        "    return nm\n",
        "\n",
        "def _parse_ids(s):\n",
        "    if pd.isna(s): return []\n",
        "    items = [x.strip() for x in re.split(r\"[|,;]+\", str(s)) if x.strip()]\n",
        "    return items\n",
        "\n",
        "def _series_for(df_all, node_col, nid, probe, use_clean=False):\n",
        "    \"\"\"Return duplicate-safe time series for one neighbor & probe (as float, DatetimeIndex).\"\"\"\n",
        "    sub = df_all[df_all[node_col].astype(str) == str(nid)]\n",
        "    if sub.empty: return pd.Series(dtype=\"float64\")\n",
        "    sidx = pd.to_datetime(sub[\"__t\"], utc=True, errors=\"coerce\")\n",
        "    col = f\"ground_humidity_{probe}\" + (\"_clean\" if use_clean and f\"ground_humidity_{probe}_clean\" in sub.columns else \"\")\n",
        "    vals = pd.to_numeric(sub[col], errors=\"coerce\")\n",
        "    ser = pd.Series(vals.values, index=sidx).dropna()\n",
        "    if ser.index.duplicated().any():\n",
        "        ser = ser.groupby(level=0).median(numeric_only=True)\n",
        "    return ser.sort_index()\n",
        "\n",
        "def _robust_center_scale(s, horizon_days=90):\n",
        "    \"\"\"Return (median, IQR) over recent horizon for robust z-scaling; fall back to whole series.\"\"\"\n",
        "    if s.empty: return (np.nan, np.nan)\n",
        "    tmax = s.index.max()\n",
        "    recent = s[s.index >= (tmax - pd.Timedelta(days=horizon_days))]\n",
        "    base = recent if recent.notna().sum() >= 48 else s\n",
        "    med = base.median()\n",
        "    iqr = base.quantile(0.75) - base.quantile(0.25)\n",
        "    if not np.isfinite(iqr) or iqr == 0: iqr = np.nan\n",
        "    return med, iqr\n",
        "\n",
        "def build_manual_neighbor_ref(df_all, node_col, target_node, probe, t_index, neighbors_df, use_clean=False,\n",
        "                              min_neighbors=2, corr_gate=0.5):\n",
        "    \"\"\"\n",
        "    Build a 'virtual buddy' from manually listed neighbors for (target_node, probe).\n",
        "    Returns (ref_series, src, conf, nref, roll_corr).\n",
        "    conf ∈ {'med','low'}; src ∈ {'statistical'} for now.\n",
        "    \"\"\"\n",
        "    if neighbors_df is None:\n",
        "        return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    row = neighbors_df[neighbors_df[\"node_id\"].astype(str) == str(target_node)]\n",
        "    if row.empty:\n",
        "        return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    neigh_ids = _parse_ids(row.iloc[0][f\"sm{probe}_neighbors\"])\n",
        "    if not neigh_ids:\n",
        "        return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    # target series (raw preferred for divergence; you can switch to clean)\n",
        "    tgt = _series_for(df_all, node_col, target_node, probe, use_clean=False).reindex(t_index)\n",
        "    if tgt.notna().sum() < 100:\n",
        "        return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    # robust center/scale for mapping back to target units\n",
        "    t_med, t_iqr = _robust_center_scale(tgt)\n",
        "    if not np.isfinite(t_iqr):  # if fully flat or no variation, can't z-scale sensibly\n",
        "        return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    z_cols = {}\n",
        "    weights = []\n",
        "    used_ids = []\n",
        "    for nid in neigh_ids:\n",
        "        s = _series_for(df_all, node_col, nid, probe, use_clean=use_clean).reindex(t_index)\n",
        "        if s.notna().sum() < 100:\n",
        "            continue\n",
        "        n_med, n_iqr = _robust_center_scale(s)\n",
        "        if not np.isfinite(n_iqr):\n",
        "            continue\n",
        "        z = (s - n_med) / n_iqr\n",
        "        # similarity via Spearman is a bit more robust to offsets\n",
        "        ok = tgt.notna() & z.notna()\n",
        "        if ok.sum() < 100:\n",
        "            continue\n",
        "        corr = pd.Series(tgt.values, index=t_index)[ok].corr(pd.Series(z.values, index=t_index)[ok], method=\"spearman\")\n",
        "        if not np.isfinite(corr) or corr < 0.2:  # very permissive at this stage; we’ll gate later\n",
        "            continue\n",
        "        z_cols[nid] = z\n",
        "        weights.append(max(corr, 0))\n",
        "        used_ids.append(nid)\n",
        "\n",
        "    if len(z_cols) < min_neighbors:\n",
        "        return None, \"none\", \"low\", len(z_cols), np.nan\n",
        "\n",
        "    Z = pd.DataFrame(z_cols, index=t_index)\n",
        "    z_ref = Z.median(axis=1, skipna=True)  # robust across neighbors\n",
        "    ref = t_med + z_ref * t_iqr\n",
        "\n",
        "    # quality: rolling correlation over last ~48h equivalent samples\n",
        "    ok = tgt.notna() & ref.notna()\n",
        "    roll_corr = (pd.Series(tgt[ok]).rolling(48, min_periods=24).corr(pd.Series(ref[ok]))).median()\n",
        "    conf = \"med\" if (np.isfinite(roll_corr) and roll_corr >= corr_gate) else \"low\"\n",
        "    return ref, \"statistical\", conf, len(z_cols), roll_corr\n"
      ],
      "metadata": {
        "id": "60XYzNzER8hL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Step 5PD: Soil moisture**\n",
        "* Goal: Catch pair-divergence in soil moisture when a probe drifts away from its expected partner behavior, and mask those bad stretches in …_clean.\n",
        "* Inputs: Latest QC’d soil-moisture file (auto-picked from Stage 2–5/AR), plus optional neighbors_manual.csv. Requires probe 0/1 columns and timestamps.\n",
        "* Reference logic (per node, per probe):\n",
        "* Prefer the sibling probe as reference when it’s present & not already CRIT.\n",
        "* Otherwise build a manual-neighbor composite (robust median/IQR scaling, Spearman screening, median combine), and only trust it if correlation ≥ NEIGH_CORR_GATE.\n",
        "* Divergence score: Compute residuals vs. reference, then a causal rolling median baseline and MAD-based σ; flag large & meaningful gaps with thresholds:\n",
        "* Sibling: |z| ≥ Z_SIB and gap ≥ GAP_SIB\n",
        "* Neighbor: |z| ≥ Z_NEI and gap ≥ GAP_NEI\n",
        "* Run gating (fewer false positives): Require MIN_RUN consecutive flagged points; allow edge-relax (half length) if the run touches start/end of the series.\n",
        "* Masking & flags: Map runs back to rows, set qc_sm_pairdiverge_{0,1} = \"CRIT_pair_diverge\", update qc_sm_anycrit_{0,1}, and mask in ground_humidity_{0,1}_clean.\n",
        "* Robustness details: Duplicate timestamps → median; rolling stats use window = 7D with min periods to stabilize the tail; prior CRIT masks respected.\n",
        "* Outputs: Versioned CSV af_clean_v01/stage5pd_sm_pairdiverge_vNNN.csv, masked-share summary, and provenance columns (…_src, …_nref, …_corr) for auditability."
      ],
      "metadata": {
        "id": "nRvKHpNF_XMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 5PD: Soil moisture Pair-Divergence with Manual-Neighbor Fallback\n",
        "# =========================\n",
        "# (UNCHANGED preamble/inputs/outputs)\n",
        "\n",
        "import os, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Config ----\n",
        "IN_SEARCH = [\n",
        "    \"af_clean_v01/stage5r_sm_regimes_v*.csv\",\n",
        "    \"af_clean_v01/stage4_sm_flatline_v*.csv\",\n",
        "    \"af_clean_v01/stage3_sm_knownbroken_v*.csv\",\n",
        "    \"af_clean_v01/stage2_soilmoisture_range_v*.csv\",\n",
        "    \"af_clean_v01/analysis_ready_v*.csv\",\n",
        "    \"analysis_ready_v*.csv\"\n",
        "]\n",
        "OUT_DIR = Path(\"af_clean_v01\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_STEM = \"stage5pd_sm_pairdiverge_v\"\n",
        "\n",
        "# Gates\n",
        "ROLL_WIN = \"7D\"\n",
        "MIN_RUN  = 24\n",
        "Z_SIB    = 6.0\n",
        "GAP_SIB  = 2.0\n",
        "Z_NEI    = 7.0\n",
        "GAP_NEI  = 2.5\n",
        "EPS_SIG  = 1e-6\n",
        "NEIGH_CORR_GATE = 0.5\n",
        "\n",
        "# --- NEW: make rolling robust at the edges and relax runs that touch edges\n",
        "ROLL_CENTER = False                 # NEW: causal stats so tail has a baseline\n",
        "ROLL_MINP   = max(12, MIN_RUN//2)   # NEW: enough history to be stable at edges\n",
        "EDGE_RELAX  = 0.5                   # NEW: half-length ok if a run touches start/end\n",
        "\n",
        "# ---- small I/O helpers ----\n",
        "def _pick_latest():\n",
        "    for pat in IN_SEARCH:\n",
        "        hits = sorted(glob.glob(pat))\n",
        "        if hits:\n",
        "            def _ver(p):\n",
        "                m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(p))\n",
        "                return int(m.group(1)) if m else -1\n",
        "            best = max(hits, key=_ver)\n",
        "            return best if _ver(best) >= 0 else hits[-1]\n",
        "    raise FileNotFoundError(\"No SM stage file found for 5PD.\")\n",
        "\n",
        "def _next_versioned(stem):\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = OUT_DIR / f\"{stem}{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "def _to_dt(df):\n",
        "    if \"timestamp\" in df.columns:     return pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "    if \"timestamp_iso\" in df.columns: return pd.to_datetime(df[\"timestamp_iso\"], utc=True, errors=\"coerce\")\n",
        "    return pd.to_datetime(pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "\n",
        "def _dup_median(series):\n",
        "    s = series.dropna()\n",
        "    if s.index.duplicated().any():\n",
        "        s = s.groupby(level=0).median(numeric_only=True)\n",
        "    return s.sort_index()\n",
        "\n",
        "# --- CHANGED: causal rolling + smaller min_periods\n",
        "def _roll_med(series, win=ROLL_WIN, minp=ROLL_MINP, center=ROLL_CENTER):\n",
        "    return series.rolling(win, min_periods=minp, center=center).median()\n",
        "\n",
        "def _roll_mad_sigma(diff, win=ROLL_WIN, minp=ROLL_MINP, center=ROLL_CENTER):\n",
        "    dev = (diff - _roll_med(diff, win, minp, center)).abs()\n",
        "    mad = dev.rolling(win, min_periods=minp, center=center).median()\n",
        "    return 1.4826 * mad\n",
        "\n",
        "# --- CHANGED: edge-relaxed contiguous run detector\n",
        "def _contig_run_mask(mask_bool, min_len=MIN_RUN, edge_relax=EDGE_RELAX):\n",
        "    m = pd.Series(mask_bool.astype(bool))\n",
        "    if m.empty or not m.any():\n",
        "        return pd.Series(False, index=m.index)\n",
        "    idx = m.index\n",
        "    vals = m.values\n",
        "    n = len(vals)\n",
        "    out = pd.Series(False, index=idx)\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        if vals[i]:\n",
        "            j = i + 1\n",
        "            while j < n and vals[j]:\n",
        "                j += 1\n",
        "            seg_len = j - i\n",
        "            touches_edge = (i == 0) or (j == n)\n",
        "            need = min_len if not touches_edge else int(np.ceil(edge_relax * min_len))\n",
        "            if seg_len >= need:\n",
        "                out.iloc[i:j] = True\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "def _coerce_anycrit_bool(s):\n",
        "    if s is None: return None\n",
        "    s = pd.Series(s)\n",
        "    if s.dtype == bool: return s.fillna(False)\n",
        "    return s.astype(str).str.startswith(\"CRIT\", na=False)\n",
        "\n",
        "# ---- neighbor helpers must exist (unchanged guard)\n",
        "if '_load_manual_neighbors' not in globals() or 'build_manual_neighbor_ref' not in globals():\n",
        "    raise RuntimeError(\"Missing neighbor helpers. Run the helper cell defining `_load_manual_neighbors` and `build_manual_neighbor_ref` first.\")\n",
        "\n",
        "# ---- Load input (unchanged) ----\n",
        "src = _pick_latest()\n",
        "df  = pd.read_csv(src, low_memory=False)\n",
        "print(f\"[5PD] Input: {src} | rows={len(df)}\")\n",
        "\n",
        "# ---- Time & node columns (unchanged) ----\n",
        "df[\"__t\"] = _to_dt(df)\n",
        "df = df.dropna(subset=[\"__t\"]).copy()\n",
        "node_col = None\n",
        "for c in [\"node_id\",\"node\",\"station\",\"device\",\"site\"]:\n",
        "    if c in df.columns: node_col = c; break\n",
        "if node_col is None:\n",
        "    df[\"__node\"] = \"__all__\"; node_col = \"__node\"\n",
        "print(f\"[5PD] Using node column: {node_col}\")\n",
        "\n",
        "# ---- Ensure clean columns exist (unchanged) ----\n",
        "for suf in (\"0\",\"1\"):\n",
        "    raw = f\"ground_humidity_{suf}\"\n",
        "    clean = f\"{raw}_clean\"\n",
        "    if clean not in df.columns:\n",
        "        df[clean] = pd.to_numeric(df[raw], errors=\"coerce\")\n",
        "\n",
        "# ---- Load manual neighbors (unchanged) ----\n",
        "neighbors_df = _load_manual_neighbors()\n",
        "\n",
        "# ---- Pre-anycrit (unchanged) ----\n",
        "def _pre_anycrit_bool(g, suf):\n",
        "    idx = pd.to_datetime(g[\"__t\"])\n",
        "    def _crit_from(col):\n",
        "        if col not in g.columns:\n",
        "            return pd.Series(False, index=idx)\n",
        "        s = g[col].astype(str)\n",
        "        return pd.Series(s.str.startswith(\"CRIT\", na=False).values, index=idx)\n",
        "    cols = [f\"qc_sm_range_{suf}\", f\"qc_sm_flatline_{suf}\", f\"qc_sm_known_broken_{suf}\", f\"qc_sm_regime_{suf}\"]\n",
        "    m = pd.Series(False, index=idx)\n",
        "    for c in cols:\n",
        "        m = m | _crit_from(c)\n",
        "    if m.index.duplicated().any():\n",
        "        m = m.groupby(level=0).max()\n",
        "    return m\n",
        "\n",
        "# ---- Prepare output columns (unchanged) ----\n",
        "df[\"qc_sm_pairdiverge_0\"] = \"OK\"; df[\"qc_sm_pairdiverge_1\"] = \"OK\"\n",
        "df[\"qc_sm_pairdiverge_src_0\"] = \"\"; df[\"qc_sm_pairdiverge_src_1\"] = \"\"\n",
        "df[\"qc_sm_pairdiverge_nref_0\"] = 0; df[\"qc_sm_pairdiverge_nref_1\"] = 0\n",
        "df[\"qc_sm_pairdiverge_corr_0\"] = np.nan; df[\"qc_sm_pairdiverge_corr_1\"] = np.nan\n",
        "for suf in (\"0\",\"1\"):\n",
        "    col_any = f\"qc_sm_anycrit_{suf}\"\n",
        "    if col_any in df.columns:\n",
        "        df[col_any] = _coerce_anycrit_bool(df[col_any])\n",
        "    else:\n",
        "        df[col_any] = False\n",
        "\n",
        "# ---- Per-node processing (unchanged logic apart from new gates) ----\n",
        "masked_counts = {\"0\":0, \"1\":0}\n",
        "raw_counts    = {\"0\":0, \"1\":0}\n",
        "\n",
        "for node, g in df.sort_values([node_col, \"__t\"]).groupby(df[node_col].astype(str), sort=False):\n",
        "    t_idx = pd.to_datetime(g[\"__t\"])\n",
        "    s0_raw = _dup_median(pd.Series(pd.to_numeric(g[\"ground_humidity_0\"], errors=\"coerce\").values, index=t_idx))\n",
        "    s1_raw = _dup_median(pd.Series(pd.to_numeric(g[\"ground_humidity_1\"], errors=\"coerce\").values, index=t_idx))\n",
        "    s0_clean = _dup_median(pd.Series(pd.to_numeric(g[\"ground_humidity_0_clean\"], errors=\"coerce\").values, index=t_idx))\n",
        "    s1_clean = _dup_median(pd.Series(pd.to_numeric(g[\"ground_humidity_1_clean\"], errors=\"coerce\").values, index=t_idx))\n",
        "\n",
        "    T = s0_raw.index.union(s1_raw.index).unique().sort_values()\n",
        "    s0r = s0_raw.reindex(T)\n",
        "    s1r = s1_raw.reindex(T)\n",
        "\n",
        "    pre0 = _pre_anycrit_bool(g, \"0\").reindex(T).fillna(False)\n",
        "    pre1 = _pre_anycrit_bool(g, \"1\").reindex(T).fillna(False)\n",
        "\n",
        "    sib_ok_for_0 = s1r.notna() & (~pre1)\n",
        "    sib_ok_for_1 = s0r.notna() & (~pre0)\n",
        "\n",
        "    ref0_nei, src0, conf0, nref0, rcorr0 = build_manual_neighbor_ref(\n",
        "        df_all=df, node_col=node_col, target_node=node, probe=\"0\",\n",
        "        t_index=T, neighbors_df=neighbors_df, use_clean=False,\n",
        "        min_neighbors=2, corr_gate=NEIGH_CORR_GATE\n",
        "    )\n",
        "    ref1_nei, src1, conf1, nref1, rcorr1 = build_manual_neighbor_ref(\n",
        "        df_all=df, node_col=node_col, target_node=node, probe=\"1\",\n",
        "        t_index=T, neighbors_df=neighbors_df, use_clean=False,\n",
        "        min_neighbors=2, corr_gate=NEIGH_CORR_GATE\n",
        "    )\n",
        "    ref0_nei = ref0_nei if isinstance(ref0_nei, pd.Series) else pd.Series(np.nan, index=T)\n",
        "    ref1_nei = ref1_nei if isinstance(ref1_nei, pd.Series) else pd.Series(np.nan, index=T)\n",
        "\n",
        "    ref0 = s1r.where(sib_ok_for_0).combine_first(ref0_nei)\n",
        "    ref1 = s0r.where(sib_ok_for_1).combine_first(ref1_nei)\n",
        "    src0_series = pd.Series(np.where(sib_ok_for_0, \"sibling\",\n",
        "                            np.where(ref0_nei.notna(), \"statistical\", \"none\")), index=T)\n",
        "    src1_series = pd.Series(np.where(sib_ok_for_1, \"sibling\",\n",
        "                            np.where(ref1_nei.notna(), \"statistical\", \"none\")), index=T)\n",
        "\n",
        "    d0 = s0r - ref0; d1 = s1r - ref1\n",
        "    med0 = _roll_med(d0); med1 = _roll_med(d1)\n",
        "    sig0 = _roll_mad_sigma(d0).clip(lower=EPS_SIG)\n",
        "    sig1 = _roll_mad_sigma(d1).clip(lower=EPS_SIG)\n",
        "    z0 = (d0 - med0).abs() / sig0\n",
        "    z1 = (d1 - med1).abs() / sig1\n",
        "    gap0 = (d0 - med0).abs(); gap1 = (d1 - med1).abs()\n",
        "\n",
        "    cand0_sib = (src0_series.eq(\"sibling\")     & (z0 >= Z_SIB) & (gap0 >= GAP_SIB))\n",
        "    cand0_nei = (src0_series.eq(\"statistical\") & (conf0 == \"med\") & (z0 >= Z_NEI) & (gap0 >= GAP_NEI))\n",
        "    cand1_sib = (src1_series.eq(\"sibling\")     & (z1 >= Z_SIB) & (gap1 >= GAP_SIB))\n",
        "    cand1_nei = (src1_series.eq(\"statistical\") & (conf1 == \"med\") & (z1 >= Z_NEI) & (gap1 >= GAP_NEI))\n",
        "\n",
        "    cand0 = (cand0_sib | cand0_nei).fillna(False)\n",
        "    cand1 = (cand1_sib | cand1_nei).fillna(False)\n",
        "\n",
        "    # --- CHANGED: edge-relaxed run gating\n",
        "    crit0 = _contig_run_mask(cand0, MIN_RUN, EDGE_RELAX)\n",
        "    crit1 = _contig_run_mask(cand1, MIN_RUN, EDGE_RELAX)\n",
        "\n",
        "    # Map back to rows\n",
        "    t_rows = pd.to_datetime(g[\"__t\"])\n",
        "    mask0_rows = pd.Series(crit0.values, index=cand0.index).reindex(t_rows).fillna(False).values\n",
        "    mask1_rows = pd.Series(crit1.values, index=cand1.index).reindex(t_rows).fillna(False).values\n",
        "\n",
        "    idx = g.index\n",
        "    df.loc[idx, \"qc_sm_pairdiverge_0\"] = np.where(mask0_rows, \"CRIT_pair_diverge\", df.loc[idx, \"qc_sm_pairdiverge_0\"])\n",
        "    df.loc[idx, \"qc_sm_pairdiverge_1\"] = np.where(mask1_rows, \"CRIT_pair_diverge\", df.loc[idx, \"qc_sm_pairdiverge_1\"])\n",
        "    df.loc[idx, \"ground_humidity_0_clean\"] = pd.to_numeric(df.loc[idx, \"ground_humidity_0_clean\"], errors=\"coerce\").mask(mask0_rows)\n",
        "    df.loc[idx, \"ground_humidity_1_clean\"] = pd.to_numeric(df.loc[idx, \"ground_humidity_1_clean\"], errors=\"coerce\").mask(mask1_rows)\n",
        "\n",
        "    df.loc[idx, \"qc_sm_anycrit_0\"] = df.loc[idx, \"qc_sm_anycrit_0\"] | mask0_rows\n",
        "    df.loc[idx, \"qc_sm_anycrit_1\"] = df.loc[idx, \"qc_sm_anycrit_1\"] | mask1_rows\n",
        "\n",
        "    src0_rows = pd.Series(src0_series.values, index=cand0.index).reindex(t_rows).astype(str).values\n",
        "    src1_rows = pd.Series(src1_series.values, index=cand1.index).reindex(t_rows).astype(str).values\n",
        "    df.loc[idx, \"qc_sm_pairdiverge_src_0\"] = np.where(mask0_rows, src0_rows, df.loc[idx, \"qc_sm_pairdiverge_src_0\"])\n",
        "    df.loc[idx, \"qc_sm_pairdiverge_src_1\"] = np.where(mask1_rows, src1_rows, df.loc[idx, \"qc_sm_pairdiverge_src_1\"])\n",
        "\n",
        "    if nref0:\n",
        "        df.loc[idx, \"qc_sm_pairdiverge_nref_0\"] = int(nref0)\n",
        "        df.loc[idx, \"qc_sm_pairdiverge_corr_0\"] = float(rcorr0) if pd.notna(rcorr0) else np.nan\n",
        "    if nref1:\n",
        "        df.loc[idx, \"qc_sm_pairdiverge_nref_1\"] = int(nref1)\n",
        "        df.loc[idx, \"qc_sm_pairdiverge_corr_1\"] = float(rcorr1) if pd.notna(rcorr1) else np.nan\n",
        "\n",
        "    masked_counts[\"0\"] += int(mask0_rows.sum())\n",
        "    masked_counts[\"1\"] += int(mask1_rows.sum())\n",
        "    raw_counts[\"0\"]    += int(pd.to_numeric(g[\"ground_humidity_0\"], errors=\"coerce\").notna().sum())\n",
        "    raw_counts[\"1\"]    += int(pd.to_numeric(g[\"ground_humidity_1\"], errors=\"coerce\").notna().sum())\n",
        "\n",
        "# ---- Save & summary (unchanged) ----\n",
        "out_path = _next_versioned(OUT_STEM)\n",
        "df.to_csv(out_path, index=False)\n",
        "print(f\"[5PD] wrote: {out_path}\")\n",
        "for suf in (\"0\",\"1\"):\n",
        "    denom = raw_counts[suf] or 1\n",
        "    print(f\"[5PD] Masked share probe {suf}: {masked_counts[suf]}/{denom} = {masked_counts[suf]/denom:.2%}\")\n",
        "print(\"[5PD] Notes:\",\n",
        "      f\" Z_SIB={Z_SIB}, GAP_SIB={GAP_SIB}, Z_NEI={Z_NEI}, GAP_NEI={GAP_NEI}, MIN_RUN={MIN_RUN}, ROLL_MINP={ROLL_MINP}, EDGE_RELAX={EDGE_RELAX}, NEIGH_CORR_GATE={NEIGH_CORR_GATE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu0Acdx7X6QB",
        "outputId": "a145f0c7-53f2-45d4-c66e-be92184e6fd4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5PD] Input: af_clean_v01/stage5r_sm_regimes_v001.csv | rows=483787\n",
            "[5PD] Using node column: node_id\n",
            "[5PD] wrote: af_clean_v01/stage5pd_sm_pairdiverge_v001.csv\n",
            "[5PD] Masked share probe 0: 8440/483787 = 1.74%\n",
            "[5PD] Masked share probe 1: 7462/483787 = 1.54%\n",
            "[5PD] Notes:  Z_SIB=6.0, GAP_SIB=2.0, Z_NEI=7.0, GAP_NEI=2.5, MIN_RUN=24, ROLL_MINP=12, EDGE_RELAX=0.5, NEIGH_CORR_GATE=0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6a: Soil temperature range QC**\n",
        "* Loads your latest stage file (prefers the most recent; doesn’t overwrite anything).\n",
        "* Parses timestamp (naive) and checks required columns exist: ground_temp_0, ground_temp_1, node, station_type.\n",
        "* Casts soil temperature columns to numeric (bad parses → NaN), leaving the raw columns untouched.\n",
        "* Applies physical bounds (°C): OK range: −20.0 to 50.0\n",
        "* Tolerance band: [−20.5, −20.0) and (50.0, 60.5] → WARN_boundary\n",
        "* Outside tolerance: < −20.5 or > 50.5 → CRIT_out_of_range\n",
        "* Creates QC flag columns per probe: qc_st_range_0 and qc_st_range_1 with OK / WARN_boundary / CRIT_out_of_range.\n",
        "* Adds rollups: qc_st_anycrit_* (True when CRIT), qc_st_anywarn_* (True when WARN and not CRIT).\n",
        "* Builds clean series: ground_temp_{0|1}_clean\n",
        "* CRIT → NaN\n",
        "* WARN_boundary below/above limits → clamped to −20.0 / 50.0\n",
        "* OK → value passes through unchanged\n",
        "* Sorts deterministically by node, timestamp and saves a new versioned CSV: stage6a_soiltemp_range_v###.csv.\n",
        "* Writes summaries: overall counts per flag and per-node counts to separate CSVs.\n",
        "* Prints breadcrumbs: input used, time span, node list—so you can verify at a glance."
      ],
      "metadata": {
        "id": "LVOdkrr1iD6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 6a: Soil temperature Range QC (no unit inference)\n",
        "# GUARANTEES:\n",
        "#  - Reads latest prior stage (prefers 6c→6b→6a→5PD→5R→4→3→2→1)\n",
        "#  - Reconstructs timestamp from ts_ns (no free-text parsing)\n",
        "#  - No row loss; preserves row_id\n",
        "#  - Carries forward SM clean/QC from Step 5PD if present (so masks survive)\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os, re\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core columns from Stage 1\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"     # in-memory datetime64[ns]\n",
        "NODE_COL      = \"node\"\n",
        "STATION_TYPE  = \"station_type\"\n",
        "\n",
        "# Soil temperature columns\n",
        "ST_COLS  = [\"ground_temp_0\", \"ground_temp_1\"]   # °C at ~35 cm\n",
        "ST_CLEAN = {c: f\"{c}_clean\" for c in ST_COLS}\n",
        "\n",
        "# Physical bounds\n",
        "LOWER_OK, UPPER_OK   = -20.0, 50.0\n",
        "LOWER_TOL, UPPER_TOL = -20.5, 50.5   # tolerance band → WARN_boundary\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "# ---- Load the latest available stage (prefer later) ----\n",
        "def pick_latest_by_priority(patterns):\n",
        "    for pat in patterns:\n",
        "        files = sorted(glob(str(OUT_DIR / pat)))\n",
        "        if files:\n",
        "            return files[-1]\n",
        "    return None\n",
        "\n",
        "# ---- Carry-forward 5PD soil-moisture outputs (if present) ----\n",
        "def carry_forward_sm_from_5pd(df_base: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If a stage5pd_sm_pairdiverge_v*.csv exists, bring its SM clean/QC columns\n",
        "    into df_base (overwrite by row_id). This preserves 5PD masks through later steps.\n",
        "    \"\"\"\n",
        "    hits = sorted(glob(str(OUT_DIR / \"stage5pd_sm_pairdiverge_v*.csv\")))\n",
        "    if not hits:\n",
        "        return df_base\n",
        "\n",
        "    # prefer highest version number\n",
        "    def _ver(path):\n",
        "        m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(path))\n",
        "        return int(m.group(1)) if m else -1\n",
        "    src = max(hits, key=_ver) if _ver(hits[-1]) >= 0 else hits[-1]\n",
        "\n",
        "    want = [\n",
        "        # Clean values (these carry the masks)\n",
        "        \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "        # 5PD divergence flags\n",
        "        \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "        # Rollups and other SM QC families (safe no-op if missing)\n",
        "        \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\n",
        "        \"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "        \"qc_sm_range_0\",\"qc_sm_range_1\",\n",
        "        \"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "        \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "        \"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "    ]\n",
        "    hdr = pd.read_csv(src, nrows=0, low_memory=False)\n",
        "    have = [c for c in want if c in hdr.columns]\n",
        "    if not have:\n",
        "        return df_base\n",
        "\n",
        "    add = pd.read_csv(src, usecols=[ROW_ID_COL] + have, low_memory=False)\n",
        "    out = df_base.drop(columns=[c for c in have if c in df_base.columns], errors=\"ignore\")\n",
        "    out = out.merge(add, on=ROW_ID_COL, how=\"left\")\n",
        "    print(f\"[carry] Brought {len(have)} SM cols from 5PD: {os.path.basename(src)}\")\n",
        "    return out\n",
        "\n",
        "# ---- Choose input (prefer 5PD over 5R) ----\n",
        "in_path = pick_latest_by_priority([\n",
        "    \"stage6c_soiltemp_step_v*.csv\",\n",
        "    \"stage6b_soiltemp_flatline_v*.csv\",\n",
        "    \"stage6a_soiltemp_range_v*.csv\",\n",
        "    \"stage5pd_sm_pairdiverge_v*.csv\",   # ← NEW: ensure we see 5PD outputs\n",
        "    \"stage5r_sm_regimes_v*.csv\",\n",
        "    \"stage4_sm_flatline_v*.csv\",\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage2_soilmoisture_range_v*.csv\",\n",
        "    \"stage1_parsed_v*.csv\",\n",
        "])\n",
        "assert in_path, f\"No earlier stage files found in {OUT_DIR}.\"\n",
        "print(\"Reading:\", in_path)\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "\n",
        "# ---- Carry-forward SM masks/flags from 5PD (if present) ----\n",
        "df = carry_forward_sm_from_5pd(df)\n",
        "\n",
        "# ---- Sanity: required columns ----\n",
        "required = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL, STATION_TYPE, *ST_COLS}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns for Stage 6a: {sorted(missing)}\")\n",
        "\n",
        "# Reconstruct timestamp from ts_ns (authoritative)\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "\n",
        "# Invariants\n",
        "n_in = len(df)\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id must be unique (from Stage 1).\"\n",
        "\n",
        "# Ensure numeric for temp columns (raw columns remain unchanged)\n",
        "for c in ST_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "def temp_range_qc(df: pd.DataFrame, col: str):\n",
        "    \"\"\"\n",
        "    Adds per-probe:\n",
        "      - qc_st_range_<k> : 'OK' / 'WARN_boundary' / 'CRIT_out_of_range'\n",
        "      - qc_st_anycrit_<k> (bool)\n",
        "      - qc_st_anywarn_<k> (bool)\n",
        "      - <col>_clean     : CRIT -> NaN; WARN_boundary clamped; OK passthrough\n",
        "    \"\"\"\n",
        "    k = \"0\" if col.endswith(\"_0\") else \"1\" if col.endswith(\"_1\") else col\n",
        "\n",
        "    v = df[col]\n",
        "    qc = pd.Series(\"OK\", index=v.index, dtype=\"object\")\n",
        "\n",
        "    warn_low  = v.ge(LOWER_TOL) & v.lt(LOWER_OK)    # [-20.5, -20.0)\n",
        "    warn_high = v.gt(UPPER_OK) & v.le(UPPER_TOL)    # (50.0, 50.5]\n",
        "    crit_low  = v.lt(LOWER_TOL)\n",
        "    crit_high = v.gt(UPPER_TOL)\n",
        "\n",
        "    qc.loc[warn_low | warn_high] = \"WARN_boundary\"\n",
        "    qc.loc[crit_low | crit_high] = \"CRIT_out_of_range\"\n",
        "\n",
        "    clean = v.copy()\n",
        "    clean.loc[crit_low | crit_high] = np.nan\n",
        "    clean.loc[warn_low]  = LOWER_OK\n",
        "    clean.loc[warn_high] = UPPER_OK\n",
        "\n",
        "    df[f\"qc_st_range_{k}\"]   = qc\n",
        "    df[f\"qc_st_anycrit_{k}\"] = qc.eq(\"CRIT_out_of_range\")\n",
        "    df[f\"qc_st_anywarn_{k}\"] = qc.eq(\"WARN_boundary\")\n",
        "    df[f\"{col}_clean\"]       = clean\n",
        "\n",
        "    return {\n",
        "        \"probe\": col,\n",
        "        \"n_OK\": int(qc.eq(\"OK\").sum()),\n",
        "        \"n_WARN_boundary\": int(qc.eq(\"WARN_boundary\").sum()),\n",
        "        \"n_CRIT_out_of_range\": int(qc.eq(\"CRIT_out_of_range\").sum()),\n",
        "    }\n",
        "\n",
        "# Apply to both probes\n",
        "summaries = [temp_range_qc(df, col) for col in ST_COLS]\n",
        "\n",
        "# Deterministic sort & save\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage6a_soiltemp_range\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# Summary CSVs\n",
        "sum_df = pd.DataFrame(summaries)\n",
        "sum_path = next_versioned_csv(OUT_DIR, \"stage6a_soiltemp_range_counts_overall\")\n",
        "sum_df.to_csv(sum_path, index=False)\n",
        "print(\"Wrote overall counts:\", sum_path)\n",
        "print(\"\\nOverall temperature range QC:\")\n",
        "print(sum_df.to_string(index=False))\n",
        "\n",
        "pernode = (\n",
        "    df.melt(id_vars=[NODE_COL, TIMESTAMP_COL], value_vars=[\"qc_st_range_0\", \"qc_st_range_1\"],\n",
        "            var_name=\"which\", value_name=\"flag\")\n",
        "      .groupby([NODE_COL, \"which\", \"flag\"]).size().rename(\"n\").reset_index()\n",
        "      .sort_values([NODE_COL, \"which\", \"flag\"])\n",
        ")\n",
        "pernode_path = next_versioned_csv(OUT_DIR, \"stage6a_soiltemp_range_counts_pernode\")\n",
        "pernode.to_csv(pernode_path, index=False)\n",
        "print(\"Wrote per-node counts:\", pernode_path)\n",
        "\n",
        "# Breadcrumbs\n",
        "tmin = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).min()\n",
        "tmax = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).max()\n",
        "print(\"\\nTime span (dataset, from ts_ns):\", tmin, \"→\", tmax)\n",
        "print(\"Rows in/out (should match input):\", n_in, \"→\", len(df))\n",
        "print(\"Nodes:\", sorted(df[NODE_COL].dropna().unique().tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkwGwhAFEBf7",
        "outputId": "95ebebf9-f316-4045-d831-4244a8e24f76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: af_clean_v01/stage5pd_sm_pairdiverge_v001.csv\n",
            "[carry] Brought 16 SM cols from 5PD: stage5pd_sm_pairdiverge_v001.csv\n",
            "Wrote: af_clean_v01/stage6a_soiltemp_range_v001.csv\n",
            "Wrote overall counts: af_clean_v01/stage6a_soiltemp_range_counts_overall_v001.csv\n",
            "\n",
            "Overall temperature range QC:\n",
            "        probe   n_OK  n_WARN_boundary  n_CRIT_out_of_range\n",
            "ground_temp_0 455253              154                28380\n",
            "ground_temp_1 467617               29                16141\n",
            "Wrote per-node counts: af_clean_v01/stage6a_soiltemp_range_counts_pernode_v001.csv\n",
            "\n",
            "Time span (dataset, from ts_ns): 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "Rows in/out (should match input): 483787 → 483787\n",
            "Nodes: ['node_176', 'node_179', 'node_181', 'node_182', 'node_183', 'node_184', 'node_186', 'node_187', 'node_189']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 6b (Soil temperature — flatline detection)**\n",
        "* Loaded the correct prior file: Picked the latest stage6a_soiltemp_range_v*.csv and rebuilt timestamp from ts_ns (no free-text parsing). Preserved row_id; verified no row loss.\n",
        "* Prepared temperature series: Ensured numeric ground_temp_0/1 (raw). Used the already-created ground_temp_0_clean / ground_temp_1_clean for future masking.\n",
        "* Estimated cadence per node: Computed a median sampling interval (minutes) for each node to translate “24 h” and “48 h” into the equivalent number of consecutive samples.\n",
        "* Resolution-aware equality: In each node×probe, inferred the instrument’s numeric resolution (smallest positive step) and rounded values to that grid before comparing, so tiny floating-point jitter doesn’t break runs.\n",
        "* Detected flatlines: Marked contiguous runs of identical (rounded) values and flagged per row:\n",
        "* WARN_flatline if the run length ≥ 24 h but < 48 h,\n",
        "* CRIT_flatline if the run length ≥ 48 h,\n",
        "* otherwise OK.\n",
        "* Updated rollups: For each probe (0/1), updated qc_st_anycrit_* and qc_st_anywarn_*:\n",
        "* anycrit := previous anycrit OR new CRIT_flatline\n",
        "* anywarn := (previous anywarn OR new WARN_flatline) but not rows that are now anycrit.\n",
        "* Masked only where needed: Set ground_temp_*_clean = NaN only on rows flagged CRIT_flatline. WARN_flatline remains visible (no masking).\n",
        "* Saved versioned outputs: Wrote the full table to stage6b_soiltemp_flatline_v###.csv.\n",
        "* Produced summaries:\n",
        "* Overall counts (OK / WARN_flatline / CRIT_flatline) per probe → stage6b_soiltemp_flatline_counts_overall_v###.csv.\n",
        "* Per-node duration estimates (hours spent in each flatline state), computed from node-specific cadence → stage6b_soiltemp_flatline_counts_pernode_v###.csv."
      ],
      "metadata": {
        "id": "qSZHMgUk7b6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 6b: Soil temperature — flatline detection (exact-repeat, resolution-aware)\n",
        "# GUARANTEES:\n",
        "#  - Reads latest Stage 6a (REQUIRED; needs *_clean columns)\n",
        "#  - Reconstructs timestamp from ts_ns (no free-text parsing)\n",
        "#  - No row loss; preserves row_id\n",
        "#  - Flags: qc_st_flatline_{0|1} = OK / WARN_flatline / CRIT_flatline\n",
        "#  - Rollups: qc_st_anycrit_{0|1}, qc_st_anywarn_{0|1} updated\n",
        "#  - Masks: ground_temp_{0|1}_clean set to NaN where CRIT_flatline\n",
        "#  - NEW: prefers 5PD in priority list and carries SM masks forward from 5PD\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os, re, math\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core columns\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"   # in-memory datetime64[ns]\n",
        "NODE_COL      = \"node\"\n",
        "\n",
        "# Soil temperature\n",
        "ST_COLS  = [\"ground_temp_0\", \"ground_temp_1\"]\n",
        "ST_CLEAN = {c: f\"{c}_clean\" for c in ST_COLS}\n",
        "\n",
        "# Thresholds (time-based)\n",
        "WARN_HOURS = 24    # identical value for ≥ 24 h → WARN\n",
        "CRIT_HOURS = 48    # identical value for ≥ 48 h → CRIT\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "def pick_latest_by_priority(patterns):\n",
        "    \"\"\"Return path from the FIRST pattern that has matches, taking its latest file.\"\"\"\n",
        "    for pat in patterns:\n",
        "        files = sorted(glob(str(OUT_DIR / pat)))\n",
        "        if files:\n",
        "            return files[-1]\n",
        "    return None\n",
        "\n",
        "# ---- Carry-forward 5PD soil-moisture outputs (if present) ----\n",
        "def carry_forward_sm_from_5pd(df_base: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If a stage5pd_sm_pairdiverge_v*.csv exists, bring its SM clean/QC columns\n",
        "    into df_base (overwrite by row_id). This preserves 5PD masks through later steps.\n",
        "    \"\"\"\n",
        "    hits = sorted(glob(str(OUT_DIR / \"stage5pd_sm_pairdiverge_v*.csv\")))\n",
        "    if not hits:\n",
        "        return df_base\n",
        "\n",
        "    def _ver(path):\n",
        "        m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(path))\n",
        "        return int(m.group(1)) if m else -1\n",
        "    src = max(hits, key=_ver) if _ver(hits[-1]) >= 0 else hits[-1]\n",
        "\n",
        "    want = [\n",
        "        # Clean values (carry masks)\n",
        "        \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "        # 5PD divergence flags\n",
        "        \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "        # Rollups / other SM QC (safe if missing)\n",
        "        \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\n",
        "        \"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "        \"qc_sm_range_0\",\"qc_sm_range_1\",\n",
        "        \"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "        \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "        \"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "    ]\n",
        "    hdr = pd.read_csv(src, nrows=0, low_memory=False)\n",
        "    have = [c for c in want if c in hdr.columns]\n",
        "    if not have:\n",
        "        return df_base\n",
        "\n",
        "    add = pd.read_csv(src, usecols=[ROW_ID_COL] + have, low_memory=False)\n",
        "    out = df_base.drop(columns=[c for c in have if c in df_base.columns], errors=\"ignore\")\n",
        "    out = out.merge(add, on=ROW_ID_COL, how=\"left\")\n",
        "    print(f\"[carry] Brought {len(have)} SM cols from 5PD: {os.path.basename(src)}\")\n",
        "    return out\n",
        "\n",
        "# ---- Load latest (prefer 6b→6a→5PD→5R→4→3→2→1) ----\n",
        "in_path = pick_latest_by_priority([\n",
        "    \"stage6b_soiltemp_flatline_v*.csv\",  # if re-running\n",
        "    \"stage6a_soiltemp_range_v*.csv\",     # REQUIRED normally (provides *_clean)\n",
        "    \"stage5pd_sm_pairdiverge_v*.csv\",    # NEW: ensure 5PD is seen if 6a missing\n",
        "    \"stage5r_sm_regimes_v*.csv\",\n",
        "    \"stage4_sm_flatline_v*.csv\",\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage2_soilmoisture_range_v*.csv\",\n",
        "    \"stage1_parsed_v*.csv\",\n",
        "])\n",
        "assert in_path is not None, \"No earlier stage files found.\"\n",
        "print(\"Reading:\", in_path)\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "\n",
        "# ---- Carry-forward SM masks/flags from 5PD (if present) ----\n",
        "df = carry_forward_sm_from_5pd(df)\n",
        "\n",
        "# ---- Required columns ----\n",
        "required = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL, *ST_COLS, *ST_CLEAN.values()}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns for Stage 6b: {sorted(missing)}\")\n",
        "\n",
        "# Reconstruct timestamp from ts_ns (authoritative)\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "\n",
        "# Invariants\n",
        "n_in = len(df)\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id should be unique from Stage 1.\"\n",
        "\n",
        "# Ensure numeric for BOTH raw and clean\n",
        "for c in ST_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df[ST_CLEAN[c]] = pd.to_numeric(df[ST_CLEAN[c]], errors=\"coerce\")\n",
        "\n",
        "# ---- Helpers ----\n",
        "def infer_resolution(x: pd.Series) -> float:\n",
        "    \"\"\"Infer numeric resolution from unique positive deltas; clamp to [1e-3, 1.0].\"\"\"\n",
        "    vals = np.sort(x.dropna().unique())\n",
        "    if len(vals) < 2:\n",
        "        return 0.01\n",
        "    diffs = np.diff(vals)\n",
        "    pos = diffs[diffs > 0]\n",
        "    res = float(np.nanmin(pos)) if len(pos) else 0.01\n",
        "    return max(1e-3, min(res, 1.0))\n",
        "\n",
        "def decimals_from_resolution(res: float) -> int:\n",
        "    if res <= 0:\n",
        "        return 2\n",
        "    dec = max(0, int(round(-np.log10(res))))\n",
        "    return min(dec, 6)\n",
        "\n",
        "def median_sampling_minutes(ts: pd.Series) -> float:\n",
        "    \"\"\"Estimate typical sampling interval (minutes) for the node.\"\"\"\n",
        "    if ts.size < 2:\n",
        "        return np.nan\n",
        "    d = (ts.values[1:] - ts.values[:-1]).astype(\"timedelta64[s]\").astype(float)\n",
        "    med = np.nanmedian(d) if d.size else np.nan\n",
        "    if not (med > 0):\n",
        "        return np.nan\n",
        "    return float(med) / 60.0\n",
        "\n",
        "def flag_flatlines_for_probe(g: pd.DataFrame, clean_col: str, suffix: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Return per-row 'OK' / 'WARN_flatline' / 'CRIT_flatline' for one node+probe group,\n",
        "    operating on the CLEAN column.\n",
        "    \"\"\"\n",
        "    dt_min = median_sampling_minutes(g[TIMESTAMP_COL].reset_index(drop=True))\n",
        "    if not (dt_min > 0):\n",
        "        dt_min = 10.0\n",
        "\n",
        "    warn_needed = max(2, int(math.ceil((WARN_HOURS * 60.0) / dt_min)))\n",
        "    crit_needed = max(2, int(math.ceil((CRIT_HOURS * 60.0) / dt_min)))\n",
        "\n",
        "    # Infer resolution from CLEAN values and quantize before equality test\n",
        "    vals = pd.to_numeric(g[clean_col], errors=\"coerce\")\n",
        "    res = infer_resolution(vals)\n",
        "    dec = decimals_from_resolution(res)\n",
        "\n",
        "    v = vals.to_numpy(dtype=float)\n",
        "    v_rounded = np.round(v / res) * res\n",
        "    v_rounded = np.round(v_rounded, dec)\n",
        "\n",
        "    same_as_prev = np.r_[True, v_rounded[1:] == v_rounded[:-1]]\n",
        "    run_id = np.cumsum(~same_as_prev)\n",
        "    counts = np.bincount(run_id)\n",
        "    run_len = counts[run_id]\n",
        "\n",
        "    flag = np.full(len(g), \"OK\", dtype=object)\n",
        "    warn_mask = (run_len >= warn_needed) & (run_len < crit_needed)\n",
        "    crit_mask = (run_len >= crit_needed)\n",
        "    flag[warn_mask] = \"WARN_flatline\"\n",
        "    flag[crit_mask] = \"CRIT_flatline\"\n",
        "\n",
        "    return pd.Series(flag, index=g.index, name=f\"qc_st_flatline_{suffix}\")\n",
        "\n",
        "# ---- Apply flatline detection per node for each probe (use *_clean) ----\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True)\n",
        "\n",
        "for col in ST_COLS:\n",
        "    suffix    = \"0\" if col.endswith(\"_0\") else \"1\"\n",
        "    clean_col = ST_CLEAN[col]\n",
        "    flag_name = f\"qc_st_flatline_{suffix}\"\n",
        "    df[flag_name] = \"OK\"\n",
        "    flags = (\n",
        "        df.groupby(NODE_COL, group_keys=False)[[TIMESTAMP_COL, clean_col]]\n",
        "          .apply(lambda g: flag_flatlines_for_probe(g, clean_col, suffix))\n",
        "    )\n",
        "    df.loc[flags.index, flag_name] = flags.values\n",
        "\n",
        "# ---- Update rollups and clean series ----\n",
        "for col in ST_COLS:\n",
        "    suffix      = \"0\" if col.endswith(\"_0\") else \"1\"\n",
        "    flat_col    = f\"qc_st_flatline_{suffix}\"\n",
        "    anycrit_col = f\"qc_st_anycrit_{suffix}\"\n",
        "    anywarn_col = f\"qc_st_anywarn_{suffix}\"\n",
        "    clean_col   = ST_CLEAN[col]\n",
        "\n",
        "    if anycrit_col not in df.columns:\n",
        "        df[anycrit_col] = False\n",
        "    if anywarn_col not in df.columns:\n",
        "        df[anywarn_col] = False\n",
        "\n",
        "    prev_anycrit = df[anycrit_col].astype(bool)\n",
        "    prev_anywarn = df[anywarn_col].astype(bool)\n",
        "\n",
        "    new_crit = df[flat_col].eq(\"CRIT_flatline\")\n",
        "    new_warn = df[flat_col].eq(\"WARN_flatline\")\n",
        "\n",
        "    updated_anycrit = prev_anycrit | new_crit\n",
        "    updated_anywarn = (prev_anywarn | new_warn) & (~updated_anycrit)\n",
        "\n",
        "    df[anycrit_col] = updated_anycrit\n",
        "    df[anywarn_col] = updated_anywarn\n",
        "\n",
        "    # Mask CRIT flatlines in the clean series (WARN stays visible)\n",
        "    df.loc[new_crit, clean_col] = np.nan\n",
        "\n",
        "# ---- Deterministic ordering & save ----\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage6b_soiltemp_flatline\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# ---- Summaries ----\n",
        "def summarize_flags(df, suffix):\n",
        "    col = f\"qc_st_flatline_{suffix}\"\n",
        "    return (df[col].value_counts(dropna=False)\n",
        "              .rename_axis(\"flag\").reset_index(name=f\"n_rows_{suffix}\"))\n",
        "\n",
        "sum0 = summarize_flags(df, \"0\")\n",
        "sum1 = summarize_flags(df, \"1\")\n",
        "summary = sum0.merge(sum1, on=\"flag\", how=\"outer\").fillna(0).sort_values(\"flag\")\n",
        "sum_path = next_versioned_csv(OUT_DIR, \"stage6b_soiltemp_flatline_counts_overall\")\n",
        "summary.to_csv(sum_path, index=False)\n",
        "print(\"Wrote overall counts:\", sum_path)\n",
        "print(\"\\nOverall soil-temp flatline counts:\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# Per-node duration estimate (hours), using median dt per node\n",
        "def pernode_duration_estimate(df, probe_col, flat_col):\n",
        "    med_dt = (df.groupby(NODE_COL)[TIMESTAMP_COL]\n",
        "                .apply(lambda s: median_sampling_minutes(s.sort_values()))\n",
        "                .rename(\"dt_min\"))\n",
        "    tmp = df[[NODE_COL, flat_col]].copy()\n",
        "    tmp[\"n\"] = 1\n",
        "    tmp = tmp.groupby([NODE_COL, flat_col])[\"n\"].sum().reset_index()\n",
        "    tmp = tmp.merge(med_dt, on=NODE_COL, how=\"left\")\n",
        "    tmp[\"hours\"] = (tmp[\"n\"] * tmp[\"dt_min\"]) / 60.0\n",
        "    tmp[\"probe\"] = probe_col\n",
        "    return tmp[[NODE_COL, \"probe\", flat_col, \"n\", \"hours\"]]\n",
        "\n",
        "pernode0 = pernode_duration_estimate(df, \"probe0\", \"qc_st_flatline_0\")\n",
        "pernode1 = pernode_duration_estimate(df, \"probe1\", \"qc_st_flatline_1\")\n",
        "pernode = pd.concat([pernode0, pernode1], ignore_index=True)\n",
        "pernode_path = next_versioned_csv(OUT_DIR, \"stage6b_soiltemp_flatline_counts_pernode\")\n",
        "pernode.to_csv(pernode_path, index=False)\n",
        "print(\"Wrote per-node duration estimates:\", pernode_path)\n",
        "\n",
        "# Breadcrumbs\n",
        "tmin = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).min()\n",
        "tmax = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).max()\n",
        "print(\"\\nTime span (dataset, from ts_ns):\", tmin, \"→\", tmax)\n",
        "print(\"Rows in/out (should match input):\", n_in, \"→\", len(df))\n",
        "print(\"Nodes:\", sorted(df[NODE_COL].dropna().unique().tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz1ZEjDcEbYY",
        "outputId": "425fb250-91c4-48c7-b921-6e0cd2af8e88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: af_clean_v01/stage6a_soiltemp_range_v001.csv\n",
            "[carry] Brought 16 SM cols from 5PD: stage5pd_sm_pairdiverge_v001.csv\n",
            "Wrote: af_clean_v01/stage6b_soiltemp_flatline_v001.csv\n",
            "Wrote overall counts: af_clean_v01/stage6b_soiltemp_flatline_counts_overall_v001.csv\n",
            "\n",
            "Overall soil-temp flatline counts:\n",
            "flag  n_rows_0  n_rows_1\n",
            "  OK    483787    483787\n",
            "Wrote per-node duration estimates: af_clean_v01/stage6b_soiltemp_flatline_counts_pernode_v001.csv\n",
            "\n",
            "Time span (dataset, from ts_ns): 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "Rows in/out (should match input): 483787 → 483787\n",
            "Nodes: ['node_176', 'node_179', 'node_181', 'node_182', 'node_183', 'node_184', 'node_186', 'node_187', 'node_189']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6PD — Soil Temperature Pair-Divergence**\n",
        "* Goal: Detect and mask soil-temperature spikes/divergences (both probes) using sibling/neighbor references and self-based checks if refs are weak/missing.\n",
        "* Inputs: Latest earlier-stage CSV (auto-picked) + optional neighbors_manual.csv; expects ground_temp_{0,1} (and _clean if present) with timestamps.\n",
        "* Reference choice (per node, per probe):\n",
        "* Prefer sibling probe when it’s present and not previously CRIT.\n",
        "* Else build a statistical neighbor composite (robust median/IQR scaling, Spearman screen, median combine), require min neighbors + corr gate.\n",
        "* Anomaly signals (any one can trigger):\n",
        "* Ref z+gap: big residual vs reference (stricter for sibling; slightly looser for neighbors).\n",
        "* Spikes: fast per-sample jumps + large gap (uses ref gap, falls back to self gap).\n",
        "* Self anomalies: large z and gap vs own rolling median, or large rate vs self.\n",
        "* Cross-depth conflict: unusual ST0–ST1 difference; votes the farther probe as outlier.\n",
        "* Robust stats: Causal 7-day rolling median + MAD-σ; duplicate timestamps collapsed by median; zeros in σ guarded with small EPS.\n",
        "* Consolidation: Require consecutive runs (edge-relaxed), add a window cover rule, then fill tiny gaps and dilate slightly to avoid pepper noise.\n",
        "* Index safety: Keeps a DatetimeIndex through all steps so masks align with rows (prevents silent all-False).\n",
        "* Mask & flags: Write CRIT_pair_diverge to qc_st_pairdiverge_{0,1}, mask ground_temp_{0,1}_clean, and update qc_st_anycrit_{0,1}; store provenance (src, nref, corr).\n",
        "* *Outputs*: Versioned CSV af_clean_v01/stage6pd_soiltemp_pairdiverge_vNNN.csv + per-probe masked share summary and gate settings for audit."
      ],
      "metadata": {
        "id": "ddEqVCWzAPVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 6PD — Soil Temperature Pair-Divergence (AGGRESSIVE + self fallback, index-safe)\n",
        "# =========================\n",
        "# Masks spikes/divergences even when refs are missing by using self-based anomaly checks.\n",
        "# Maintains DatetimeIndex across all steps to avoid silent \"all False\" outcomes.\n",
        "\n",
        "import os, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR  = Path(\"af_clean_v01\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_STEM = \"stage6pd_soiltemp_pairdiverge_v\"\n",
        "\n",
        "IN_SEARCH = [\n",
        "    \"af_clean_v01/stage6b_soiltemp_flatline_v*.csv\",\n",
        "    \"af_clean_v01/stage6a_soiltemp_range_v*.csv\",\n",
        "    \"af_clean_v01/stage5pd_sm_pairdiverge_v*.csv\",\n",
        "    \"af_clean_v01/stage5r_sm_regimes_v*.csv\",\n",
        "    \"af_clean_v01/stage4_sm_flatline_v*.csv\",\n",
        "    \"af_clean_v01/stage3_sm_knownbroken_v*.csv\",\n",
        "    \"af_clean_v01/stage2_soilmoisture_range_v*.csv\",\n",
        "    \"af_clean_v01/stage1_parsed_v*.csv\",\n",
        "    \"analysis_ready_v*.csv\",\n",
        "]\n",
        "\n",
        "# -------- Aggressive gates (tuned to \"mask most of it\") --------\n",
        "ROLL_WIN, ROLL_MINP, ROLL_CENTER = \"7D\", 12, False\n",
        "EPS_SIG = 1e-6\n",
        "\n",
        "# z+gap vs sibling / neighbor\n",
        "Z_SIB, GAP_SIB = 3.5, 1.6\n",
        "Z_NEI, GAP_NEI = 4.5, 2.0\n",
        "\n",
        "# spike (fast bursts)\n",
        "SPIKE_RATE = 2.5       # °C per sample\n",
        "SPIKE_GAP  = 2.0       # °C vs ref (or self baseline if ref missing)\n",
        "\n",
        "# cross-depth (difference between ST0 and ST1)\n",
        "Z_DIFF, GAP_DIFF = 3.5, 1.2\n",
        "\n",
        "# self-based anomalies (fallback path when refs are weak/missing)\n",
        "SELF_Z    = 3.0        # z vs own rolling median\n",
        "SELF_GAP  = 1.8        # °C vs own rolling median\n",
        "SELF_RATE = 2.2        # °C per sample\n",
        "\n",
        "# consolidation\n",
        "MIN_RUN     = 6\n",
        "EDGE_RELAX  = 0.3\n",
        "WIN_SIZE    = 24\n",
        "WIN_COVER   = 0.25\n",
        "FILL_GAP    = 4\n",
        "DILATE_STEPS= 1\n",
        "\n",
        "NEIGH_CORR_GATE, USE_NEIGH_CLEAN = 0.4, True\n",
        "\n",
        "# -------- helpers (index-preserving) --------\n",
        "def _pick_latest():\n",
        "    for pat in IN_SEARCH:\n",
        "        hits = sorted(glob.glob(pat))\n",
        "        if hits:\n",
        "            def _ver(p):\n",
        "                m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(p))\n",
        "                return int(m.group(1)) if m else -1\n",
        "            best = max(hits, key=_ver)\n",
        "            return best if _ver(best) >= 0 else hits[-1]\n",
        "    raise FileNotFoundError(\"No earlier stage file found for 6PD.\")\n",
        "\n",
        "def _next_versioned(stem):\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = OUT_DIR / f\"{stem}{i:03d}.csv\"\n",
        "        if not p.exists(): return p\n",
        "        i += 1\n",
        "\n",
        "def _to_dt(df):\n",
        "    if \"timestamp\" in df.columns:     return pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "    if \"timestamp_iso\" in df.columns: return pd.to_datetime(df[\"timestamp_iso\"], utc=True, errors=\"coerce\")\n",
        "    return pd.to_datetime(pd.to_numeric(df.get(\"ts_ns\", np.nan), errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "\n",
        "def _dup_med(series):\n",
        "    s = pd.Series(series).dropna()\n",
        "    if s.index.duplicated().any():\n",
        "        s = s.groupby(level=0).median(numeric_only=True)\n",
        "    return s.sort_index()\n",
        "\n",
        "def _roll_med(s):\n",
        "    s = pd.Series(s)\n",
        "    return s.rolling(ROLL_WIN, min_periods=ROLL_MINP, center=ROLL_CENTER).median()\n",
        "\n",
        "def _roll_mad_sigma(s):\n",
        "    s = pd.Series(s)\n",
        "    dev = (s - _roll_med(s)).abs()\n",
        "    mad = dev.rolling(ROLL_WIN, min_periods=ROLL_MINP, center=ROLL_CENTER).median()\n",
        "    sig = 1.4826 * mad\n",
        "    return sig.replace(0, np.nan)\n",
        "\n",
        "def _as_bool(x):\n",
        "    s = pd.Series(x)\n",
        "    if s.dtype != bool:\n",
        "        s = s.astype(\"boolean\").fillna(False).astype(bool)\n",
        "    return s\n",
        "\n",
        "def _contig(mask, min_len=MIN_RUN, edge_relax=EDGE_RELAX):\n",
        "    m = _as_bool(mask)\n",
        "    idx = m.index; v = m.values; n = len(v)\n",
        "    out = np.zeros(n, dtype=bool); i = 0\n",
        "    while i < n:\n",
        "        if v[i]:\n",
        "            j = i + 1\n",
        "            while j < n and v[j]: j += 1\n",
        "            seg = j - i\n",
        "            need = min_len if (i>0 and j<n) else int(np.ceil(edge_relax * min_len))\n",
        "            if seg >= need: out[i:j] = True\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return pd.Series(out, index=idx)\n",
        "\n",
        "def _window(mask, win=WIN_SIZE, cover=WIN_COVER):\n",
        "    m = _as_bool(mask)\n",
        "    hits = m.astype(int).rolling(win, min_periods=max(2, win//4)).sum()\n",
        "    res = (hits >= max(1, int(np.ceil(cover * win))))\n",
        "    return res.reindex(m.index).fillna(False).astype(bool)\n",
        "\n",
        "def _fill_gaps(mask, max_gap=FILL_GAP):\n",
        "    m = _as_bool(mask); idx = m.index\n",
        "    v = m.values; out = v.copy()\n",
        "    n = len(v); i = 0\n",
        "    while i < n:\n",
        "        if v[i]:\n",
        "            j = i + 1\n",
        "            while j < n and v[j]: j += 1\n",
        "            k = j\n",
        "            while k < n and (not v[k]) and (k - j) <= max_gap: k += 1\n",
        "            if k < n and v[k] and (k - j) <= max_gap: out[j:k] = True; i = k\n",
        "            else: i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return pd.Series(out, index=idx)\n",
        "\n",
        "def _dilate(mask, steps=DILATE_STEPS):\n",
        "    m = _as_bool(mask); idx = m.index\n",
        "    v = m.values\n",
        "    if steps <= 0: return m\n",
        "    out = v.copy()\n",
        "    for _ in range(steps):\n",
        "        out = np.logical_or(out, np.r_[False, out[:-1]] | np.r_[out[1:], False])\n",
        "    return pd.Series(out, index=idx)\n",
        "\n",
        "# -------- neighbors --------\n",
        "NEIGH_PATH = \"neighbors_manual.csv\"\n",
        "def _load_neigh():\n",
        "    try:\n",
        "        nm = pd.read_csv(NEIGH_PATH)\n",
        "    except FileNotFoundError:\n",
        "        return None, False\n",
        "    nm.columns = [c.strip().lower() for c in nm.columns]\n",
        "    have_st = {\"node_id\",\"st0_neighbors\",\"st1_neighbors\"}.issubset(nm.columns)\n",
        "    have_sm = {\"node_id\",\"sm0_neighbors\",\"sm1_neighbors\"}.issubset(nm.columns)\n",
        "    if not have_st and not have_sm:\n",
        "        raise ValueError(\"neighbors_manual.csv needs st0/st1 or sm0/sm1 neighbor lists.\")\n",
        "    return nm, have_st\n",
        "\n",
        "def _parse_ids(s):\n",
        "    if pd.isna(s): return []\n",
        "    return [x.strip() for x in re.split(r\"[|,;]+\", str(s)) if x.strip()]\n",
        "\n",
        "def _series_for(df_all, node_col, nid, probe, use_clean=True):\n",
        "    sub = df_all[df_all[node_col].astype(str) == str(nid)]\n",
        "    if sub.empty: return pd.Series(dtype=\"float64\")\n",
        "    idx = pd.to_datetime(sub[\"__t\"], utc=True, errors=\"coerce\")\n",
        "    base = f\"ground_temp_{probe}\"\n",
        "    col  = base + (\"_clean\" if use_clean and f\"{base}_clean\" in sub.columns else \"\")\n",
        "    vals = pd.to_numeric(sub[col], errors=\"coerce\")\n",
        "    ser  = pd.Series(vals.values, index=idx).dropna()\n",
        "    if ser.index.duplicated().any():\n",
        "        ser = ser.groupby(level=0).median(numeric_only=True)\n",
        "    return ser.sort_index()\n",
        "\n",
        "def _robust_center_scale(s, horizon_days=90):\n",
        "    if s.empty: return (np.nan, np.nan)\n",
        "    tmax = s.index.max()\n",
        "    recent = s[s.index >= (tmax - pd.Timedelta(days=horizon_days))]\n",
        "    base = recent if recent.notna().sum() >= 48 else s\n",
        "    med = base.median()\n",
        "    iqr = base.quantile(0.75) - base.quantile(0.25)\n",
        "    if not np.isfinite(iqr) or iqr == 0: iqr = np.nan\n",
        "    return med, iqr\n",
        "\n",
        "def build_neighbor_ref(df_all, node_col, target_node, probe, t_index, neigh_tuple,\n",
        "                       prefer_st=True, use_clean=USE_NEIGH_CLEAN, min_neighbors=2, corr_gate=NEIGH_CORR_GATE):\n",
        "    if neigh_tuple[0] is None: return None, \"none\", \"low\", 0, np.nan\n",
        "    nm, have_st = neigh_tuple\n",
        "    row = nm[nm[\"node_id\"].astype(str) == str(target_node)]\n",
        "    if row.empty: return None, \"none\", \"low\", 0, np.nan\n",
        "    field = (\"st0_neighbors\" if probe==\"0\" else \"st1_neighbors\") if (prefer_st and have_st) \\\n",
        "            else (\"sm0_neighbors\" if probe==\"0\" else \"sm1_neighbors\")\n",
        "    neigh_ids = _parse_ids(row.iloc[0].get(field, np.nan))\n",
        "    if not neigh_ids: return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    tgt = _series_for(df_all, node_col, target_node, probe, use_clean=False).reindex(t_index)\n",
        "    if tgt.notna().sum() < 80: return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    t_med, t_iqr = _robust_center_scale(tgt)\n",
        "    if not np.isfinite(t_iqr): return None, \"none\", \"low\", 0, np.nan\n",
        "\n",
        "    z_cols = {}\n",
        "    for nid in neigh_ids:\n",
        "        s = _series_for(df_all, node_col, nid, probe, use_clean=use_clean).reindex(t_index)\n",
        "        if s.notna().sum() < 80: continue\n",
        "        n_med, n_iqr = _robust_center_scale(s)\n",
        "        if not np.isfinite(n_iqr): continue\n",
        "        z = (s - n_med) / n_iqr\n",
        "        ok = tgt.notna() & z.notna()\n",
        "        if ok.sum() < 60: continue\n",
        "        corr = pd.Series(tgt[ok]).corr(pd.Series(z[ok]), method=\"spearman\")\n",
        "        if not np.isfinite(corr) or corr < 0.2: continue\n",
        "        z_cols[nid] = z\n",
        "\n",
        "    if len(z_cols) < min_neighbors: return None, \"none\", \"low\", len(z_cols), np.nan\n",
        "\n",
        "    Z = pd.DataFrame(z_cols, index=t_index)\n",
        "    z_ref = Z.median(axis=1, skipna=True)\n",
        "    ref   = t_med + z_ref * t_iqr\n",
        "\n",
        "    ok = tgt.notna() & ref.notna()\n",
        "    roll_corr = (pd.Series(tgt[ok]).rolling(48, min_periods=24).corr(pd.Series(ref[ok]))).median()\n",
        "    conf = \"med\" if (np.isfinite(roll_corr) and roll_corr >= corr_gate) else \"low\"\n",
        "    return ref, \"statistical\", conf, len(z_cols), roll_corr\n",
        "\n",
        "# -------- load input --------\n",
        "src = _pick_latest()\n",
        "df  = pd.read_csv(src, low_memory=False)\n",
        "print(f\"[6PD-ST AGG] Input: {src} | rows={len(df):,}\")\n",
        "\n",
        "df[\"__t\"] = _to_dt(df)\n",
        "df = df.dropna(subset=[\"__t\"]).copy()\n",
        "\n",
        "node_col = None\n",
        "for c in [\"node_id\",\"node\",\"station\",\"device\",\"site\"]:\n",
        "    if c in df.columns: node_col = c; break\n",
        "if node_col is None:\n",
        "    df[\"__node\"] = \"__all__\"; node_col = \"__node\"\n",
        "\n",
        "# ensure *_clean exist\n",
        "for suf in (\"0\",\"1\"):\n",
        "    base = f\"ground_temp_{suf}\"\n",
        "    if f\"{base}_clean\" not in df.columns:\n",
        "        df[f\"{base}_clean\"] = pd.to_numeric(df.get(base, np.nan), errors=\"coerce\")\n",
        "\n",
        "neigh_tuple = _load_neigh()\n",
        "\n",
        "# init outputs\n",
        "for suf in (\"0\",\"1\"):\n",
        "    df[f\"qc_st_pairdiverge_{suf}\"]     = \"OK\"\n",
        "    df[f\"qc_st_pairdiverge_src_{suf}\"] = \"\"\n",
        "    df[f\"qc_st_pairdiverge_nref_{suf}\"]= 0\n",
        "    df[f\"qc_st_pairdiverge_corr_{suf}\"]= np.nan\n",
        "    anyc = f\"qc_st_anycrit_{suf}\"\n",
        "    df[anyc] = _as_bool(df[anyc]) if anyc in df.columns else False\n",
        "\n",
        "masked, raw = {\"0\":0,\"1\":0}, {\"0\":0,\"1\":0}\n",
        "\n",
        "# -------- per-node pass --------\n",
        "for node, g in df.sort_values([node_col, \"__t\"]).groupby(df[node_col].astype(str), sort=False):\n",
        "    t_rows = pd.to_datetime(g[\"__t\"])\n",
        "    t_idx  = pd.Index(t_rows.sort_values().unique())\n",
        "\n",
        "    # raw series\n",
        "    t0r = _dup_med(pd.Series(pd.to_numeric(g.get(\"ground_temp_0\"), errors=\"coerce\").values, index=t_rows)).reindex(t_idx)\n",
        "    t1r = _dup_med(pd.Series(pd.to_numeric(g.get(\"ground_temp_1\"), errors=\"coerce\").values, index=t_rows)).reindex(t_idx)\n",
        "    if (t0r.notna().sum() < 50) and (t1r.notna().sum() < 50):\n",
        "        continue\n",
        "\n",
        "    # don't trust sibling if earlier CRIT\n",
        "    def _pre_anycrit(sub, suf):\n",
        "        idx = pd.to_datetime(sub[\"__t\"])\n",
        "        def f(col):\n",
        "            if col not in sub.columns: return pd.Series(False, index=idx)\n",
        "            s = sub[col].astype(str)\n",
        "            return pd.Series(s.str.startswith(\"CRIT\", na=False).values, index=idx)\n",
        "        cols = [f\"qc_st_range_{suf}\", f\"qc_st_flatline_{suf}\", f\"qc_st_known_broken_{suf}\"]\n",
        "        m = pd.Series(False, index=idx)\n",
        "        for c in cols: m = m | f(c)\n",
        "        if m.index.duplicated().any(): m = m.groupby(level=0).max()\n",
        "        return m\n",
        "\n",
        "    pre0 = _pre_anycrit(g, \"0\").reindex(t_idx).fillna(False)\n",
        "    pre1 = _pre_anycrit(g, \"1\").reindex(t_idx).fillna(False)\n",
        "    sib_ok_for_0 = t1r.notna() & (~pre1)\n",
        "    sib_ok_for_1 = t0r.notna() & (~pre0)\n",
        "\n",
        "    # neighbors\n",
        "    ref0_nei, _, conf0, nref0, rcorr0 = build_neighbor_ref(df, node_col, node, \"0\", t_idx, neigh_tuple, prefer_st=True)\n",
        "    ref1_nei, _, conf1, nref1, rcorr1 = build_neighbor_ref(df, node_col, node, \"1\", t_idx, neigh_tuple, prefer_st=True)\n",
        "    ref0_nei = ref0_nei if isinstance(ref0_nei, pd.Series) else pd.Series(np.nan, index=t_idx)\n",
        "    ref1_nei = ref1_nei if isinstance(ref1_nei, pd.Series) else pd.Series(np.nan, index=t_idx)\n",
        "\n",
        "    # preferred refs\n",
        "    ref0 = t1r.where(sib_ok_for_0).combine_first(ref0_nei)\n",
        "    ref1 = t0r.where(sib_ok_for_1).combine_first(ref1_nei)\n",
        "    src0_series = pd.Series(np.where(sib_ok_for_0, \"sibling\", np.where(ref0_nei.notna(), \"statistical\", \"none\")), index=t_idx)\n",
        "    src1_series = pd.Series(np.where(sib_ok_for_1, \"sibling\", np.where(ref1_nei.notna(), \"statistical\", \"none\")), index=t_idx)\n",
        "\n",
        "    # diffs vs ref\n",
        "    d0 = (t0r - ref0)\n",
        "    d1 = (t1r - ref1)\n",
        "\n",
        "    # robust z+gap vs ref (fill sig NaNs; keep med NaNs)\n",
        "    med0 = _roll_med(d0); sig0 = _roll_mad_sigma(d0).fillna(EPS_SIG)\n",
        "    med1 = _roll_med(d1); sig1 = _roll_mad_sigma(d1).fillna(EPS_SIG)\n",
        "    z0   = (d0 - med0).abs()/sig0; gap0 = (d0 - med0).abs()\n",
        "    z1   = (d1 - med1).abs()/sig1; gap1 = (d1 - med1).abs()\n",
        "\n",
        "    base0 = ((src0_series.eq(\"sibling\")     & (z0 >= Z_SIB) & (gap0 >= GAP_SIB)) |\n",
        "             (src0_series.eq(\"statistical\") & (conf0 == \"med\") & (z0 >= Z_NEI) & (gap0 >= GAP_NEI)))\n",
        "    base1 = ((src1_series.eq(\"sibling\")     & (z1 >= Z_SIB) & (gap1 >= GAP_SIB)) |\n",
        "             (src1_series.eq(\"statistical\") & (conf1 == \"med\") & (z1 >= Z_NEI) & (gap1 >= GAP_NEI)))\n",
        "\n",
        "    base0 = _as_bool(base0); base1 = _as_bool(base1)\n",
        "\n",
        "    # own baseline (self) stats\n",
        "    self_med0 = _roll_med(t0r); self_sig0 = _roll_mad_sigma(t0r).fillna(EPS_SIG)\n",
        "    self_med1 = _roll_med(t1r); self_sig1 = _roll_mad_sigma(t1r).fillna(EPS_SIG)\n",
        "    self_gap0 = (t0r - self_med0).abs();  self_z0 = self_gap0 / self_sig0\n",
        "    self_gap1 = (t1r - self_med1).abs();  self_z1 = self_gap1 / self_sig1\n",
        "    r0 = pd.Series(t0r).diff().abs();     r1 = pd.Series(t1r).diff().abs()\n",
        "\n",
        "    # spike: prefer ref gap; if ref missing, fall back to self gap\n",
        "    spike_gap0 = (d0.abs()).fillna(self_gap0)\n",
        "    spike_gap1 = (d1.abs()).fillna(self_gap1)\n",
        "    spike0 = _as_bool((r0 >= SPIKE_RATE) & (spike_gap0 >= SPIKE_GAP))\n",
        "    spike1 = _as_bool((r1 >= SPIKE_RATE) & (spike_gap1 >= SPIKE_GAP))\n",
        "\n",
        "    # strong self anomalies (even if ref missing)\n",
        "    self_anom0 = _as_bool((self_z0 >= SELF_Z) & (self_gap0 >= SELF_GAP) | (r0 >= SELF_RATE) & (self_gap0 >= SELF_GAP))\n",
        "    self_anom1 = _as_bool((self_z1 >= SELF_Z) & (self_gap1 >= SELF_GAP) | (r1 >= SELF_RATE) & (self_gap1 >= SELF_GAP))\n",
        "\n",
        "    # cross-depth: assign outlier to the farther probe\n",
        "    diff01 = t0r - t1r\n",
        "    mD = _roll_med(diff01); sD = _roll_mad_sigma(diff01).fillna(EPS_SIG)\n",
        "    bigD = _as_bool(((diff01 - mD).abs()/sD >= Z_DIFF) & ((diff01 - mD).abs() >= GAP_DIFF))\n",
        "    vote0 = _as_bool(bigD & (spike_gap0 >= spike_gap1))\n",
        "    vote1 = _as_bool(bigD & (spike_gap1 >  spike_gap0))\n",
        "\n",
        "    # AGGRESSIVE OR: base OR spike OR self_anom OR cross-depth\n",
        "    cand0 = _as_bool(base0 | spike0 | self_anom0 | vote0)\n",
        "    cand1 = _as_bool(base1 | spike1 | self_anom1 | vote1)\n",
        "\n",
        "    # consolidate\n",
        "    crit0 = _contig(cand0);   crit1 = _contig(cand1)\n",
        "    win0  = _window(cand0);   win1  = _window(cand1)\n",
        "    final0 = _dilate(_fill_gaps(crit0 | win0, FILL_GAP), DILATE_STEPS)\n",
        "    final1 = _dilate(_fill_gaps(crit1 | win1, FILL_GAP), DILATE_STEPS)\n",
        "\n",
        "    # map to rows (DatetimeIndex preserved)\n",
        "    m0_rows = final0.reindex(t_rows).fillna(False).to_numpy()\n",
        "    m1_rows = final1.reindex(t_rows).fillna(False).to_numpy()\n",
        "    idx_rows = g.index\n",
        "\n",
        "    # write masks + flags\n",
        "    df.loc[idx_rows, \"qc_st_pairdiverge_0\"] = np.where(m0_rows, \"CRIT_pair_diverge\", df.loc[idx_rows, \"qc_st_pairdiverge_0\"])\n",
        "    df.loc[idx_rows, \"qc_st_pairdiverge_1\"] = np.where(m1_rows, \"CRIT_pair_diverge\", df.loc[idx_rows, \"qc_st_pairdiverge_1\"])\n",
        "    df.loc[idx_rows, \"ground_temp_0_clean\"] = pd.to_numeric(df.loc[idx_rows, \"ground_temp_0_clean\"], errors=\"coerce\").mask(m0_rows)\n",
        "    df.loc[idx_rows, \"ground_temp_1_clean\"] = pd.to_numeric(df.loc[idx_rows, \"ground_temp_1_clean\"], errors=\"coerce\").mask(m1_rows)\n",
        "\n",
        "    df.loc[idx_rows, \"qc_st_anycrit_0\"] = _as_bool(df.loc[idx_rows, \"qc_st_anycrit_0\"] | m0_rows)\n",
        "    df.loc[idx_rows, \"qc_st_anycrit_1\"] = _as_bool(df.loc[idx_rows, \"qc_st_anycrit_1\"] | m1_rows)\n",
        "\n",
        "    # lightweight per-node debug summary\n",
        "    n0, n1 = int(cand0.sum()), int(cand1.sum())\n",
        "    f0, f1 = int(final0.sum()), int(final1.sum())\n",
        "    print(f\"[node {node}] cand0={n0} final0={f0} | cand1={n1} final1={f1}\")\n",
        "\n",
        "    masked[\"0\"] += int(m0_rows.sum()); raw[\"0\"] += int(pd.to_numeric(g.get(\"ground_temp_0\"), errors=\"coerce\").notna().sum())\n",
        "    masked[\"1\"] += int(m1_rows.sum()); raw[\"1\"] += int(pd.to_numeric(g.get(\"ground_temp_1\"), errors=\"coerce\").notna().sum())\n",
        "\n",
        "# -------- save --------\n",
        "out = _next_versioned(OUT_STEM)\n",
        "df.to_csv(out, index=False)\n",
        "print(f\"[6PD-ST AGG] wrote: {out}\")\n",
        "for k in (\"0\",\"1\"):\n",
        "    print(f\"[6PD-ST AGG] probe{k}: masked {masked[k]}/{raw[k]} = {masked[k]/max(raw[k],1):.2%}\")\n",
        "print(\"[6PD-ST AGG] Gates:\",\n",
        "      f\"Z_SIB={Z_SIB}, GAP_SIB={GAP_SIB}; Z_NEI={Z_NEI}, GAP_NEI={GAP_NEI};\",\n",
        "      f\"SPIKE_RATE={SPIKE_RATE}, SPIKE_GAP={SPIKE_GAP}; SELF_Z={SELF_Z}, SELF_GAP={SELF_GAP}, SELF_RATE={SELF_RATE};\",\n",
        "      f\"Z_DIFF={Z_DIFF}, GAP_DIFF={GAP_DIFF}; MIN_RUN={MIN_RUN}, WIN_COVER={WIN_COVER}, FILL_GAP={FILL_GAP}, DILATE_STEPS={DILATE_STEPS}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVrnyirNm0a_",
        "outputId": "27d0c81f-5571-456b-8dad-0958c91525e8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6PD-ST AGG] Input: af_clean_v01/stage6b_soiltemp_flatline_v001.csv | rows=483,787\n",
            "[node node_176] cand0=1381 final0=1772 | cand1=1799 final1=2170\n",
            "[node node_179] cand0=2668 final0=3456 | cand1=1657 final1=2119\n",
            "[node node_181] cand0=2256 final0=3086 | cand1=3261 final1=4633\n",
            "[node node_182] cand0=1456 final0=1962 | cand1=1281 final1=1850\n",
            "[node node_183] cand0=1172 final0=1488 | cand1=1228 final1=1581\n",
            "[node node_184] cand0=6919 final0=11511 | cand1=961 final1=1207\n",
            "[node node_186] cand0=1323 final0=1719 | cand1=2924 final1=3635\n",
            "[node node_187] cand0=8048 final0=11817 | cand1=1297 final1=1741\n",
            "[node node_189] cand0=3104 final0=3765 | cand1=2869 final1=2691\n",
            "[6PD-ST AGG] wrote: af_clean_v01/stage6pd_soiltemp_pairdiverge_v008.csv\n",
            "[6PD-ST AGG] probe0: masked 40576/483787 = 8.39%\n",
            "[6PD-ST AGG] probe1: masked 21627/483787 = 4.47%\n",
            "[6PD-ST AGG] Gates: Z_SIB=3.5, GAP_SIB=1.6; Z_NEI=4.5, GAP_NEI=2.0; SPIKE_RATE=2.5, SPIKE_GAP=2.0; SELF_Z=3.0, SELF_GAP=1.8, SELF_RATE=2.2; Z_DIFF=3.5, GAP_DIFF=1.2; MIN_RUN=6, WIN_COVER=0.25, FILL_GAP=4, DILATE_STEPS=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 (Air QC (self-comparison for T/RH, farm-wide for p))**\n",
        "* Loads latest table (prefers Stage 6c/7/8 as before), rebuilds timestamp from ts_ns.\n",
        "* Air temperature & humidity (per node):\n",
        "* For each node, compute a 24-hour rolling median baseline. * Flag a row as an outlier when its absolute deviation from that baseline exceeds a strict absolute threshold AND a robust relative threshold (k×rolling MAD, with fallback). Promote to WARN_outlier only if this condition is sustained ≥ 60 minutes continuously for that node.\n",
        "* Pressure (farm-wide): Compare each row to the same-time farm median; flag WARN_outlier if deviation ≥ 4 hPa is sustained ≥ 30 minutes.\n",
        "* No masking. Only adds: qc_air_selfcheck_temp, qc_air_selfcheck_hum, qc_air_crosscheck_pres ∈ {OK, WARN_outlier}.\n",
        "* Saves a versioned file stage7r_air_selfqc_v###.csv + summaries."
      ],
      "metadata": {
        "id": "0P_xLZmF_GBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== Step 7a (hardened): Air Temperature QC + masking ==============\n",
        "# - Picks an input that REALLY has 'temperature' (skips *counts/*summary/*audit*)\n",
        "# - WARN if T in [-25, -20) or (45, 50]; CRIT if T < -25 or T > 50; missing -> CRIT_missing\n",
        "# - Flatline: ≥12 identical consecutive samples per node -> WARN_flatline\n",
        "# - Time: use timestamp/timestamp_iso/ts_ns if present; else backfill via row_id from a donor; else synthesize.\n",
        "# - NEW: carry forward soil MOISTURE (5PD) and soil TEMP (6PD) clean/flags so masks persist.\n",
        "\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR, OUT_STEM = \"af_clean_v01\", \"stage7a_air_temp_qc_v\"\n",
        "MIN_FLAT_RUN = 12  # 12 consecutive identical samples\n",
        "\n",
        "# Prefer later stages but FILTER to files that actually contain 'temperature'\n",
        "CANDIDATE_PATTERNS = [\n",
        "    # anything after air QC that might still contain temperature\n",
        "    \"af_clean_v01/stage8_*v*.csv\",\n",
        "    # air chain\n",
        "    \"af_clean_v01/stage7c_air_pres_qc_v*.csv\",\n",
        "    \"af_clean_v01/stage7b_air_hum_qc_v*.csv\",\n",
        "    # step 6 soil-temp (now includes all earlier cols in your pipeline)\n",
        "    \"af_clean_v01/stage6pd_soiltemp_pairdiverge_v*.csv\",  # ← NEW: prefer 6PD outputs if they carry temperature\n",
        "    \"af_clean_v01/stage6c_*v*.csv\",\n",
        "    \"af_clean_v01/stage6b_*v*.csv\",\n",
        "    \"af_clean_v01/stage6a_*v*.csv\",\n",
        "    # soil moisture stages, may still carry 'temperature' in many dumps\n",
        "    \"af_clean_v01/stage5pd_sm_pairdiverge_v*.csv\",\n",
        "    \"af_clean_v01/stage5r_sm_regimes_v*.csv\",\n",
        "    # fallbacks\n",
        "    \"af_clean_v01/stage1_parsed_v*.csv\",\n",
        "    \"af_clean_v01/analysis_ready_v*.csv\",\n",
        "    \"analysis_ready_v*.csv\",\n",
        "]\n",
        "DONOR_PATTERNS = [\n",
        "    \"af_clean_v01/stage8_*v*.csv\",\n",
        "    \"af_clean_v01/stage7*_*v*.csv\",\n",
        "    \"af_clean_v01/stage6*_*v*.csv\",\n",
        "    \"af_clean_v01/stage5pd_*v*.csv\",\n",
        "    \"af_clean_v01/stage5r_*v*.csv\",\n",
        "    \"af_clean_v01/analysis_ready_v*.csv\",\n",
        "    \"analysis_ready_v*.csv\",\n",
        "]\n",
        "\n",
        "def _ver(path):\n",
        "    m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(path))\n",
        "    return int(m.group(1)) if m else -1\n",
        "\n",
        "def _skip_name(p):\n",
        "    name = os.path.basename(p).lower()\n",
        "    return any(k in name for k in [\"count\", \"counts\", \"summary\", \"overall\", \"audit\"])\n",
        "\n",
        "def _pick_input(required_cols=(\"temperature\",)):\n",
        "    for pat in CANDIDATE_PATTERNS:\n",
        "        files = [p for p in sorted(glob.glob(pat), key=_ver, reverse=True) if not _skip_name(p)]\n",
        "        for p in files:\n",
        "            try:\n",
        "                hdr = pd.read_csv(p, nrows=0, low_memory=False)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if all(c in hdr.columns for c in required_cols):\n",
        "                print(f\"[7a] Input picked: {p}\")\n",
        "                return p\n",
        "    raise FileNotFoundError(\"[7a] No suitable input that contains 'temperature'.\")\n",
        "\n",
        "def _pick_donor(exclude=None):\n",
        "    for pat in DONOR_PATTERNS:\n",
        "        files = [p for p in sorted(glob.glob(pat), key=_ver, reverse=True) if p != exclude and not _skip_name(p)]\n",
        "        for p in files:\n",
        "            try:\n",
        "                hdr = pd.read_csv(p, nrows=0, low_memory=False)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if \"row_id\" in hdr.columns and any(c in hdr.columns for c in [\"timestamp\",\"timestamp_iso\",\"ts_ns\"]):\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "def _pick_node_col(df):\n",
        "    for c in [\"node_id\",\"node\",\"station_id\",\"station\",\"device_id\",\"device\",\"logger_id\",\"site\"]:\n",
        "        if c in df.columns:\n",
        "            print(f\"[7a] Using node column: {c!r}\")\n",
        "            return c\n",
        "    df[\"__node_synth\"] = \"__all__\"\n",
        "    print(\"[7a] No node column found → using synthetic group '__node_synth'.\")\n",
        "    return \"__node_synth\"\n",
        "\n",
        "def _ensure_time(df, current_path):\n",
        "    if \"timestamp\" in df.columns:\n",
        "        print(\"[7a] Found 'timestamp'.\")\n",
        "        return pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\"), False\n",
        "    if \"timestamp_iso\" in df.columns:\n",
        "        print(\"[7a] Found 'timestamp_iso'.\")\n",
        "        return pd.to_datetime(df[\"timestamp_iso\"], utc=True, errors=\"coerce\"), False\n",
        "    if \"ts_ns\" in df.columns:\n",
        "        print(\"[7a] Found 'ts_ns'.\")\n",
        "        return pd.to_datetime(pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\"), False\n",
        "\n",
        "    # backfill from donor by row_id\n",
        "    if \"row_id\" in df.columns:\n",
        "        donor = _pick_donor(exclude=current_path)\n",
        "        if donor:\n",
        "            hdr = pd.read_csv(donor, nrows=0, low_memory=False)\n",
        "            take = [\"row_id\"] + [c for c in [\"timestamp\",\"timestamp_iso\",\"ts_ns\"] if c in hdr.columns]\n",
        "            don = pd.read_csv(donor, usecols=take, low_memory=False)\n",
        "            merged = df.merge(don, on=\"row_id\", how=\"left\")\n",
        "            if \"timestamp\" in merged.columns:\n",
        "                print(f\"[7a] Backfilled time from donor: {donor} (timestamp).\")\n",
        "                return pd.to_datetime(merged[\"timestamp\"], utc=True, errors=\"coerce\"), False\n",
        "            if \"timestamp_iso\" in merged.columns:\n",
        "                print(f\"[7a] Backfilled time from donor: {donor} (timestamp_iso).\")\n",
        "                return pd.to_datetime(merged[\"timestamp_iso\"], utc=True, errors=\"coerce\"), False\n",
        "            if \"ts_ns\" in merged.columns:\n",
        "                print(f\"[7a] Backfilled time from donor: {donor} (ts_ns).\")\n",
        "                return pd.to_datetime(pd.to_numeric(merged[\"ts_ns\"], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\"), False\n",
        "\n",
        "    # synthesize (last resort)\n",
        "    ts = pd.date_range(\"2000-01-01\", periods=len(df), freq=\"s\", tz=\"UTC\")\n",
        "    print(\"[7a] WARNING: No time available; using synthetic monotonic timestamps.\")\n",
        "    return pd.Series(ts, index=df.index), True\n",
        "\n",
        "def _flatline(df, node_col, val_col, minrun=MIN_FLAT_RUN):\n",
        "    \"\"\"Flag sequences of >= minrun identical values (per node).\"\"\"\n",
        "    if val_col not in df.columns:\n",
        "        return pd.Series(False, index=df.index)\n",
        "    out = pd.Series(False, index=df.index)\n",
        "    for _, gg in df.groupby(node_col, sort=False):\n",
        "        v = gg[val_col]\n",
        "        valid = v.notna()\n",
        "        # segment id increases whenever value changes or is NaN\n",
        "        gid = ((~valid) | (v != v.shift())).cumsum()\n",
        "        runlen = gid.groupby(gid).transform(\"size\")\n",
        "        out.loc[gg.index] = valid & (runlen >= minrun)\n",
        "    return out\n",
        "\n",
        "# ---- Carry-forward helpers ----\n",
        "def _carry_forward_from(src_pattern, want_cols, tag):\n",
        "    \"\"\"Merge selected columns from the latest file matching pattern, by row_id.\"\"\"\n",
        "    hits = sorted(glob.glob(os.path.join(OUT_DIR, src_pattern)), key=_ver, reverse=True)\n",
        "    if not hits:\n",
        "        return None\n",
        "    src = hits[0]\n",
        "    try:\n",
        "        hdr = pd.read_csv(src, nrows=0, low_memory=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "    have = [c for c in want_cols if c in hdr.columns]\n",
        "    if not have:\n",
        "        return None\n",
        "    add = pd.read_csv(src, usecols=[\"row_id\"] + have, low_memory=False)\n",
        "    print(f\"[7a] Carry-forward from {tag}: {os.path.basename(src)} ({len(have)} cols)\")\n",
        "    return add\n",
        "\n",
        "def _carry_forward_masks(df_base: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Bring over SM (5PD) + ST (6PD) clean/flags so they persist through Step 7 outputs.\"\"\"\n",
        "    out = df_base.copy()\n",
        "    if \"row_id\" not in out.columns:\n",
        "        return out\n",
        "\n",
        "    # Soil MOISTURE (5PD)\n",
        "    sm_want = [\n",
        "        \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "        \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "        \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\n",
        "        \"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "        \"qc_sm_range_0\",\"qc_sm_range_1\",\n",
        "        \"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "        \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "        \"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "    ]\n",
        "    sm_add = _carry_forward_from(\"stage5pd_sm_pairdiverge_v*.csv\", sm_want, \"5PD (SM)\")\n",
        "    if sm_add is not None:\n",
        "        out = out.drop(columns=[c for c in sm_want if c in out.columns], errors=\"ignore\").merge(sm_add, on=\"row_id\", how=\"left\")\n",
        "\n",
        "    # Soil TEMP (6PD) — includes earlier 6a/6b flags if they existed when 6PD ran\n",
        "    st_want = [\n",
        "        \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "        \"qc_st_pairdiverge_0\",\"qc_st_pairdiverge_1\",\n",
        "        \"qc_st_anycrit_0\",\"qc_st_anycrit_1\",\n",
        "        \"qc_st_anywarn_0\",\"qc_st_anywarn_1\",\n",
        "        \"qc_st_range_0\",\"qc_st_range_1\",\n",
        "        \"qc_st_flatline_0\",\"qc_st_flatline_1\",\n",
        "        \"qc_st_known_broken_0\",\"qc_st_known_broken_1\",\n",
        "    ]\n",
        "    st_add = _carry_forward_from(\"stage6pd_soiltemp_pairdiverge_v*.csv\", st_want, \"6PD (ST)\")\n",
        "    if st_add is not None:\n",
        "        out = out.drop(columns=[c for c in st_want if c in out.columns], errors=\"ignore\").merge(st_add, on=\"row_id\", how=\"left\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# ---- run\n",
        "src = _pick_input(required_cols=(\"temperature\",))\n",
        "df  = pd.read_csv(src, low_memory=False)\n",
        "\n",
        "# carry-forward SM (5PD) + ST (6PD) columns (if any)\n",
        "df = _carry_forward_masks(df)\n",
        "\n",
        "node_col = _pick_node_col(df)\n",
        "df[\"__timestamp\"], used_synth_time = _ensure_time(df, current_path=src)\n",
        "\n",
        "sort_keys = [node_col, \"__timestamp\"] + ([\"row_id\"] if \"row_id\" in df.columns else [])\n",
        "df = df.sort_values(sort_keys, kind=\"stable\").reset_index(drop=True)\n",
        "\n",
        "# ---- QC + masking (fixed thresholds)\n",
        "t = pd.to_numeric(df[\"temperature\"], errors=\"coerce\")\n",
        "\n",
        "qc = pd.Series(\"OK\", index=df.index, dtype=\"object\")\n",
        "miss = t.isna()\n",
        "crit = (~miss) & ((t < -25) | (t > 50))\n",
        "warn = (~miss) & (~crit) & (((t >= -25) & (t < -20)) | ((t > 45) & (t <= 50)))\n",
        "\n",
        "qc[miss] = \"CRIT_missing\"\n",
        "qc[crit] = \"CRIT_out_of_range\"\n",
        "qc[warn] = \"WARN_out_of_range\"\n",
        "\n",
        "# Flatline (node-wise)\n",
        "flat = _flatline(df, node_col, \"temperature\", MIN_FLAT_RUN)\n",
        "\n",
        "# Consolidate air-temp flags\n",
        "is_crit = qc.str.startswith(\"CRIT\")\n",
        "is_ok   = qc.eq(\"OK\")\n",
        "qc[~is_crit & is_ok & flat] = \"WARN_flatline\"\n",
        "\n",
        "df[\"qc_air_selfcheck_temp\"] = qc.astype(\"category\")\n",
        "\n",
        "# Preferred series: mask WARN/CRIT (keep OK)\n",
        "df[\"temperature__pref\"] = t.mask(qc.str.startswith((\"WARN\",\"CRIT\"), na=False)).astype(\"float64\")\n",
        "\n",
        "# Rollup booleans (safe OR with any existing columns)\n",
        "def _as_bool(s, default=False):\n",
        "    if isinstance(s, pd.Series):\n",
        "        if s.dtype == bool:\n",
        "            return s\n",
        "        return s.astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"y\",\"yes\"]).fillna(default)\n",
        "    return pd.Series(default, index=df.index)\n",
        "\n",
        "df[\"ok_temperature\"]       = df[\"temperature__pref\"].notna()\n",
        "\n",
        "df[\"qc_air_anywarn\"]       = (_as_bool(df.get(\"qc_air_anywarn\", False)) | qc.str.startswith(\"WARN\", na=False)).astype(bool)\n",
        "df[\"qc_air_anycrit\"]       = (_as_bool(df.get(\"qc_air_anycrit\", False)) | qc.str.startswith(\"CRIT\", na=False)).astype(bool)\n",
        "df[\"qc_air_flatline_any\"]  = (_as_bool(df.get(\"qc_air_flatline_any\", False)) | flat).astype(bool)\n",
        "\n",
        "# ---- save\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "i = 1\n",
        "while True:\n",
        "    out = os.path.join(OUT_DIR, f\"{OUT_STEM}{i:03d}.csv\")\n",
        "    if not os.path.exists(out): break\n",
        "    i += 1\n",
        "df.to_csv(out, index=False)\n",
        "print(f\"[7a] wrote: {out}\")\n",
        "\n",
        "# ---- audit\n",
        "s = df[\"qc_air_selfcheck_temp\"].astype(str)\n",
        "print(f\"[7a] temp QC counts — OK={int((s=='OK').sum())}  WARN={int(s.str.startswith('WARN').sum())}  CRIT={int(s.str.startswith('CRIT').sum())}\")\n",
        "raw_present  = t.notna()\n",
        "pref_missing = df[\"temperature__pref\"].isna()\n",
        "denom = int(raw_present.sum()) or 1\n",
        "mshare = (pref_missing & raw_present).sum()/denom\n",
        "print(f\"[7a] masked share (temp): {mshare:.2%}\")\n",
        "if used_synth_time:\n",
        "    print(\"[7a] NOTE: Used synthetic time; flatline = 12 consecutive samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf42_R4ZoHmw",
        "outputId": "62015a7c-28bb-4578-e188-72b61379686e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7a] Input picked: af_clean_v01/stage6pd_soiltemp_pairdiverge_v008.csv\n",
            "[7a] Carry-forward from 5PD (SM): stage5pd_sm_pairdiverge_v001.csv (16 cols)\n",
            "[7a] Carry-forward from 6PD (ST): stage6pd_soiltemp_pairdiverge_v008.csv (14 cols)\n",
            "[7a] Using node column: 'node_id'\n",
            "[7a] Found 'timestamp'.\n",
            "[7a] wrote: af_clean_v01/stage7a_air_temp_qc_v001.csv\n",
            "[7a] temp QC counts — OK=469158  WARN=4272  CRIT=10357\n",
            "[7a] masked share (temp): 3.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== Step 7b (LENIENT): Air Humidity QC + masking with context (daylight + duration) ==============\n",
        "# One preferred series: humidity__pref (lenient)\n",
        "# Mask CRIT, and only those flatline/sticky runs that are implausible (long or in daylight).\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- knobs (tune here) ----\n",
        "MIN_FLAT_RUN       = 12          # samples for exact-value flatline detection (>= -> flagged)\n",
        "STICKY_MINRUN      = 8           # samples for sticky-high detection (>= -> flagged)\n",
        "STICKY_MIN         = 99.0        # %RH threshold for sticky-high candidates\n",
        "STICKY_SPREAD      = 0.5         # %RH max (max-min) within a sticky run\n",
        "LONG_RUN_SAMPLES   = 48          # >= -> implausible (≈12 h at 15-min cadence)\n",
        "DAYLIGHT_RAD_WM2   = 200.0       # radiation threshold for \"daylight\"\n",
        "RH_CRIT_HIGH       = 102.0       # > -> CRIT_out_of_range; (100, RH_CRIT_HIGH] -> WARN_minor_high\n",
        "\n",
        "# ---- file picking ----\n",
        "SEARCH_INPUT = [\n",
        "    \"af_clean_v01/stage7a_air_temp_qc_v*.csv\",\n",
        "    \"af_clean_v01/stage6pd_soiltemp_pairdiverge_v*.csv\",   # ← NEW: include ST-6PD in the chain\n",
        "    \"af_clean_v01/stage8_*v*.csv\",\n",
        "    \"af_clean_v01/stage5pd_*v*.csv\",\n",
        "    \"af_clean_v01/stage5r_*v*.csv\",\n",
        "    \"af_clean_v01/stage6c_*v*.csv\", \"af_clean_v01/stage6b_*v*.csv\", \"af_clean_v01/stage6a_*v*.csv\",\n",
        "    \"af_clean_v01/stage1_parsed_v*.csv\",\n",
        "    \"af_clean_v01/analysis_ready_v*.csv\", \"analysis_ready_v*.csv\",\n",
        "]\n",
        "OUT_DIR, OUT_STEM = \"af_clean_v01\", \"stage7b_air_hum_qc_v\"\n",
        "\n",
        "def _ver(path):\n",
        "    m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(path)); return int(m.group(1)) if m else -1\n",
        "\n",
        "def _pick_input():\n",
        "    def _skip(n): n=n.lower(); return any(k in n for k in [\"count\",\"counts\",\"summary\",\"overall\",\"audit\"])\n",
        "    for pat in SEARCH_INPUT:\n",
        "        for p in sorted(glob.glob(pat), key=_ver, reverse=True):\n",
        "            if _skip(os.path.basename(p)): continue\n",
        "            try:\n",
        "                hdr = pd.read_csv(p, nrows=0, low_memory=False)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if \"humidity\" in hdr.columns:\n",
        "                print(f\"[7b] Input picked: {p}\")\n",
        "                return p\n",
        "    raise FileNotFoundError(\"[7b] No input found that contains 'humidity'.\")\n",
        "\n",
        "def _next_out():\n",
        "    os.makedirs(OUT_DIR, exist_ok=True); i=1\n",
        "    while True:\n",
        "        out = os.path.join(OUT_DIR, f\"{OUT_STEM}{i:03d}.csv\")\n",
        "        if not os.path.exists(out): return out\n",
        "        i += 1\n",
        "\n",
        "def _to_time(df):\n",
        "    if \"timestamp\" in df.columns:     return pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "    if \"timestamp_iso\" in df.columns: return pd.to_datetime(df[\"timestamp_iso\"], utc=True, errors=\"coerce\")\n",
        "    if \"ts_ns\" in df.columns:         return pd.to_datetime(pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "    return pd.date_range(\"2000-01-01\", periods=len(df), freq=\"s\", tz=\"UTC\")\n",
        "\n",
        "# ---- Carry-forward helpers (SM from 5PD + ST from 6PD) ----\n",
        "def _carry_forward_from(src_pattern, want_cols, tag):\n",
        "    hits = sorted(glob.glob(os.path.join(OUT_DIR, src_pattern)), key=_ver, reverse=True)\n",
        "    if not hits: return None\n",
        "    src = hits[0]\n",
        "    try:\n",
        "        hdr = pd.read_csv(src, nrows=0, low_memory=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "    have = [c for c in want_cols if c in hdr.columns]\n",
        "    if not have: return None\n",
        "    add = pd.read_csv(src, usecols=[\"row_id\"] + have, low_memory=False)\n",
        "    print(f\"[7b] Carry-forward from {tag}: {os.path.basename(src)} ({len(have)} cols)\")\n",
        "    return add\n",
        "\n",
        "def _carry_forward_masks(df_base: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df_base.copy()\n",
        "    if \"row_id\" not in out.columns:  # nothing to merge on\n",
        "        return out\n",
        "\n",
        "    # Soil MOISTURE (5PD)\n",
        "    sm_want = [\n",
        "        \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "        \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "        \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\n",
        "        \"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "        \"qc_sm_range_0\",\"qc_sm_range_1\",\n",
        "        \"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "        \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "        \"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "    ]\n",
        "    sm_add = _carry_forward_from(\"stage5pd_sm_pairdiverge_v*.csv\", sm_want, \"5PD (SM)\")\n",
        "    if sm_add is not None:\n",
        "        out = out.drop(columns=[c for c in sm_want if c in out.columns], errors=\"ignore\").merge(sm_add, on=\"row_id\", how=\"left\")\n",
        "\n",
        "    # Soil TEMP (6PD)\n",
        "    st_want = [\n",
        "        \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "        \"qc_st_pairdiverge_0\",\"qc_st_pairdiverge_1\",\n",
        "        \"qc_st_anycrit_0\",\"qc_st_anycrit_1\",\n",
        "        \"qc_st_anywarn_0\",\"qc_st_anywarn_1\",\n",
        "        \"qc_st_range_0\",\"qc_st_range_1\",\n",
        "        \"qc_st_flatline_0\",\"qc_st_flatline_1\",\n",
        "        \"qc_st_known_broken_0\",\"qc_st_known_broken_1\",\n",
        "    ]\n",
        "    st_add = _carry_forward_from(\"stage6pd_soiltemp_pairdiverge_v*.csv\", st_want, \"6PD (ST)\")\n",
        "    if st_add is not None:\n",
        "        out = out.drop(columns=[c for c in st_want if c in out.columns], errors=\"ignore\").merge(st_add, on=\"row_id\", how=\"left\")\n",
        "\n",
        "    return out\n",
        "\n",
        "def _run_ids(mask_bool: pd.Series) -> pd.Series:\n",
        "    m = mask_bool.fillna(False)\n",
        "    return (m != m.shift()).cumsum()\n",
        "\n",
        "def _flatline_flag(series: pd.Series, minrun: int) -> pd.Series:\n",
        "    v = pd.to_numeric(series, errors=\"coerce\")\n",
        "    valid = v.notna()\n",
        "    gid = ((~valid) | (v != v.shift())).cumsum()\n",
        "    runlen = gid.groupby(gid).transform(\"size\")\n",
        "    return valid & (runlen >= minrun)\n",
        "\n",
        "def _sticky_flags(group_df: pd.DataFrame, minrun:int, thr:float, spread:float) -> pd.Series:\n",
        "    h = pd.to_numeric(group_df[\"humidity\"], errors=\"coerce\")\n",
        "    cand = h.notna() & (h >= thr)\n",
        "    rid = _run_ids(cand)\n",
        "    lengths = rid.groupby(rid).transform(\"size\")\n",
        "    if cand.any():\n",
        "        run_spread = group_df.loc[cand].groupby(rid.loc[cand]).agg(rmin=(\"humidity\",\"min\"), rmax=(\"humidity\",\"max\"))\n",
        "        spread_map = (run_spread[\"rmax\"] - run_spread[\"rmin\"])\n",
        "        spread_b = rid.map(spread_map).reindex(group_df.index, fill_value=np.nan)\n",
        "    else:\n",
        "        spread_b = pd.Series(np.nan, index=group_df.index)\n",
        "    return cand & (lengths >= minrun) & (spread_b <= spread)\n",
        "\n",
        "def _as_bool(s, default=False, idx=None):\n",
        "    if isinstance(s, pd.Series):\n",
        "        if s.dtype == bool: return s\n",
        "        return s.astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"y\",\"yes\"]).fillna(default)\n",
        "    return pd.Series(default, index=idx)\n",
        "\n",
        "# ---- run 7b (lenient) ----\n",
        "src = _pick_input()\n",
        "df  = pd.read_csv(src, low_memory=False)\n",
        "\n",
        "# carry-forward SM (5PD) + ST (6PD) columns so masks persist\n",
        "df = _carry_forward_masks(df)\n",
        "\n",
        "# node/time\n",
        "node_col = \"node_id\" if \"node_id\" in df.columns else (\"node\" if \"node\" in df.columns else None)\n",
        "if node_col is None:\n",
        "    df[\"__node_synth\"] = \"__all__\"; node_col=\"__node_synth\"; print(\"[7b] No node_id/node -> using '__node_synth'.\")\n",
        "\n",
        "df[\"__timestamp\"] = _to_time(df)\n",
        "sort_cols = [node_col, \"__timestamp\"] + ([\"row_id\"] if \"row_id\" in df.columns else [])\n",
        "df = df.sort_values(sort_cols, kind=\"stable\").reset_index(drop=True)\n",
        "\n",
        "# daylight context (accept more column names)\n",
        "rad_full = None\n",
        "for c in [\"global_radiation__pref\", \"global_radiation_clean\", \"global_radiation\"]:\n",
        "    if c in df.columns:\n",
        "        rad_full = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        break\n",
        "\n",
        "if \"humidity\" not in df.columns:\n",
        "    print(\"[7b] No 'humidity' column; 7b is a no-op.\")\n",
        "else:\n",
        "    h  = pd.to_numeric(df[\"humidity\"], errors=\"coerce\")\n",
        "    qc = pd.Series(\"OK\", index=df.index, dtype=\"object\")\n",
        "\n",
        "    # Base physics thresholds\n",
        "    miss = h.isna()\n",
        "    crit = (~miss) & ((h < 0) | (h > RH_CRIT_HIGH))\n",
        "    warn_minor_high = (~miss) & (~crit) & (h > 100)\n",
        "\n",
        "    qc[miss] = \"CRIT_missing\"\n",
        "    qc[crit] = \"CRIT_out_of_range\"\n",
        "    qc[warn_minor_high] = \"WARN_minor_high\"  # kept (not masked unless implausible context)\n",
        "\n",
        "    # Containers for context-based masks\n",
        "    mask_flat_impl = pd.Series(False, index=df.index)\n",
        "    mask_sticky_impl = pd.Series(False, index=df.index)\n",
        "\n",
        "    # Per-node processing for flatline & sticky with context\n",
        "    for node, g in df.groupby(df[node_col].astype(str), sort=False):\n",
        "        idx = g.index\n",
        "\n",
        "        # flatline candidates (flag all)\n",
        "        flat_flag = _flatline_flag(g[\"humidity\"], MIN_FLAT_RUN)\n",
        "        qc.loc[idx[flat_flag]] = np.where(qc.loc[idx[flat_flag]].eq(\"OK\"),\n",
        "                                          \"WARN_flatline\", qc.loc[idx[flat_flag]])\n",
        "\n",
        "        # sticky-high candidates (flag all; separate daylight tag)\n",
        "        sticky_flag = _sticky_flags(g, STICKY_MINRUN, STICKY_MIN, STICKY_SPREAD)\n",
        "\n",
        "        has_day = pd.Series(False, index=g.index)\n",
        "        if rad_full is not None:\n",
        "            has_day = (rad_full.loc[idx] > DAYLIGHT_RAD_WM2).fillna(False)\n",
        "\n",
        "        # run-level daylight and duration for both flat & sticky\n",
        "        rid_flat   = _run_ids(flat_flag)\n",
        "        rid_sticky = _run_ids(sticky_flag)\n",
        "\n",
        "        dur_flat   = rid_flat.groupby(rid_flat).transform(\"size\").where(flat_flag, other=0)\n",
        "        dur_sticky = rid_sticky.groupby(rid_sticky).transform(\"size\").where(sticky_flag, other=0)\n",
        "\n",
        "        anyday_flat   = rid_flat.groupby(rid_flat).transform(lambda r: has_day.loc[r.index].max()).astype(bool)\n",
        "        anyday_sticky = rid_sticky.groupby(rid_sticky).transform(lambda r: has_day.loc[r.index].max()).astype(bool)\n",
        "\n",
        "        # implausibility tests (these will be masked)\n",
        "        impl_flat   = flat_flag   & (anyday_flat | (dur_flat   >= LONG_RUN_SAMPLES))\n",
        "        impl_sticky = sticky_flag & (anyday_sticky | (dur_sticky >= LONG_RUN_SAMPLES))\n",
        "\n",
        "        mask_flat_impl.loc[idx]   = impl_flat\n",
        "        mask_sticky_impl.loc[idx] = impl_sticky\n",
        "\n",
        "        # QC labels for sticky variants\n",
        "        qc.loc[idx[sticky_flag & anyday_sticky]] = np.where(qc.loc[idx[sticky_flag & anyday_sticky]].eq(\"OK\"),\n",
        "                                                            \"WARN_sticky_high_daylight\", qc.loc[idx[sticky_flag & anyday_sticky]])\n",
        "        qc.loc[idx[sticky_flag & ~anyday_sticky]] = np.where(qc.loc[idx[sticky_flag & ~anyday_sticky]].eq(\"OK\"),\n",
        "                                                             \"WARN_sticky_high\", qc.loc[idx[sticky_flag & ~anyday_sticky]])\n",
        "\n",
        "    # Build lenient mask: CRIT + implausible flat/sticky + missing\n",
        "    mask_lenient = miss | crit | mask_flat_impl | mask_sticky_impl\n",
        "    df[\"humidity__pref\"] = h.mask(mask_lenient).astype(\"float64\")\n",
        "    df[\"ok_humidity\"] = df[\"humidity__pref\"].notna()\n",
        "\n",
        "    # Persist QC column from this step\n",
        "    df[\"qc_air_selfcheck_hum\"] = qc.astype(\"category\")\n",
        "\n",
        "    # Rollups (safe OR)\n",
        "    df[\"qc_air_anywarn\"] = (_as_bool(df.get(\"qc_air_anywarn\"), False, idx=df.index) | qc.str.startswith(\"WARN\", na=False)).astype(bool)\n",
        "    df[\"qc_air_anycrit\"] = (_as_bool(df.get(\"qc_air_anycrit\"), False, idx=df.index) | qc.str.startswith(\"CRIT\", na=False)).astype(bool)\n",
        "\n",
        "    # Optional: compact reason for mask (handy for audits/plots)\n",
        "    reason = pd.Series(\"\", index=df.index, dtype=\"object\")\n",
        "    reason[miss]                 = \"CRIT_missing\"\n",
        "    reason[crit & ~miss]         = \"CRIT_out_of_range\"\n",
        "    reason[mask_flat_impl]       = \"MASK_flatline_implausible\"\n",
        "    reason[mask_sticky_impl]     = \"MASK_sticky_implausible\"\n",
        "    df[\"humidity_mask_reason\"]   = reason.mask(~mask_lenient, other=\"\")\n",
        "\n",
        "# ---- save ----\n",
        "out_path = _next_out()\n",
        "df.to_csv(out_path, index=False)\n",
        "print(f\"[7b lenient] wrote: {out_path}\")\n",
        "\n",
        "# ---- audit print ----\n",
        "if \"humidity\" in df.columns:\n",
        "    s = df[\"qc_air_selfcheck_hum\"].astype(str) if \"qc_air_selfcheck_hum\" in df.columns else qc.astype(str)\n",
        "    raw_present  = pd.to_numeric(df[\"humidity\"], errors=\"coerce\").notna()\n",
        "    pref_missing = pd.to_numeric(df[\"humidity__pref\"], errors=\"coerce\").isna()\n",
        "    denom = int(raw_present.sum()) or 1\n",
        "    mshare = float((raw_present & pref_missing).sum())/denom\n",
        "    print(f\"[7b lenient] masked share: {mshare:.2%}\")\n",
        "    if \"humidity_mask_reason\" in df.columns:\n",
        "        rb = df.loc[pref_missing & raw_present, \"humidity_mask_reason\"].value_counts()\n",
        "        if len(rb):\n",
        "            print(\"[7b lenient] mask reasons:\")\n",
        "            for k,v in rb.items():\n",
        "                print(f\"   {k:>28s}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0oZBX4IpAVZ",
        "outputId": "5f7fde8f-14cf-43c2-be07-31ad0d076f27"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7b] Input picked: af_clean_v01/stage7a_air_temp_qc_v001.csv\n",
            "[7b] Carry-forward from 5PD (SM): stage5pd_sm_pairdiverge_v001.csv (16 cols)\n",
            "[7b] Carry-forward from 6PD (ST): stage6pd_soiltemp_pairdiverge_v008.csv (14 cols)\n",
            "[7b lenient] wrote: af_clean_v01/stage7b_air_hum_qc_v001.csv\n",
            "[7b lenient] masked share: 58.98%\n",
            "[7b lenient] mask reasons:\n",
            "        MASK_sticky_implausible: 230650\n",
            "      MASK_flatline_implausible: 52184\n",
            "              CRIT_out_of_range: 2511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7c: Air Pressure QC + masking**\n",
        "\n",
        "Goal:\n",
        "* Build a lenient, usable humidity series (humidity__pref) by masking only clear problems and implausible artifacts.\n",
        "* Input picking: Auto-selects the latest upstream CSV that contains humidity (prefers recent QC stages).\n",
        "* Carry-forward: Pulls over soil moisture & soil temperature clean columns + QC flags so earlier masks persist.\n",
        "* Time & grouping: Normalizes timestamps, sorts by node × time.\n",
        "* Daylight context: Uses available radiation to tag daylight (>~200 W/m²) for plausibility checks.\n",
        "\n",
        "QC rules:\n",
        "* CRIT: missing, <0, or >102% RH → masked.\n",
        "* WARN (kept unless implausible): >100% RH, flatlines (exact repeats), sticky-high runs near 99–100% with tiny spread.\n",
        "* Context gating (lenient masking): Flatline/sticky runs are only masked if they (a) include daylight or (b) last very long (≥ ~12 h at 15-min cadence). Otherwise they stay as WARN (visible, not masked).\n",
        "\n",
        "\n",
        "Outputs:\n",
        "* humidity__pref (leniently masked humidity) and ok_humidity (not-NaN).\n",
        "* qc_air_selfcheck_hum with detailed WARN/CRIT labels.\n",
        "* Rollups: qc_air_anywarn, qc_air_anycrit.\n",
        "* Optional humidity_mask_reason for audits.\n",
        "\n",
        "Saving & audit: Writes versioned CSV to af_clean_v01/; prints total masked share and a breakdown of mask reasons.\n",
        "\n"
      ],
      "metadata": {
        "id": "B_hB_MpmCWJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== Step 7c: Air Pressure QC + masking (independent) ==============\n",
        "# Thresholds: WARN if <940 or >1050; CRIT if <920 or >1060; missing -> CRIT_missing\n",
        "# Flatline ≥6 -> WARN_flatline\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "IN_SEARCH = [\n",
        "    \"af_clean_v01/stage7b_air_hum_qc_v*.csv\",  # prefer just-run 7b\n",
        "    \"af_clean_v01/stage7a_air_temp_qc_v*.csv\",\n",
        "    \"af_clean_v01/stage5pd_*v*.csv\",           # <-- NEW: include 5PD so its masks can ride along\n",
        "    \"af_clean_v01/stage6c_*.csv\",\"af_clean_v01/stage6b_*.csv\",\"af_clean_v01/stage6a_*.csv\",\n",
        "    \"af_clean_v01/stage5r_*.csv\",\n",
        "    \"af_clean_v01/analysis_ready_v*.csv\",\"analysis_ready_v*.csv\",\n",
        "]\n",
        "OUT_DIR, OUT_STEM = \"af_clean_v01\", \"stage7c_air_pres_qc_v\"\n",
        "MIN_FLAT_RUN = 6\n",
        "\n",
        "def _pick():\n",
        "    for pat in IN_SEARCH:\n",
        "        hits = sorted(glob.glob(pat))\n",
        "        if hits:\n",
        "            def _ver(p):\n",
        "                m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(p)); return int(m.group(1)) if m else -1\n",
        "            best = max(hits, key=_ver); return best if _ver(best)>=0 else hits[-1]\n",
        "    raise FileNotFoundError(\"No input for 7c.\")\n",
        "\n",
        "def _next(stem):\n",
        "    os.makedirs(OUT_DIR, exist_ok=True); i=1\n",
        "    while True:\n",
        "        p = os.path.join(OUT_DIR, f\"{stem}{i:03d}.csv\")\n",
        "        if not os.path.exists(p): return p\n",
        "        i+=1\n",
        "\n",
        "def _dt(df):\n",
        "    if \"timestamp\" in df.columns:     return pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "    if \"timestamp_iso\" in df.columns: return pd.to_datetime(df[\"timestamp_iso\"], utc=True, errors=\"coerce\")\n",
        "    return pd.to_datetime(pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "\n",
        "def _flatline(df, node_col, val_col, minrun=6):\n",
        "    if val_col not in df.columns: return pd.Series(False, index=df.index)\n",
        "    out = pd.Series(False, index=df.index)\n",
        "    for _, gg in df.groupby(node_col, sort=False):\n",
        "        v = gg[val_col]; valid = v.notna()\n",
        "        gid = ((~valid) | (v != v.shift())).cumsum()\n",
        "        runlen = gid.groupby(gid).transform(\"size\")\n",
        "        out.loc[gg.index] = valid & (runlen >= minrun)\n",
        "    return out\n",
        "\n",
        "# --- carry-forward SM from 5PD (so masks persist downstream) ---\n",
        "def _carry_forward_sm_from_5pd(df_base: pd.DataFrame) -> pd.DataFrame:\n",
        "    hits = sorted(glob.glob(os.path.join(OUT_DIR, \"stage5pd_sm_pairdiverge_v*.csv\")))\n",
        "    if not hits or \"row_id\" not in df_base.columns:\n",
        "        return df_base\n",
        "    def _ver(p):\n",
        "        m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(p)); return int(m.group(1)) if m else -1\n",
        "    src = max(hits, key=_ver) if _ver(hits[-1]) >= 0 else hits[-1]\n",
        "    want = [\n",
        "        \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "        \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "        \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "        \"qc_sm_range_0\",\"qc_sm_range_1\",\"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "        \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "    ]\n",
        "    try:\n",
        "        hdr = pd.read_csv(src, nrows=0, low_memory=False)\n",
        "    except Exception:\n",
        "        return df_base\n",
        "    have = [c for c in want if c in hdr.columns]\n",
        "    if not have:\n",
        "        return df_base\n",
        "    add = pd.read_csv(src, usecols=[\"row_id\"]+have, low_memory=False)\n",
        "    out = df_base.drop(columns=[c for c in have if c in df_base.columns], errors=\"ignore\").merge(add, on=\"row_id\", how=\"left\")\n",
        "    print(f\"[7c] Carry-forward from 5PD: {os.path.basename(src)} ({len(have)} cols)\")\n",
        "    return out\n",
        "\n",
        "# ---- run\n",
        "src = _pick()\n",
        "df = pd.read_csv(src, low_memory=False)\n",
        "df = _carry_forward_sm_from_5pd(df)  # keep 5PD masks\n",
        "\n",
        "node_col = \"node_id\" if \"node_id\" in df.columns else (\"node\" if \"node\" in df.columns else None)\n",
        "assert node_col is not None, \"No node_id/node\"\n",
        "df[\"__timestamp\"] = _dt(df)\n",
        "df = df.sort_values([node_col, \"__timestamp\", \"row_id\" if \"row_id\" in df.columns else \"__timestamp\"],\n",
        "                    kind=\"stable\").reset_index(drop=True)\n",
        "\n",
        "if \"pressure\" in df.columns:\n",
        "    qc = pd.Series(\"OK\", index=df.index, dtype=\"object\")\n",
        "    p = pd.to_numeric(df[\"pressure\"], errors=\"coerce\")\n",
        "\n",
        "    miss = p.isna()\n",
        "    crit = (~miss) & ((p < 920) | (p > 1060))\n",
        "    warn = (~miss) & (~crit) & ((p < 940) | (p > 1050))\n",
        "\n",
        "    qc[miss] = \"CRIT_missing\"\n",
        "    qc[crit] = \"CRIT_out_of_range\"\n",
        "    qc[warn] = \"WARN_out_of_range\"\n",
        "\n",
        "    flat = _flatline(df, node_col, \"pressure\", MIN_FLAT_RUN)\n",
        "    is_crit = qc.str.startswith(\"CRIT\"); is_ok = qc.eq(\"OK\")\n",
        "    qc[~is_crit & is_ok & flat] = \"WARN_flatline\"\n",
        "\n",
        "    # Persist QC column\n",
        "    df[\"qc_air_crosscheck_pres\"] = qc.astype(\"category\")\n",
        "\n",
        "    # ---- IMPORTANT: mask ONLY CRIT (incl. missing) for pressure; WARN stays visible\n",
        "    pref = p.mask(qc.str.startswith(\"CRIT\", na=False))\n",
        "    df[\"pressure__pref\"] = pref.astype(\"float64\")\n",
        "    df[\"ok_pressure\"] = df[\"pressure__pref\"].notna()\n",
        "\n",
        "    # Rollups (boolean-safe)\n",
        "    prev_warn = pd.Series(df.get(\"qc_air_anywarn\", False), index=df.index).astype(bool)\n",
        "    prev_crit = pd.Series(df.get(\"qc_air_anycrit\", False), index=df.index).astype(bool)\n",
        "    df[\"qc_air_anywarn\"]      = (prev_warn | qc.str.startswith(\"WARN\", na=False)).astype(bool)\n",
        "    df[\"qc_air_anycrit\"]      = (prev_crit | qc.str.startswith(\"CRIT\", na=False)).astype(bool)\n",
        "    df[\"qc_air_flatline_any\"] = (pd.Series(df.get(\"qc_air_flatline_any\", False), index=df.index).astype(bool) | flat).astype(bool)\n",
        "else:\n",
        "    print(\"No 'pressure' column; 7c is a no-op.\")\n",
        "\n",
        "out = _next(OUT_STEM)\n",
        "df.to_csv(out, index=False)\n",
        "print(f\"7c wrote: {out}\")\n",
        "\n",
        "# ---- audit ----\n",
        "if \"pressure\" in df.columns:\n",
        "    s = df[\"qc_air_crosscheck_pres\"].astype(str)\n",
        "    print(f\"[7c] pressure QC counts — OK={int((s=='OK').sum())}  \"\n",
        "          f\"WARN={int(s.str.startswith('WARN').sum())}  \"\n",
        "          f\"CRIT={int(s.str.startswith('CRIT').sum())}\")\n",
        "    raw_present  = pd.to_numeric(df[\"pressure\"], errors=\"coerce\").notna()\n",
        "    pref_missing = pd.to_numeric(df[\"pressure__pref\"], errors=\"coerce\").isna()\n",
        "    denom = int(raw_present.sum()) or 1\n",
        "    mshare = float((raw_present & pref_missing).sum())/denom\n",
        "    print(f\"[7c] masked share (pressure): {mshare:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atFN0U2vH3Tx",
        "outputId": "c63fdf30-796b-478a-d2e0-af53b8c8aee9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7c] Carry-forward from 5PD: stage5pd_sm_pairdiverge_v001.csv (16 cols)\n",
            "7c wrote: af_clean_v01/stage7c_air_pres_qc_v001.csv\n",
            "[7c] pressure QC counts — OK=65276  WARN=405052  CRIT=13459\n",
            "[7c] masked share (pressure): 2.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 8: Radiation & Wind range QC**\n",
        "\n",
        "* Loads the latest pipeline file (prefers Stage 7 → 6c → …) and rebuilds timestamp from ts_ns. Preserves row_id; asserts no row loss.\n",
        "* Ensures numeric for:\n",
        "* global_radiation (W/m²)\n",
        "* wind_speed_mean, wind_speed_min, wind_speed_max (km/h)\n",
        "* Applies range sanity with conservative WARN bands and CRIT outside:\n",
        "* Radiation: OK in [0, 1200]; WARN in [-5, 0) low and (1200, 1400] high; CRIT < -5 or > 1400.\n",
        "* Wind speeds: OK in [0, 100]; WARN in [-0.5, 0) low and (100, 150] high; CRIT < -0.5 or > 150.\n",
        "* Writes _clean versions (global_radiation_clean, wind_speed_*_clean):\n",
        "* CRIT → NaN (masked),\n",
        "* WARN → clamped to the nearest OK bound (e.g., −0.2 → 0; 145 → 100 for wind mean, 1205 → 1200 for radiation),\n",
        "* OK → passthrough.\n",
        "* Adds per-variable QC flags (qc_rad_range, qc_wind_*_range) plus boolean rollups (*_anycrit, *_anywarn) for easy filtering later.\n",
        "* Adds a wind ordering check (qc_wind_order): flags WARN_inconsistent where min > mean or mean > max. (Diagnostic only; no masking.)\n",
        "* Saves a versioned output and summary CSVs (overall + per-node), prints time span and node list.\n",
        "\n"
      ],
      "metadata": {
        "id": "RJwmKhIcCYE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Stage 8: Radiation & Wind — range QC (mask CRIT only; clamp WARN)\n",
        "# GUARANTEES:\n",
        "#  - Reads latest prior stage (prefer 7c→7b→7a → 6PD → 6c → 6b → 6a → 5PD → 5R → 4 → 3 → 2 → 1)\n",
        "#  - Skips summary/audit files\n",
        "#  - Reconstructs timestamp from ts_ns (no free-text parsing)\n",
        "#  - Carries forward ST (6PD), SM (5PD), and Air (7a/7b/7c) cols by row_id (no overwrite)\n",
        "#  - No row loss; preserves row_id\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os, re\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core columns\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"   # in-memory datetime64[ns]\n",
        "\n",
        "# Node column will be detected dynamically\n",
        "def _pick_node_col(df):\n",
        "    for c in [\"node\", \"node_id\", \"station_id\", \"station\", \"device_id\", \"device\", \"logger_id\", \"site\"]:\n",
        "        if c in df.columns:\n",
        "            print(f\"[Stage 8] Using node column: {c!r}\")\n",
        "            return c\n",
        "    df[\"__node_synth\"] = \"__all__\"\n",
        "    print(\"[Stage 8] No node column found → using synthetic group '__node_synth'.\")\n",
        "    return \"__node_synth\"\n",
        "\n",
        "# Variables (exact column names in your file)\n",
        "RAD_COL   = \"global_radiation\"     # W/m²\n",
        "W_MEAN    = \"wind_speed_mean\"      # km/h\n",
        "W_MIN     = \"wind_speed_min\"       # km/h\n",
        "W_MAX     = \"wind_speed_max\"       # km/h\n",
        "\n",
        "# Clean column names to write\n",
        "RAD_CLEAN = f\"{RAD_COL}_clean\"\n",
        "W_MEAN_C  = f\"{W_MEAN}_clean\"\n",
        "W_MIN_C   = f\"{W_MIN}_clean\"\n",
        "W_MAX_C   = f\"{W_MAX}_clean\"\n",
        "\n",
        "# ---------------- Range thresholds ----------------\n",
        "# Radiation (W/m²)\n",
        "RAD_LOWER_OK, RAD_UPPER_OK   = 0.0, 1200.0\n",
        "RAD_LOWER_TOL, RAD_UPPER_TOL = -5.0, 1400.0   # WARN bands near edges\n",
        "\n",
        "# Wind speed (km/h)\n",
        "W_LOWER_OK, W_UPPER_OK   = 0.0, 100.0\n",
        "W_LOWER_TOL, W_UPPER_TOL = -0.5, 150.0\n",
        "\n",
        "def _ver(path):\n",
        "    m = re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(path)); return int(m.group(1)) if m else -1\n",
        "\n",
        "def _skip_name(p):\n",
        "    name = os.path.basename(p).lower()\n",
        "    return any(k in name for k in [\"count\", \"counts\", \"summary\", \"overall\", \"audit\"])\n",
        "\n",
        "def _has_cols(path, needed):\n",
        "    try:\n",
        "        hdr = pd.read_csv(path, nrows=0, low_memory=False)\n",
        "        return all(c in hdr.columns for c in needed)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _pick_latest_input():\n",
        "    patterns = [\n",
        "        # Prefer 7 outputs if they contain RAD/WIND\n",
        "        \"af_clean_v01/stage7c_air_pres_qc_v*.csv\",\n",
        "        \"af_clean_v01/stage7b_air_hum_qc_v*.csv\",\n",
        "        \"af_clean_v01/stage7a_air_temp_qc_v*.csv\",\n",
        "        # Then 6PD (soil temp diverge stage carrying all earlier cols)\n",
        "        \"af_clean_v01/stage6pd_soiltemp_pairdiverge_v*.csv\",  # ← NEW\n",
        "        # Then classic 6x, 5PD, 5R, …\n",
        "        \"af_clean_v01/stage6c_*v*.csv\",\n",
        "        \"af_clean_v01/stage6b_*v*.csv\",\n",
        "        \"af_clean_v01/stage6a_*v*.csv\",\n",
        "        \"af_clean_v01/stage5pd_*v*.csv\",\n",
        "        \"af_clean_v01/stage5r_*v*.csv\",\n",
        "        \"af_clean_v01/stage4_*v*.csv\",\n",
        "        \"af_clean_v01/stage3_*v*.csv\",\n",
        "        \"af_clean_v01/stage2_*v*.csv\",\n",
        "        \"af_clean_v01/stage1_parsed_v*.csv\",\n",
        "        # fallbacks\n",
        "        \"af_clean_v01/analysis_ready_v*.csv\",\n",
        "        \"analysis_ready_v*.csv\",\n",
        "    ]\n",
        "    needed = [ROW_ID_COL, TS_NS_COL, TS_ISO_COL, RAD_COL, W_MEAN, W_MIN, W_MAX]\n",
        "    for pat in patterns:\n",
        "        files = [p for p in sorted(glob(str(pat)), key=_ver, reverse=True) if not _skip_name(p)]\n",
        "        for p in files:\n",
        "            if _has_cols(p, needed):\n",
        "                print(\"[Stage 8] Reading:\", p)\n",
        "                return p\n",
        "    raise FileNotFoundError(\"[Stage 8] No prior stage contains all radiation & wind columns.\")\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "# ---- Load chosen input\n",
        "in_path = _pick_latest_input()\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "n_in = len(df)\n",
        "\n",
        "# Detect node column (after load)\n",
        "NODE_COL = _pick_node_col(df)\n",
        "\n",
        "# ---- Carry-forward helpers (merge by row_id without overwriting present cols)\n",
        "def _carry_forward(df_base: pd.DataFrame, src_glob: str, want_cols: list, tag: str) -> pd.DataFrame:\n",
        "    hits = sorted(glob(src_glob), key=_ver, reverse=True)\n",
        "    if not hits or ROW_ID_COL not in df_base.columns:\n",
        "        return df_base\n",
        "    src = hits[0]\n",
        "    try:\n",
        "        hdr = pd.read_csv(src, nrows=0, low_memory=False)\n",
        "    except Exception:\n",
        "        return df_base\n",
        "    have = [c for c in want_cols if c in hdr.columns]\n",
        "    if not have:\n",
        "        return df_base\n",
        "    add = pd.read_csv(src, usecols=[ROW_ID_COL] + have, low_memory=False)\n",
        "    keep_cols = [c for c in have if c not in df_base.columns]\n",
        "    if not keep_cols:\n",
        "        return df_base\n",
        "    print(f\"[Stage 8] Carry-forward {len(keep_cols)} cols from {tag}: {os.path.basename(src)}\")\n",
        "    return df_base.merge(add[[ROW_ID_COL] + keep_cols], on=ROW_ID_COL, how=\"left\")\n",
        "\n",
        "# Carry forward SM (5PD)\n",
        "SM_WANT = [\n",
        "    \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "    \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "    \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "    \"qc_sm_range_0\",\"qc_sm_range_1\",\"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "    \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "]\n",
        "df = _carry_forward(df, str(OUT_DIR / \"stage5pd_sm_pairdiverge_v*.csv\"), SM_WANT, \"5PD (SM)\")\n",
        "\n",
        "# Carry forward ST (6PD)  ← NEW\n",
        "ST_WANT = [\n",
        "    \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "    \"qc_st_pairdiverge_0\",\"qc_st_pairdiverge_1\",\n",
        "    \"qc_st_anycrit_0\",\"qc_st_anycrit_1\",\n",
        "    \"qc_st_anywarn_0\",\"qc_st_anywarn_1\",\n",
        "    \"qc_st_range_0\",\"qc_st_range_1\",\n",
        "    \"qc_st_flatline_0\",\"qc_st_flatline_1\",\n",
        "    \"qc_st_known_broken_0\",\"qc_st_known_broken_1\",\n",
        "]\n",
        "df = _carry_forward(df, str(OUT_DIR / \"stage6pd_soiltemp_pairdiverge_v*.csv\"), ST_WANT, \"6PD (ST)\")\n",
        "\n",
        "# Carry forward Air (7a/7b/7c)\n",
        "AIR7A_WANT = [\"temperature__pref\",\"qc_air_selfcheck_temp\",\"ok_temperature\"]\n",
        "AIR7B_WANT = [\"humidity__pref\",\"qc_air_selfcheck_hum\",\"humidity_mask_reason\",\"ok_humidity\"]\n",
        "AIR7C_WANT = [\"pressure__pref\",\"qc_air_crosscheck_pres\",\"ok_pressure\"]\n",
        "df = _carry_forward(df, str(OUT_DIR / \"stage7a_air_temp_qc_v*.csv\"), AIR7A_WANT, \"7a\")\n",
        "df = _carry_forward(df, str(OUT_DIR / \"stage7b_air_hum_qc_v*.csv\"), AIR7B_WANT, \"7b\")\n",
        "df = _carry_forward(df, str(OUT_DIR / \"stage7c_air_pres_qc_v*.csv\"), AIR7C_WANT, \"7c\")\n",
        "\n",
        "# ---- Sanity: required columns ----\n",
        "required = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, RAD_COL, W_MEAN, W_MIN, W_MAX}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns for Stage 8: {sorted(missing)}\")\n",
        "\n",
        "# Reconstruct timestamp from ts_ns (authoritative)\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(pd.to_numeric(df[TS_NS_COL], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id must be unique from Stage 1.\"\n",
        "\n",
        "# Ensure numeric\n",
        "for c in [RAD_COL, W_MEAN, W_MIN, W_MAX]:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---------------- Helpers to apply range QC consistently ----------------\n",
        "def apply_range_qc(series: pd.Series, lower_ok, upper_ok, lower_tol, upper_tol,\n",
        "                   flag_col: str, clean_col: str, df_target: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Adds:\n",
        "      - flag_col : 'OK' / 'WARN_boundary' / 'CRIT_out_of_range'\n",
        "      - <*_anycrit>, <*_anywarn> booleans\n",
        "      - clean_col: CRIT -> NaN; WARN clamped to [lower_ok, upper_ok]; OK passthrough.\n",
        "    \"\"\"\n",
        "    v = series\n",
        "    qc = pd.Series(\"OK\", index=v.index, dtype=\"object\")\n",
        "\n",
        "    warn_low  = v.ge(lower_tol) & v.lt(lower_ok)\n",
        "    warn_high = v.gt(upper_ok) & v.le(upper_tol)\n",
        "    crit_low  = v.lt(lower_tol)\n",
        "    crit_high = v.gt(upper_tol)\n",
        "\n",
        "    qc.loc[warn_low | warn_high] = \"WARN_boundary\"\n",
        "    qc.loc[crit_low | crit_high] = \"CRIT_out_of_range\"\n",
        "\n",
        "    clean = v.copy()\n",
        "    clean.loc[crit_low | crit_high] = np.nan\n",
        "    clean.loc[warn_low]  = lower_ok\n",
        "    clean.loc[warn_high] = upper_ok\n",
        "\n",
        "    df_target[flag_col] = qc\n",
        "    df_target[flag_col.replace(\"_range\",\"_anycrit\")] = qc.eq(\"CRIT_out_of_range\")\n",
        "    df_target[flag_col.replace(\"_range\",\"_anywarn\")] = qc.eq(\"WARN_boundary\")\n",
        "    df_target[clean_col] = pd.to_numeric(clean, errors=\"coerce\")\n",
        "\n",
        "    # Masked share (CRIT only): raw present & clean missing\n",
        "    raw_present = v.notna()\n",
        "    masked_share = float((raw_present & df_target[clean_col].isna()).sum()) / (int(raw_present.sum()) or 1)\n",
        "    return {\n",
        "        \"var\": series.name,\n",
        "        \"n_OK\": int((qc == \"OK\").sum()),\n",
        "        \"n_WARN_boundary\": int((qc == \"WARN_boundary\").sum()),\n",
        "        \"n_CRIT_out_of_range\": int((qc == \"CRIT_out_of_range\").sum()),\n",
        "        \"masked_share\": masked_share,\n",
        "    }\n",
        "\n",
        "# ---------------- Apply range QC ----------------\n",
        "summaries = []\n",
        "summaries.append(apply_range_qc(\n",
        "    df[RAD_COL], RAD_LOWER_OK, RAD_UPPER_OK, RAD_LOWER_TOL, RAD_UPPER_TOL,\n",
        "    flag_col=\"qc_rad_range\", clean_col=RAD_CLEAN, df_target=df))\n",
        "\n",
        "summaries.append(apply_range_qc(\n",
        "    df[W_MEAN], W_LOWER_OK, W_UPPER_OK, W_LOWER_TOL, W_UPPER_TOL,\n",
        "    flag_col=\"qc_wind_mean_range\", clean_col=W_MEAN_C, df_target=df))\n",
        "\n",
        "summaries.append(apply_range_qc(\n",
        "    df[W_MIN], W_LOWER_OK, W_UPPER_OK, W_LOWER_TOL, W_UPPER_TOL,\n",
        "    flag_col=\"qc_wind_min_range\", clean_col=W_MIN_C, df_target=df))\n",
        "\n",
        "summaries.append(apply_range_qc(\n",
        "    df[W_MAX], W_LOWER_OK, W_UPPER_OK, W_LOWER_TOL, W_UPPER_TOL,\n",
        "    flag_col=\"qc_wind_max_range\", clean_col=W_MAX_C, df_target=df))\n",
        "\n",
        "# ---------------- Wind internal consistency (diagnostic only) ----------------\n",
        "# min <= mean <= max; otherwise WARN_inconsistent\n",
        "order_ok = (\n",
        "    (df[[W_MIN, W_MEAN, W_MAX]].notna().all(axis=1)) &\n",
        "    (df[W_MIN] <= df[W_MEAN]) & (df[W_MEAN] <= df[W_MAX])\n",
        ")\n",
        "df[\"qc_wind_order\"] = np.where(order_ok | df[[W_MIN, W_MEAN, W_MAX]].isna().any(axis=1),\n",
        "                               \"OK\", \"WARN_inconsistent\")\n",
        "\n",
        "# ---------------- Save outputs ----------------\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL] if NODE_COL in df.columns else [TS_NS_COL, ROW_ID_COL],\n",
        "               inplace=True, ignore_index=True)\n",
        "\n",
        "out_main = next_versioned_csv(OUT_DIR, \"stage8_rad_wind_range\")\n",
        "df.to_csv(out_main, index=False)\n",
        "print(\"Wrote:\", out_main)\n",
        "\n",
        "# Overall summary CSV\n",
        "sum_df = pd.DataFrame(summaries)\n",
        "sum_path = next_versioned_csv(OUT_DIR, \"stage8_rad_wind_range_counts_overall\")\n",
        "sum_df.to_csv(sum_path, index=False)\n",
        "print(\"Wrote overall counts:\", sum_path)\n",
        "print(\"\\nOverall range QC (radiation & wind):\")\n",
        "print(sum_df.to_string(index=False))\n",
        "\n",
        "# Per-node summaries\n",
        "def pernode_counts(flag_col: str, label: str):\n",
        "    key = NODE_COL if NODE_COL in df.columns else None\n",
        "    if key is None:\n",
        "        return pd.DataFrame(columns=[\"node\",\"which\",\"flag\",\"n\"])\n",
        "    tmp = (df.groupby([key, flag_col]).size()\n",
        "             .rename(\"n\").reset_index()\n",
        "             .sort_values([key, flag_col]))\n",
        "    tmp.insert(1, \"which\", label)\n",
        "    tmp.rename(columns={key: \"node\"}, inplace=True)\n",
        "    return tmp\n",
        "\n",
        "pernode = pd.concat([\n",
        "    pernode_counts(\"qc_rad_range\", \"radiation\"),\n",
        "    pernode_counts(\"qc_wind_mean_range\", \"wind_mean\"),\n",
        "    pernode_counts(\"qc_wind_min_range\",  \"wind_min\"),\n",
        "    pernode_counts(\"qc_wind_max_range\",  \"wind_max\"),\n",
        "    pernode_counts(\"qc_wind_order\",      \"wind_order\"),\n",
        "], ignore_index=True)\n",
        "\n",
        "pernode_path = next_versioned_csv(OUT_DIR, \"stage8_rad_wind_range_counts_pernode\")\n",
        "pernode.to_csv(pernode_path, index=False)\n",
        "print(\"Wrote per-node counts:\", pernode_path)\n",
        "\n",
        "# Breadcrumbs\n",
        "tmin = pd.to_datetime(pd.to_numeric(df[TS_NS_COL], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\").min()\n",
        "tmax = pd.to_datetime(pd.to_numeric(df[TS_NS_COL], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\").max()\n",
        "print(\"\\nTime span (dataset, from ts_ns):\", tmin, \"→\", tmax)\n",
        "print(\"Rows in/out (should match input):\", n_in, \"→\", len(df))\n",
        "if NODE_COL in df.columns:\n",
        "    print(\"Nodes:\", sorted(pd.Series(df[NODE_COL]).dropna().astype(str).unique().tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv1cOv9irrF6",
        "outputId": "c4f00401-6fc8-405e-a805-f70a61ea6196"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage 8] Reading: af_clean_v01/stage7b_air_hum_qc_v001.csv\n",
            "[Stage 8] Using node column: 'node'\n",
            "Wrote: af_clean_v01/stage8_rad_wind_range_v001.csv\n",
            "Wrote overall counts: af_clean_v01/stage8_rad_wind_range_counts_overall_v001.csv\n",
            "\n",
            "Overall range QC (radiation & wind):\n",
            "             var   n_OK  n_WARN_boundary  n_CRIT_out_of_range  masked_share\n",
            "global_radiation 482121              793                  873      0.001805\n",
            " wind_speed_mean 483787                0                    0      0.000000\n",
            "  wind_speed_min 483787                0                    0      0.000000\n",
            "  wind_speed_max 483787                0                    0      0.000000\n",
            "Wrote per-node counts: af_clean_v01/stage8_rad_wind_range_counts_pernode_v001.csv\n",
            "\n",
            "Time span (dataset, from ts_ns): 2023-10-30 12:18:43+00:00 → 2025-09-18 10:29:42+00:00\n",
            "Rows in/out (should match input): 483787 → 483787\n",
            "Nodes: ['node_176', 'node_179', 'node_181', 'node_182', 'node_183', 'node_184', 'node_186', 'node_187', 'node_189']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: assemble “analysis_ready” (no ST ramp)**\n",
        "\n",
        "* Pick input: Loads the most advanced prior stage it can find (prefers Stage 8, then 7, 6, 5…).\n",
        "* Core setup: Verifies required IDs/time columns, builds a canonical timestamp from ts_ns, drops stale soil-temp ramp flags.\n",
        "* Soil moisture: Prefers Step 5PD outputs (pair-diverge) to fill *_clean + all SM QC flags; otherwise falls back to 5R.\n",
        "* Known-broken backfill: Pulls Stage 3 “known broken” flags for SM/ST if any are missing.\n",
        "* Soil temperature: Backfills ST *_clean + QC from 6a/6b/6c if missing; *masks _clean where known-broken and updates qc_st_anycrit.\n",
        "* Radiation & wind: Backfills *_clean + range/QC from Stage 8 (and sets benign defaults if still absent).\n",
        "* Air (T/RH/Pres): Imports __pref and QC from 7a/7b/7c when available; otherwise constructs __pref from raw masked by the step-7 QC labels; adds ok_temperature/ok_humidity/ok_pressure.\n",
        "* Preferred series: Creates __pref for SM, ST, radiation, wind from their *_clean (or raw if clean missing).\n",
        "* OK booleans: Builds ok_* for SM/ST from *_anycrit, and for RAD/W from range flags.\n",
        "* Row rollups: Maintains qc_air_anywarn/anycrit, row_has_warn/crit.\n",
        "* Export (reproducible): Selects a fixed column order: IDs/time → all __pref → all ok_* → all QC → key *_clean → raw columns; writes a versioned CSV (analysis_ready_v###.csv) and a Parquet copy if possible; includes quoting fallback + console summary."
      ],
      "metadata": {
        "id": "TTP6leZwD06m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 9: Assemble analysis-ready dataset (no 6d/ramp)\n",
        "# - Respects Step 5PD (SM pair-diverge) and Step 7 (air masking)\n",
        "# - Exports *_clean (SM/ST/RAD/W), all __pref, all QC flags, ok_* booleans\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core identity/time columns\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"   # in-memory datetime64[ns]\n",
        "NODE_COL      = \"node\"\n",
        "NODE_ID_COL   = \"node_id\"\n",
        "STATION_TYPE  = \"station_type\"\n",
        "\n",
        "# Variable families\n",
        "SM_COLS  = [\"ground_humidity_0\", \"ground_humidity_1\"]\n",
        "ST_COLS  = [\"ground_temp_0\", \"ground_temp_1\"]\n",
        "RAD_COL  = \"global_radiation\"\n",
        "W_COLS   = [\"wind_speed_min\", \"wind_speed_mean\", \"wind_speed_max\"]\n",
        "AIR_COLS = [\"temperature\", \"humidity\", \"pressure\"]  # Step 7 provides __pref + QC\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "def pick_latest_by_priority(patterns):\n",
        "    \"\"\"Return latest path from the FIRST pattern that has matches.\"\"\"\n",
        "    for pat in patterns:\n",
        "        files = sorted(glob(str(OUT_DIR / pat)))\n",
        "        if files:\n",
        "            print(f\"[pick] {pat} -> {files[-1]}\")\n",
        "            return files[-1]\n",
        "        else:\n",
        "            print(f\"[pick] {pat} -> (no match)\")\n",
        "    return None\n",
        "\n",
        "def merge_missing_cols_on_row_id(df_base: pd.DataFrame, src_path: str, want_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"Only add columns that are currently missing; avoids duplicate merges.\"\"\"\n",
        "    if not want_cols or src_path is None:\n",
        "        return df_base\n",
        "    hdr = pd.read_csv(src_path, nrows=0, low_memory=False)\n",
        "    have = [c for c in want_cols if (c in hdr.columns) and (c not in df_base.columns)]\n",
        "    if not have:\n",
        "        return df_base\n",
        "    usecols = [ROW_ID_COL] + have\n",
        "    add = pd.read_csv(src_path, usecols=usecols, low_memory=False)\n",
        "    return df_base.merge(add, on=ROW_ID_COL, how=\"left\")\n",
        "\n",
        "def replace_cols_on_row_id(df_base: pd.DataFrame, src_path: str, want_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"Overwrite df_base[want_cols] using src_path (matched by row_id).\"\"\"\n",
        "    if not want_cols or src_path is None:\n",
        "        return df_base\n",
        "    hdr = pd.read_csv(src_path, nrows=0, low_memory=False)\n",
        "    have = [c for c in want_cols if c in hdr.columns]\n",
        "    if not have:\n",
        "        return df_base\n",
        "    usecols = [ROW_ID_COL] + have\n",
        "    add = pd.read_csv(src_path, usecols=usecols, low_memory=False)\n",
        "    df_out = df_base.drop(columns=[c for c in have if c in df_base.columns], errors=\"ignore\")\n",
        "    return df_out.merge(add, on=ROW_ID_COL, how=\"left\")\n",
        "\n",
        "# ---- Load the most advanced stage available (prefer Stage 8) ----\n",
        "in_path = pick_latest_by_priority([\n",
        "    \"stage8_rad_wind_range_v*.csv\",\n",
        "    \"stage7c_air_pres_qc_v*.csv\",\n",
        "    \"stage7b_air_hum_qc_v*.csv\",\n",
        "    \"stage7a_air_temp_qc_v*.csv\",\n",
        "    \"stage6c_soiltemp_step_v*.csv\",\n",
        "    \"stage6b_soiltemp_flatline_v*.csv\",\n",
        "    \"stage6a_soiltemp_range_v*.csv\",\n",
        "    \"stage5pd_sm_pairdiverge_v*.csv\",\n",
        "    \"stage5r_sm_regimes_v*.csv\",\n",
        "    \"stage4_sm_flatline_v*.csv\",\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage2_soilmoisture_range_v*.csv\",\n",
        "    \"stage1_parsed_v*.csv\",\n",
        "])\n",
        "assert in_path is not None, \"No prior stage files found.\"\n",
        "print(\"Stage 9 INPUT:\", in_path)\n",
        "\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "n_in = len(df)\n",
        "\n",
        "# ---- Core checks & authoritative timestamp ----\n",
        "required_core = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, NODE_COL, NODE_ID_COL, STATION_TYPE}\n",
        "missing_core = required_core - set(df.columns)\n",
        "if missing_core:\n",
        "    raise ValueError(f\"Missing core columns: {sorted(missing_core)}\")\n",
        "\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(df[TS_NS_COL].astype(\"int64\"))\n",
        "assert df[ROW_ID_COL].is_unique, \"row_id should be unique from Stage 1.\"\n",
        "\n",
        "# ---- Drop stale soil-temp ramp flags (6d removed) ----\n",
        "ramp_cols = [c for c in df.columns if c.startswith(\"qc_st_ramp_\")]\n",
        "if ramp_cols:\n",
        "    print(f\"[Stage 9] Dropping stale ramp flags: {ramp_cols}\")\n",
        "    df.drop(columns=ramp_cols, inplace=True, errors=\"ignore\")\n",
        "\n",
        "# ========= Soil moisture: prefer 5PD (pair-diverge) over 5R =========\n",
        "sm_5pd_src = pick_latest_by_priority([\"stage5pd_sm_pairdiverge_v*.csv\"])\n",
        "SM_FROM_5PD = [\n",
        "    \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "    \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "    \"qc_sm_range_0\",\"qc_sm_range_1\",\n",
        "    \"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "    \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "    \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\n",
        "    \"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "    \"qc_sm_regime_0\",\"qc_sm_regime_1\"\n",
        "]\n",
        "if sm_5pd_src:\n",
        "    df = replace_cols_on_row_id(df, sm_5pd_src, SM_FROM_5PD)\n",
        "    print(f\"[Stage 9] Replaced soil-moisture clean/QC from 5PD: {sm_5pd_src}\")\n",
        "else:\n",
        "    step5r_src = pick_latest_by_priority([\"stage5r_sm_regimes_v*.csv\"])\n",
        "    if step5r_src:\n",
        "        SM_FROM_5R = [c for c in SM_FROM_5PD if not c.startswith(\"qc_sm_pairdiverge_\")]\n",
        "        df = replace_cols_on_row_id(df, step5r_src, SM_FROM_5R)\n",
        "        print(f\"[Stage 9] Replaced soil-moisture clean/QC from 5R: {step5r_src}\")\n",
        "    else:\n",
        "        print(\"[Stage 9] WARNING: No 5PD/5R file found; SM clean/QC may be stale.\")\n",
        "\n",
        "# ---------- Bring in Stage 3 known-broken (SM + ST) if any columns missing ----------\n",
        "ST3_SRC = pick_latest_by_priority([\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage3_*knownbroken_v*.csv\",\n",
        "])\n",
        "KB_WANT = [\n",
        "    \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "    \"qc_st_known_broken_0\",\"qc_st_known_broken_1\",\n",
        "]\n",
        "missing_kb = [c for c in KB_WANT if c not in df.columns]\n",
        "if ST3_SRC and missing_kb:\n",
        "    df = merge_missing_cols_on_row_id(df, ST3_SRC, missing_kb)\n",
        "    print(f\"[Stage 9] Backfilled known-broken flags from: {ST3_SRC}\")\n",
        "\n",
        "# ---------- Backfill (only if missing) for Soil Temperature (no ramp) ----------\n",
        "ST_QC_WANT = [\n",
        "    \"qc_st_range_0\",\"qc_st_range_1\",\n",
        "    \"qc_st_flatline_0\",\"qc_st_flatline_1\",\n",
        "    \"qc_st_step_0\",\"qc_st_step_1\",\n",
        "    \"qc_st_anycrit_0\",\"qc_st_anycrit_1\",\n",
        "    \"qc_st_anywarn_0\",\"qc_st_anywarn_1\",\n",
        "    \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "]\n",
        "st_missing = [c for c in ST_QC_WANT if c not in df.columns]\n",
        "if st_missing:\n",
        "    st_src = pick_latest_by_priority([\n",
        "        \"stage6c_soiltemp_step_v*.csv\",\n",
        "        \"stage6b_soiltemp_flatline_v*.csv\",\n",
        "        \"stage6a_soiltemp_range_v*.csv\",\n",
        "    ])\n",
        "    if st_src:\n",
        "        df = merge_missing_cols_on_row_id(df, st_src, st_missing)\n",
        "        print(f\"[Stage 9] Backfilled ST clean/QC from: {st_src}\")\n",
        "\n",
        "# ---------- ENFORCE: soil-temp known-broken masking & anycrit ----------\n",
        "def coerce_bool_from_anycrit_like(s: pd.Series) -> pd.Series:\n",
        "    \"\"\"Accept bool series or strings like 'CRIT_*'/'OK' and return boolean.\"\"\"\n",
        "    if s.dtype == bool:\n",
        "        return s\n",
        "    return s.astype(str).str.startswith(\"CRIT\")\n",
        "\n",
        "for suf in (\"0\",\"1\"):\n",
        "    kbcol_st = f\"qc_st_known_broken_{suf}\"\n",
        "    clean    = f\"ground_temp_{suf}_clean\"\n",
        "    anycrit  = f\"qc_st_anycrit_{suf}\"\n",
        "    raw      = f\"ground_temp_{suf}\"\n",
        "    if clean not in df.columns and raw in df.columns:\n",
        "        df[clean] = pd.to_numeric(df[raw], errors=\"coerce\")\n",
        "    if kbcol_st in df.columns:\n",
        "        kb = df[kbcol_st].astype(str).str.startswith(\"CRIT\")\n",
        "        if clean in df.columns:\n",
        "            df.loc[kb, clean] = np.nan\n",
        "        if anycrit in df.columns:\n",
        "            df[anycrit] = coerce_bool_from_anycrit_like(df[anycrit]) | kb\n",
        "        else:\n",
        "            df[anycrit] = kb\n",
        "\n",
        "# ---------- Backfill (only if missing) for Radiation & Wind ----------\n",
        "RW_QC_WANT = [\n",
        "    \"qc_rad_range\", \"qc_rad_anycrit\", \"qc_rad_anywarn\", f\"{RAD_COL}_clean\",\n",
        "    \"qc_wind_mean_range\", \"qc_wind_mean_anycrit\", \"qc_wind_mean_anywarn\", f\"{W_COLS[1]}_clean\",\n",
        "    \"qc_wind_min_range\",  \"qc_wind_min_anycrit\",  \"qc_wind_min_anywarn\",  f\"{W_COLS[0]}_clean\",\n",
        "    \"qc_wind_max_range\",  \"qc_wind_max_anycrit\",  \"qc_wind_max_anywarn\",  f\"{W_COLS[2]}_clean\",\n",
        "    \"qc_wind_order\",\n",
        "]\n",
        "rw_missing = [c for c in RW_QC_WANT if c not in df.columns]\n",
        "if rw_missing:\n",
        "    rw_src = pick_latest_by_priority([\"stage8_rad_wind_range_v*.csv\"])\n",
        "    if rw_src:\n",
        "        df = merge_missing_cols_on_row_id(df, rw_src, rw_missing)\n",
        "        print(f\"[Stage 9] Backfilled RAD/WIND clean/QC from: {rw_src}\")\n",
        "# Default OKs if still absent\n",
        "for c in [\"qc_rad_range\",\"qc_wind_mean_range\",\"qc_wind_min_range\",\"qc_wind_max_range\",\"qc_wind_order\"]:\n",
        "    if c not in df.columns:\n",
        "        df[c] = \"OK\"\n",
        "\n",
        "# ---------- AIR: bring QC + inherit masked __pref if present; else rebuild from QC ----------\n",
        "AIR_QC_WANT = [\"qc_air_selfcheck_temp\",\"qc_air_selfcheck_hum\",\"qc_air_crosscheck_pres\"]\n",
        "for pat in [\"stage7c_air_pres_qc_v*.csv\", \"stage7b_air_hum_qc_v*.csv\", \"stage7a_air_temp_qc_v*.csv\"]:\n",
        "    air_src = pick_latest_by_priority([pat])\n",
        "    if air_src:\n",
        "        df = merge_missing_cols_on_row_id(df, air_src, AIR_QC_WANT + [\"temperature__pref\",\"humidity__pref\",\"pressure__pref\",\"humidity_mask_reason\"])\n",
        "# normalize legacy names (rare)\n",
        "if \"qc_air_selfcheck_temp\" not in df.columns and \"qc_air_crosscheck_temp\" in df.columns:\n",
        "    df[\"qc_air_selfcheck_temp\"] = df[\"qc_air_crosscheck_temp\"]\n",
        "if \"qc_air_selfcheck_hum\" not in df.columns and \"qc_air_crosscheck_hum\" in df.columns:\n",
        "    df[\"qc_air_selfcheck_hum\"] = df[\"qc_air_crosscheck_hum\"]\n",
        "for c in [\"qc_air_crosscheck_temp\",\"qc_air_crosscheck_hum\"]:\n",
        "    if c in df.columns:\n",
        "        df.drop(columns=c, inplace=True)\n",
        "\n",
        "# --- Build preferred series ---\n",
        "def preferred(df_: pd.DataFrame, raw: str) -> pd.Series:\n",
        "    c = f\"{raw}_clean\"\n",
        "    return pd.to_numeric(df_[c], errors=\"coerce\") if c in df_.columns else pd.to_numeric(df_[raw], errors=\"coerce\")\n",
        "\n",
        "# SM/ST/RAD/W from *_clean when present\n",
        "pref = {}\n",
        "for c in SM_COLS + ST_COLS:\n",
        "    pref[c] = preferred(df, c)\n",
        "pref[RAD_COL] = preferred(df, RAD_COL)\n",
        "for c in W_COLS:\n",
        "    pref[c] = preferred(df, c)\n",
        "\n",
        "# Assign these __pref\n",
        "for name, series in pref.items():\n",
        "    df[f\"{name}__pref\"] = series\n",
        "\n",
        "# AIR: if __pref already imported from Step 7, keep it; otherwise build from QC\n",
        "for base, qc in [\n",
        "    (\"temperature\", \"qc_air_selfcheck_temp\"),\n",
        "    (\"humidity\",    \"qc_air_selfcheck_hum\"),\n",
        "    (\"pressure\",    \"qc_air_crosscheck_pres\"),\n",
        "]:\n",
        "    if base not in df.columns:\n",
        "        continue\n",
        "    pref_col = f\"{base}__pref\"\n",
        "    if pref_col in df.columns:\n",
        "        df[pref_col] = pd.to_numeric(df[pref_col], errors=\"coerce\")\n",
        "    else:\n",
        "        raw = pd.to_numeric(df[base], errors=\"coerce\")\n",
        "        if qc in df.columns:\n",
        "            bad = df[qc].astype(str).str.startswith((\"WARN\",\"CRIT\"), na=False)\n",
        "            df[pref_col] = raw.mask(bad)\n",
        "        else:\n",
        "            df[pref_col] = raw\n",
        "    df[f\"ok_{base}\"] = df[pref_col].notna()\n",
        "\n",
        "def anycrit_bool(df_: pd.DataFrame, col: str) -> pd.Series:\n",
        "    if col in df_.columns:\n",
        "        s = df_[col]\n",
        "        return s if s.dtype == bool else s.astype(str).str.startswith(\"CRIT\")\n",
        "    return pd.Series(False, index=df_.index, dtype=bool)\n",
        "\n",
        "# ok_* for SM/ST from anycrit (legacy convention)\n",
        "df[\"ok_sm0\"] = ~anycrit_bool(df, \"qc_sm_anycrit_0\")\n",
        "df[\"ok_sm1\"] = ~anycrit_bool(df, \"qc_sm_anycrit_1\")\n",
        "df[\"ok_st0\"] = ~anycrit_bool(df, \"qc_st_anycrit_0\")\n",
        "df[\"ok_st1\"] = ~anycrit_bool(df, \"qc_st_anycrit_1\")\n",
        "\n",
        "# Radiation & wind OK from explicit range flags (CRIT only masks)\n",
        "df[\"ok_rad\"]   = ~df[\"qc_rad_range\"].astype(str).eq(\"CRIT_out_of_range\")\n",
        "df[\"ok_wmin\"]  = ~df[\"qc_wind_min_range\"].astype(str).eq(\"CRIT_out_of_range\")\n",
        "df[\"ok_wmean\"] = ~df[\"qc_wind_mean_range\"].astype(str).eq(\"CRIT_out_of_range\")\n",
        "df[\"ok_wmax\"]  = ~df[\"qc_wind_max_range\"].astype(str).eq(\"CRIT_out_of_range\")\n",
        "\n",
        "# Row-level rollups for air (optional, retained for continuity)\n",
        "def _startswith(col, prefix):\n",
        "    return df[col].astype(str).str.startswith(prefix, na=False) if col in df.columns else pd.Series(False, index=df.index)\n",
        "\n",
        "air_warn = (_startswith(\"qc_air_selfcheck_temp\",\"WARN\") |\n",
        "            _startswith(\"qc_air_selfcheck_hum\",\"WARN\")  |\n",
        "            _startswith(\"qc_air_crosscheck_pres\",\"WARN\"))\n",
        "air_crit = (_startswith(\"qc_air_selfcheck_temp\",\"CRIT\") |\n",
        "            _startswith(\"qc_air_selfcheck_hum\",\"CRIT\")  |\n",
        "            _startswith(\"qc_air_crosscheck_pres\",\"CRIT\"))\n",
        "df[\"qc_air_anywarn\"] = df.get(\"qc_air_anywarn\", False) | air_warn\n",
        "df[\"qc_air_anycrit\"] = df.get(\"qc_air_anycrit\", False) | air_crit\n",
        "df[\"row_has_warn\"] = df.get(\"row_has_warn\", False) | df[\"qc_air_anywarn\"]\n",
        "df[\"row_has_crit\"] = df.get(\"row_has_crit\", False) | df[\"qc_air_anycrit\"]\n",
        "\n",
        "# =============== Choose & order columns for export ===============\n",
        "id_cols   = [ROW_ID_COL, NODE_ID_COL, NODE_COL, STATION_TYPE]\n",
        "time_cols = [TS_NS_COL, TS_ISO_COL, TIMESTAMP_COL]\n",
        "\n",
        "pref_cols = (\n",
        "    [f\"{c}__pref\" for c in SM_COLS] +\n",
        "    [f\"{c}__pref\" for c in ST_COLS] +\n",
        "    [f\"{RAD_COL}__pref\"] +\n",
        "    [f\"{c}__pref\" for c in W_COLS] +\n",
        "    [f\"{c}__pref\" for c in AIR_COLS]\n",
        ")\n",
        "\n",
        "ok_cols = [\n",
        "    \"ok_sm0\",\"ok_sm1\",\"ok_st0\",\"ok_st1\",\n",
        "    \"ok_rad\",\"ok_wmin\",\"ok_wmean\",\"ok_wmax\",\n",
        "    \"ok_temperature\",\"ok_humidity\",\"ok_pressure\",\n",
        "]\n",
        "\n",
        "# Keep *_clean for SM/ST/RAD/W so plotting can compare raw vs clean directly\n",
        "CLEAN_KEEP = [\n",
        "    \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "    \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "    \"global_radiation_clean\",\n",
        "    \"wind_speed_min_clean\",\"wind_speed_mean_clean\",\"wind_speed_max_clean\",\n",
        "]\n",
        "clean_present = [c for c in CLEAN_KEEP if c in df.columns]\n",
        "\n",
        "# Keep air mask reason if present (from 7b lenient)\n",
        "AIR_META_KEEP = [c for c in [\"humidity_mask_reason\"] if c in df.columns]\n",
        "\n",
        "# all QC flags\n",
        "qc_cols = [c for c in df.columns if c.startswith(\"qc_\")]\n",
        "\n",
        "# raw columns for traceability\n",
        "raw_cols = list({*SM_COLS, *ST_COLS, RAD_COL, *W_COLS, *AIR_COLS})\n",
        "\n",
        "export_cols = id_cols + time_cols + pref_cols + ok_cols + qc_cols + clean_present + AIR_META_KEEP\n",
        "raw_cols = [c for c in raw_cols if c not in export_cols]\n",
        "export_cols += raw_cols\n",
        "\n",
        "# Deterministic order & write\n",
        "df.sort_values([NODE_COL, TS_NS_COL, ROW_ID_COL], inplace=True, ignore_index=True)\n",
        "out_df = df[export_cols].copy()\n",
        "\n",
        "# Clean object columns for CSV\n",
        "obj_cols = out_df.select_dtypes(include=\"object\").columns\n",
        "if len(obj_cols):\n",
        "    out_df[obj_cols] = out_df[obj_cols].replace({r'[\\r\\n]': ' ', '\"': '\"\"'}, regex=True)\n",
        "\n",
        "out_main = next_versioned_csv(OUT_DIR, \"analysis_ready\")\n",
        "\n",
        "# First attempt: minimal quoting\n",
        "out_df.to_csv(out_main, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
        "\n",
        "# Verify; if mismatch, rewrite with QUOTE_ALL\n",
        "_ok = True\n",
        "try:\n",
        "    _test = pd.read_csv(out_main, engine=\"c\", low_memory=False)\n",
        "    if list(_test.columns) != list(out_df.columns):\n",
        "        _ok = False\n",
        "except Exception as e:\n",
        "    print(\"[Step 9] Re-read failed:\", e)\n",
        "    _ok = False\n",
        "\n",
        "if not _ok:\n",
        "    print(\"[Step 9] Rewriting with QUOTE_ALL for robustness…\")\n",
        "    out_df.to_csv(out_main, index=False, encoding=\"utf-8\",\n",
        "                  quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    _test = pd.read_csv(out_main, engine=\"python\", low_memory=False)\n",
        "    assert list(_test.columns) == list(out_df.columns), \"Post-rewrite verification failed.\"\n",
        "\n",
        "print(\"Wrote analysis-ready file:\", out_main)\n",
        "\n",
        "# Optional: Parquet copy\n",
        "try:\n",
        "    out_parq = out_main.with_suffix(\".parquet\")\n",
        "    out_df.to_parquet(out_parq, index=False)\n",
        "    print(\"Wrote Parquet copy:\", out_parq)\n",
        "except Exception as e:\n",
        "    print(\"Parquet write skipped:\", e)\n",
        "\n",
        "# ---- Small console summary\n",
        "print(\"\\nPreview of columns included:\")\n",
        "print(export_cols[:30], \"...\")\n",
        "print(f\"Total columns exported: {len(export_cols)}\")\n",
        "tmin = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).min()\n",
        "tmax = pd.to_datetime(df[TS_NS_COL].astype(\"int64\")).max()\n",
        "print(\"Time span (from ts_ns):\", tmin, \"→\", tmax)\n",
        "print(\"Rows in/out (should match input):\", n_in, \"→\", len(df))\n",
        "\n",
        "# Post-save sanity: quick presence checks\n",
        "hdr_ar = pd.read_csv(out_main, nrows=0)\n",
        "print(\"Has SM *_clean in AR?\",\n",
        "      all([(c in hdr_ar.columns) for c in [\"ground_humidity_0_clean\",\"ground_humidity_1_clean\"] if c in clean_present]) )\n",
        "print(\"Has air __pref in AR?\",\n",
        "      all([(f\"{c}__pref\" in hdr_ar.columns) for c in AIR_COLS]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aDSUXPewU5F",
        "outputId": "51d0ca46-9106-4bb3-cb79-b69192116bea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pick] stage8_rad_wind_range_v*.csv -> af_clean_v01/stage8_rad_wind_range_v001.csv\n",
            "Stage 9 INPUT: af_clean_v01/stage8_rad_wind_range_v001.csv\n",
            "[pick] stage5pd_sm_pairdiverge_v*.csv -> af_clean_v01/stage5pd_sm_pairdiverge_v001.csv\n",
            "[Stage 9] Replaced soil-moisture clean/QC from 5PD: af_clean_v01/stage5pd_sm_pairdiverge_v001.csv\n",
            "[pick] stage3_sm_knownbroken_v*.csv -> af_clean_v01/stage3_sm_knownbroken_v001.csv\n",
            "[pick] stage6c_soiltemp_step_v*.csv -> (no match)\n",
            "[pick] stage6b_soiltemp_flatline_v*.csv -> af_clean_v01/stage6b_soiltemp_flatline_v001.csv\n",
            "[Stage 9] Backfilled ST clean/QC from: af_clean_v01/stage6b_soiltemp_flatline_v001.csv\n",
            "[pick] stage7c_air_pres_qc_v*.csv -> af_clean_v01/stage7c_air_pres_qc_v001.csv\n",
            "[pick] stage7b_air_hum_qc_v*.csv -> af_clean_v01/stage7b_air_hum_qc_v001.csv\n",
            "[pick] stage7a_air_temp_qc_v*.csv -> af_clean_v01/stage7a_air_temp_qc_v001.csv\n",
            "Wrote analysis-ready file: af_clean_v01/analysis_ready_v001.csv\n",
            "Wrote Parquet copy: af_clean_v01/analysis_ready_v001.parquet\n",
            "\n",
            "Preview of columns included:\n",
            "['row_id', 'node_id', 'node', 'station_type', 'ts_ns', 'timestamp_iso', 'timestamp', 'ground_humidity_0__pref', 'ground_humidity_1__pref', 'ground_temp_0__pref', 'ground_temp_1__pref', 'global_radiation__pref', 'wind_speed_min__pref', 'wind_speed_mean__pref', 'wind_speed_max__pref', 'temperature__pref', 'humidity__pref', 'pressure__pref', 'ok_sm0', 'ok_sm1', 'ok_st0', 'ok_st1', 'ok_rad', 'ok_wmin', 'ok_wmean', 'ok_wmax', 'ok_temperature', 'ok_humidity', 'ok_pressure', 'qc_st_anycrit_0'] ...\n",
            "Total columns exported: 98\n",
            "Time span (from ts_ns): 2023-10-30 12:18:43 → 2025-09-18 10:29:42\n",
            "Rows in/out (should match input): 483787 → 483787\n",
            "Has SM *_clean in AR? True\n",
            "Has air __pref in AR? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 9 (analysis_ready)**\n",
        "* Loaded the latest pipeline table (preferring Step 8) and rebuilt timestamp from ts_ns; verified no row loss and that row_id is unique.\n",
        "* Chose a “preferred” value for each variable family, creating __pref columns:\n",
        "* Uses *_clean if present (i.e., after masking CRIT and clamping WARN), otherwise falls back to the raw column.\n",
        "* Built for: soil moisture (ground_humidity_0/1), soil temperature (ground_temp_0/1), radiation, wind (min/mean/max), and air triad (temperature/humidity/pressure—raw as there’s no masking step for air).\n",
        "* Kept all QC flags (every column starting with qc_) so you can filter by them during analysis.\n",
        "* Added convenience booleans that are True when the variable passed our CRIT masks:\n",
        "* ok_sm0, ok_sm1, ok_st0, ok_st1, ok_rad, ok_wmin, ok_wmean, ok_wmax.\n",
        "* Preserved provenance columns: IDs (row_id, node_id, node, station_type) and time (ts_ns, timestamp_iso, reconstructed timestamp).\n",
        "* Optionally retained all raw measurement columns alongside the __pref series for traceability.\n",
        "* Sorted deterministically by node, ts_ns, row_id and wrote a versioned analysis_ready_v###.csv.\n",
        "* Printed breadcrumbs: preview of exported columns, total columns, min/max time (from ts_ns), and row-in/out equality."
      ],
      "metadata": {
        "id": "SN_nqVtSDWoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 9: Assemble analysis-ready dataset (no 6d/ramp)\n",
        "# - Prefers 6PD (soil-temp pair-diverge) and 5PD (soil-moisture pair-diverge)\n",
        "# - Preserves Step 7 air masks; lenient humidity fallback baked-in\n",
        "# - Exports *_clean (SM/ST/RAD/W), all __pref, all QC flags, ok_* booleans\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import csv, os, re\n",
        "\n",
        "OUT_DIR = Path(\"af_clean_v01\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Core identity/time columns\n",
        "ROW_ID_COL    = \"row_id\"\n",
        "TS_NS_COL     = \"ts_ns\"\n",
        "TS_ISO_COL    = \"timestamp_iso\"\n",
        "TIMESTAMP_COL = \"timestamp\"   # in-memory datetime64[ns]\n",
        "NODE_COL      = \"node\"\n",
        "NODE_ID_COL   = \"node_id\"\n",
        "STATION_TYPE  = \"station_type\"\n",
        "\n",
        "# Variable families\n",
        "SM_COLS  = [\"ground_humidity_0\", \"ground_humidity_1\"]\n",
        "ST_COLS  = [\"ground_temp_0\", \"ground_temp_1\"]\n",
        "RAD_COL  = \"global_radiation\"\n",
        "W_COLS   = [\"wind_speed_min\", \"wind_speed_mean\", \"wind_speed_max\"]\n",
        "AIR_COLS = [\"temperature\", \"humidity\", \"pressure\"]  # Step 7 provides __pref + QC\n",
        "\n",
        "def next_versioned_csv(out_dir: Path, stem: str) -> Path:\n",
        "    i = 1\n",
        "    while True:\n",
        "        p = out_dir / f\"{stem}_v{i:03d}.csv\"\n",
        "        if not p.exists():\n",
        "            return p\n",
        "        i += 1\n",
        "\n",
        "def pick_latest_by_priority(patterns):\n",
        "    \"\"\"Return latest path from the FIRST pattern that has matches (by filename order).\"\"\"\n",
        "    for pat in patterns:\n",
        "        files = sorted(glob(str(OUT_DIR / pat)))\n",
        "        if files:\n",
        "            print(f\"[pick] {pat} -> {files[-1]}\")\n",
        "            return files[-1]\n",
        "        else:\n",
        "            print(f\"[pick] {pat} -> (no match)\")\n",
        "    return None\n",
        "\n",
        "def merge_missing_cols_on_row_id(df_base: pd.DataFrame, src_path: str, want_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"Only add columns that are currently missing; avoids duplicate merges.\"\"\"\n",
        "    if not want_cols or src_path is None:\n",
        "        return df_base\n",
        "    hdr = pd.read_csv(src_path, nrows=0, low_memory=False)\n",
        "    have = [c for c in want_cols if (c in hdr.columns) and (c not in df_base.columns)]\n",
        "    if not have:\n",
        "        return df_base\n",
        "    usecols = [ROW_ID_COL] + have\n",
        "    add = pd.read_csv(src_path, usecols=usecols, low_memory=False)\n",
        "    return df_base.merge(add, on=ROW_ID_COL, how=\"left\")\n",
        "\n",
        "def replace_cols_on_row_id(df_base: pd.DataFrame, src_path: str, want_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"Overwrite df_base[want_cols] using src_path (matched by row_id).\"\"\"\n",
        "    if not want_cols or src_path is None:\n",
        "        return df_base\n",
        "    hdr = pd.read_csv(src_path, nrows=0, low_memory=False)\n",
        "    have = [c for c in want_cols if c in hdr.columns]\n",
        "    if not have:\n",
        "        return df_base\n",
        "    usecols = [ROW_ID_COL] + have\n",
        "    add = pd.read_csv(src_path, usecols=usecols, low_memory=False)\n",
        "    df_out = df_base.drop(columns=[c for c in have if c in df_base.columns], errors=\"ignore\")\n",
        "    return df_out.merge(add, on=ROW_ID_COL, how=\"left\")\n",
        "\n",
        "# ---- Load the most advanced stage available (prefer Stage 8, then 7, then 6PD, …) ----\n",
        "in_path = pick_latest_by_priority([\n",
        "    \"stage8_rad_wind_range_v*.csv\",\n",
        "    \"stage7c_air_pres_qc_v*.csv\",\n",
        "    \"stage7b_air_hum_qc_v*.csv\",\n",
        "    \"stage7a_air_temp_qc_v*.csv\",\n",
        "    \"stage6pd_soiltemp_pairdiverge_v*.csv\",   # ← NEW: recognize 6PD\n",
        "    \"stage6c_soiltemp_step_v*.csv\",\n",
        "    \"stage6b_soiltemp_flatline_v*.csv\",\n",
        "    \"stage6a_soiltemp_range_v*.csv\",\n",
        "    \"stage5pd_sm_pairdiverge_v*.csv\",\n",
        "    \"stage5r_sm_regimes_v*.csv\",\n",
        "    \"stage4_sm_flatline_v*.csv\",\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage2_soilmoisture_range_v*.csv\",\n",
        "    \"stage1_parsed_v*.csv\",\n",
        "])\n",
        "assert in_path is not None, \"No prior stage files found.\"\n",
        "print(\"Stage 9 INPUT:\", in_path)\n",
        "\n",
        "df = pd.read_csv(in_path, low_memory=False)\n",
        "n_in = len(df)\n",
        "\n",
        "# ---- Core checks & authoritative timestamp ----\n",
        "required_core = {ROW_ID_COL, TS_NS_COL, TS_ISO_COL, STATION_TYPE}\n",
        "missing_core = required_core - set(df.columns)\n",
        "if missing_core:\n",
        "    raise ValueError(f\"Missing core columns: {sorted(missing_core)}\")\n",
        "\n",
        "# Authoritative time from ts_ns (tolerant to nulls)\n",
        "df[TIMESTAMP_COL] = pd.to_datetime(pd.to_numeric(df[TS_NS_COL], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "\n",
        "# ---- Drop stale soil-temp ramp flags (6d removed) ----\n",
        "ramp_cols = [c for c in df.columns if c.startswith(\"qc_st_ramp_\")]\n",
        "if ramp_cols:\n",
        "    print(f\"[Stage 9] Dropping stale ramp flags: {ramp_cols}\")\n",
        "    df.drop(columns=ramp_cols, inplace=True, errors=\"ignore\")\n",
        "\n",
        "# ========= Soil moisture: prefer 5PD (pair-diverge) over 5R =========\n",
        "sm_5pd_src = pick_latest_by_priority([\"stage5pd_sm_pairdiverge_v*.csv\"])\n",
        "SM_FROM_5PD = [\n",
        "    \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "    \"qc_sm_pairdiverge_0\",\"qc_sm_pairdiverge_1\",\n",
        "    \"qc_sm_pairdiverge_src_0\",\"qc_sm_pairdiverge_src_1\",\n",
        "    \"qc_sm_pairdiverge_nref_0\",\"qc_sm_pairdiverge_nref_1\",\n",
        "    \"qc_sm_pairdiverge_corr_0\",\"qc_sm_pairdiverge_corr_1\",\n",
        "    \"qc_sm_range_0\",\"qc_sm_range_1\",\n",
        "    \"qc_sm_flatline_0\",\"qc_sm_flatline_1\",\n",
        "    \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "    \"qc_sm_anycrit_0\",\"qc_sm_anycrit_1\",\n",
        "    \"qc_sm_anywarn_0\",\"qc_sm_anywarn_1\",\n",
        "    \"qc_sm_regime_0\",\"qc_sm_regime_1\",\n",
        "]\n",
        "if sm_5pd_src:\n",
        "    df = replace_cols_on_row_id(df, sm_5pd_src, SM_FROM_5PD)\n",
        "    print(f\"[Stage 9] Replaced soil-moisture clean/QC from 5PD: {sm_5pd_src}\")\n",
        "else:\n",
        "    step5r_src = pick_latest_by_priority([\"stage5r_sm_regimes_v*.csv\"])\n",
        "    if step5r_src:\n",
        "        SM_FROM_5R = [c for c in SM_FROM_5PD if not c.startswith(\"qc_sm_pairdiverge\")]\n",
        "        df = replace_cols_on_row_id(df, step5r_src, SM_FROM_5R)\n",
        "        print(f\"[Stage 9] Replaced soil-moisture clean/QC from 5R: {step5r_src}\")\n",
        "    else:\n",
        "        print(\"[Stage 9] WARNING: No 5PD/5R file found; SM clean/QC may be stale.\")\n",
        "\n",
        "# ---------- Bring in Stage 3 known-broken (SM + ST) if any columns missing ----------\n",
        "ST3_SRC = pick_latest_by_priority([\n",
        "    \"stage3_sm_knownbroken_v*.csv\",\n",
        "    \"stage3_*knownbroken_v*.csv\",\n",
        "])\n",
        "KB_WANT = [\n",
        "    \"qc_sm_known_broken_0\",\"qc_sm_known_broken_1\",\n",
        "    \"qc_st_known_broken_0\",\"qc_st_known_broken_1\",\n",
        "]\n",
        "missing_kb = [c for c in KB_WANT if c not in df.columns]\n",
        "if ST3_SRC and missing_kb:\n",
        "    df = merge_missing_cols_on_row_id(df, ST3_SRC, missing_kb)\n",
        "    print(f\"[Stage 9] Backfilled known-broken flags from: {ST3_SRC}\")\n",
        "\n",
        "# ---------- Soil Temperature: PREFER 6PD, else backfill from 6c/6b/6a ----------\n",
        "ST_FROM_6PD = [\n",
        "    \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "    \"qc_st_pairdiverge_0\",\"qc_st_pairdiverge_1\",\n",
        "    \"qc_st_pairdiverge_src_0\",\"qc_st_pairdiverge_src_1\",\n",
        "    \"qc_st_pairdiverge_nref_0\",\"qc_st_pairdiverge_nref_1\",\n",
        "    \"qc_st_pairdiverge_corr_0\",\"qc_st_pairdiverge_corr_1\",\n",
        "    \"qc_st_range_0\",\"qc_st_range_1\",\n",
        "    \"qc_st_flatline_0\",\"qc_st_flatline_1\",\n",
        "    \"qc_st_known_broken_0\",\"qc_st_known_broken_1\",\n",
        "    \"qc_st_anycrit_0\",\"qc_st_anycrit_1\",\n",
        "    \"qc_st_anywarn_0\",\"qc_st_anywarn_1\",\n",
        "]\n",
        "st_6pd_src = pick_latest_by_priority([\"stage6pd_soiltemp_pairdiverge_v*.csv\"])\n",
        "if st_6pd_src:\n",
        "    df = replace_cols_on_row_id(df, st_6pd_src, ST_FROM_6PD)\n",
        "    print(f\"[Stage 9] Replaced soil-temp clean/QC from 6PD: {st_6pd_src}\")\n",
        "else:\n",
        "    ST_QC_WANT = [\n",
        "        \"qc_st_range_0\",\"qc_st_range_1\",\n",
        "        \"qc_st_flatline_0\",\"qc_st_flatline_1\",\n",
        "        \"qc_st_step_0\",\"qc_st_step_1\",\n",
        "        \"qc_st_anycrit_0\",\"qc_st_anycrit_1\",\n",
        "        \"qc_st_anywarn_0\",\"qc_st_anywarn_1\",\n",
        "        \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "    ]\n",
        "    st_missing = [c for c in ST_QC_WANT if c not in df.columns]\n",
        "    if st_missing:\n",
        "        st_src = pick_latest_by_priority([\n",
        "            \"stage6c_soiltemp_step_v*.csv\",\n",
        "            \"stage6b_soiltemp_flatline_v*.csv\",\n",
        "            \"stage6a_soiltemp_range_v*.csv\",\n",
        "        ])\n",
        "        if st_src:\n",
        "            df = merge_missing_cols_on_row_id(df, st_src, st_missing)\n",
        "            print(f\"[Stage 9] Backfilled ST clean/QC from: {st_src}\")\n",
        "\n",
        "# ---------- ENFORCE: soil-temp anycrit to include pair-diverge & known-broken ----------\n",
        "def _as_bool(s, idx):\n",
        "    if isinstance(s, pd.Series):\n",
        "        if s.dtype == bool: return s.fillna(False)\n",
        "        return s.astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"y\",\"yes\"]).reindex(idx, fill_value=False)\n",
        "    return pd.Series(False, index=idx)\n",
        "\n",
        "for suf in (\"0\",\"1\"):\n",
        "    kbcol   = f\"qc_st_known_broken_{suf}\"\n",
        "    anycrit = f\"qc_st_anycrit_{suf}\"\n",
        "    pdv     = f\"qc_st_pairdiverge_{suf}\"\n",
        "    clean   = f\"ground_temp_{suf}_clean\"\n",
        "    raw     = f\"ground_temp_{suf}\"\n",
        "\n",
        "    if clean not in df.columns and raw in df.columns:\n",
        "        df[clean] = pd.to_numeric(df[raw], errors=\"coerce\")\n",
        "\n",
        "    kb_mask   = df[kbcol].astype(str).str.startswith(\"CRIT\", na=False) if kbcol in df.columns else pd.Series(False, index=df.index)\n",
        "    pdv_mask  = df[pdv].astype(str).str.startswith(\"CRIT\", na=False) if pdv in df.columns else pd.Series(False, index=df.index)\n",
        "    anycrit_s = _as_bool(df.get(anycrit, False), df.index) | kb_mask | pdv_mask\n",
        "    df[anycrit] = anycrit_s\n",
        "    if clean in df.columns:\n",
        "        df.loc[anycrit_s, clean] = np.nan\n",
        "\n",
        "# ---------- Backfill (only if missing) for Radiation & Wind ----------\n",
        "RW_QC_WANT = [\n",
        "    \"qc_rad_range\", \"qc_rad_anycrit\", \"qc_rad_anywarn\", f\"{RAD_COL}_clean\",\n",
        "    \"qc_wind_mean_range\", \"qc_wind_mean_anycrit\", \"qc_wind_mean_anywarn\", f\"{W_COLS[1]}_clean\",\n",
        "    \"qc_wind_min_range\",  \"qc_wind_min_anycrit\",  \"qc_wind_min_anywarn\",  f\"{W_COLS[0]}_clean\",\n",
        "    \"qc_wind_max_range\",  \"qc_wind_max_anycrit\",  \"qc_wind_max_anywarn\",  f\"{W_COLS[2]}_clean\",\n",
        "    \"qc_wind_order\",\n",
        "]\n",
        "rw_missing = [c for c in RW_QC_WANT if c not in df.columns]\n",
        "if rw_missing:\n",
        "    rw_src = pick_latest_by_priority([\"stage8_rad_wind_range_v*.csv\"])\n",
        "    if rw_src:\n",
        "        df = merge_missing_cols_on_row_id(df, rw_src, rw_missing)\n",
        "        print(f\"[Stage 9] Backfilled RAD/WIND clean/QC from: {rw_src}\")\n",
        "for c in [\"qc_rad_range\",\"qc_wind_mean_range\",\"qc_wind_min_range\",\"qc_wind_max_range\",\"qc_wind_order\"]:\n",
        "    if c not in df.columns:\n",
        "        df[c] = \"OK\"\n",
        "\n",
        "# ---------- AIR: bring QC + existing __pref + reason from Step 7 ----------\n",
        "AIR_QC_WANT   = [\"qc_air_selfcheck_temp\",\"qc_air_selfcheck_hum\",\"qc_air_crosscheck_pres\"]\n",
        "AIR_PREF_WANT = [\"temperature__pref\",\"humidity__pref\",\"pressure__pref\",\"humidity_mask_reason\"]\n",
        "\n",
        "for pat in [\"stage7c_air_pres_qc_v*.csv\", \"stage7b_air_hum_qc_v*.csv\", \"stage7a_air_temp_qc_v*.csv\"]:\n",
        "    air_src = pick_latest_by_priority([pat])\n",
        "    if air_src:\n",
        "        # QC fields: add if missing\n",
        "        df = merge_missing_cols_on_row_id(df, air_src, AIR_QC_WANT)\n",
        "        # __pref + reason: ALWAYS REPLACE from Step 7 artifacts\n",
        "        df = replace_cols_on_row_id(df, air_src, AIR_PREF_WANT)\n",
        "\n",
        "# --- Build preferred series for SM/ST/RAD/W from *_clean when present ---\n",
        "def preferred(df_: pd.DataFrame, raw: str) -> pd.Series:\n",
        "    c = f\"{raw}_clean\"\n",
        "    return pd.to_numeric(df_[c], errors=\"coerce\") if c in df_.columns else pd.to_numeric(df_[raw], errors=\"coerce\")\n",
        "\n",
        "pref = {}\n",
        "for c in SM_COLS + ST_COLS:\n",
        "    if c in df.columns:\n",
        "        pref[c] = preferred(df, c)\n",
        "if RAD_COL in df.columns:\n",
        "    pref[RAD_COL] = preferred(df, RAD_COL)\n",
        "for c in W_COLS:\n",
        "    if c in df.columns:\n",
        "        pref[c] = preferred(df, c)\n",
        "for name, series in pref.items():\n",
        "    df[f\"{name}__pref\"] = series\n",
        "\n",
        "# --- Build preferred series for AIR (preserve Step 7 values; humidity lenient fallback) ---\n",
        "for base, qc in [\n",
        "    (\"temperature\", \"qc_air_selfcheck_temp\"),\n",
        "    (\"humidity\",    \"qc_air_selfcheck_hum\"),\n",
        "    (\"pressure\",    \"qc_air_crosscheck_pres\"),\n",
        "]:\n",
        "    if base not in df.columns:\n",
        "        continue\n",
        "    pref_col = f\"{base}__pref\"\n",
        "\n",
        "    if pref_col in df.columns:\n",
        "        df[pref_col] = pd.to_numeric(df[pref_col], errors=\"coerce\")  # keep Step 7 result\n",
        "    else:\n",
        "        raw = pd.to_numeric(df[base], errors=\"coerce\")\n",
        "        if base == \"humidity\":\n",
        "            # LENIENT policy: mask if CRIT or reason != \"\"\n",
        "            qc_series = df.get(qc, pd.Series(\"\", index=df.index)).astype(str)\n",
        "            crit = qc_series.str.startswith(\"CRIT\", na=False)\n",
        "            reason = df.get(\"humidity_mask_reason\", pd.Series(\"\", index=df.index)).astype(str)\n",
        "            masked_needed = crit | (reason != \"\")\n",
        "            df[pref_col] = raw.mask(masked_needed)\n",
        "        else:\n",
        "            # Temperature & pressure: mask WARN/CRIT\n",
        "            if qc in df.columns:\n",
        "                bad = df[qc].astype(str).str.startswith((\"WARN\",\"CRIT\"), na=False)\n",
        "                df[pref_col] = raw.mask(bad)\n",
        "            else:\n",
        "                df[pref_col] = raw\n",
        "\n",
        "    df[f\"ok_{base}\"] = df[pref_col].notna()\n",
        "\n",
        "\n",
        "# ---- Enforce lenient humidity (CRIT or reason != \"\" -> mask) ----\n",
        "need = {\"humidity\",\"humidity__pref\",\"qc_air_selfcheck_hum\",\"humidity_mask_reason\"}\n",
        "if need.issubset(df.columns):\n",
        "    _raw    = pd.to_numeric(df[\"humidity\"], errors=\"coerce\")\n",
        "    _qc     = df[\"qc_air_selfcheck_hum\"].astype(str)\n",
        "    # FIX: treat missing reason as empty, and strip whitespace\n",
        "    _reason = df[\"humidity_mask_reason\"].fillna(\"\").astype(str).str.strip()\n",
        "    _need   = _qc.str.startswith(\"CRIT\", na=False) | (_reason != \"\")\n",
        "    df[\"humidity__pref\"] = _raw.mask(_need)\n",
        "    df[\"ok_humidity\"]    = df[\"humidity__pref\"].notna()\n",
        "\n",
        "\n",
        "def anycrit_bool(df_: pd.DataFrame, col: str) -> pd.Series:\n",
        "    if col in df_.columns:\n",
        "        s = df_[col]\n",
        "        return s if s.dtype == bool else s.astype(str).str.startswith(\"CRIT\", na=False)\n",
        "    return pd.Series(False, index=df_.index, dtype=bool)\n",
        "\n",
        "# ok_* for SM/ST from anycrit (includes 6PD via enforcement above)\n",
        "df[\"ok_sm0\"] = ~anycrit_bool(df, \"qc_sm_anycrit_0\") if \"qc_sm_anycrit_0\" in df.columns else False\n",
        "df[\"ok_sm1\"] = ~anycrit_bool(df, \"qc_sm_anycrit_1\") if \"qc_sm_anycrit_1\" in df.columns else False\n",
        "df[\"ok_st0\"] = ~anycrit_bool(df, \"qc_st_anycrit_0\") if \"qc_st_anycrit_0\" in df.columns else False\n",
        "df[\"ok_st1\"] = ~anycrit_bool(df, \"qc_st_anycrit_1\") if \"qc_st_anycrit_1\" in df.columns else False\n",
        "\n",
        "# Radiation & wind OK from explicit range flags (CRIT only masks)\n",
        "if \"qc_rad_range\" in df.columns:\n",
        "    df[\"ok_rad\"]   = ~df[\"qc_rad_range\"].astype(str).eq(\"CRIT_out_of_range\")\n",
        "for w,label in [(\"min\",\"ok_wmin\"),(\"mean\",\"ok_wmean\"),(\"max\",\"ok_wmax\")]:\n",
        "    qc = f\"qc_wind_{w}_range\"\n",
        "    if qc in df.columns:\n",
        "        df[label] = ~df[qc].astype(str).eq(\"CRIT_out_of_range\")\n",
        "\n",
        "# Row-level air rollups (retain for continuity)\n",
        "def _startswith(col, prefix):\n",
        "    return df[col].astype(str).str.startswith(prefix, na=False) if col in df.columns else pd.Series(False, index=df.index)\n",
        "\n",
        "air_warn = (_startswith(\"qc_air_selfcheck_temp\",\"WARN\") |\n",
        "            _startswith(\"qc_air_selfcheck_hum\",\"WARN\")  |\n",
        "            _startswith(\"qc_air_crosscheck_pres\",\"WARN\"))\n",
        "air_crit = (_startswith(\"qc_air_selfcheck_temp\",\"CRIT\") |\n",
        "            _startswith(\"qc_air_selfcheck_hum\",\"CRIT\")  |\n",
        "            _startswith(\"qc_air_crosscheck_pres\",\"CRIT\"))\n",
        "df[\"qc_air_anywarn\"] = (anycrit_bool(df, \"qc_air_anywarn\") | air_warn).astype(bool)\n",
        "df[\"qc_air_anycrit\"] = (anycrit_bool(df, \"qc_air_anycrit\") | air_crit).astype(bool)\n",
        "df[\"row_has_warn\"] = (anycrit_bool(df, \"row_has_warn\") | df[\"qc_air_anywarn\"]).astype(bool)\n",
        "df[\"row_has_crit\"] = (anycrit_bool(df, \"row_has_crit\") | df[\"qc_air_anycrit\"]).astype(bool)\n",
        "\n",
        "# =============== Choose & order columns for export ===============\n",
        "id_cols   = [c for c in [ROW_ID_COL, NODE_ID_COL, NODE_COL, STATION_TYPE] if c in df.columns]\n",
        "time_cols = [c for c in [TS_NS_COL, TS_ISO_COL, TIMESTAMP_COL] if c in df.columns]\n",
        "\n",
        "pref_cols = (\n",
        "    [f\"{c}__pref\" for c in SM_COLS if f\"{c}__pref\" in df.columns] +\n",
        "    [f\"{c}__pref\" for c in ST_COLS if f\"{c}__pref\" in df.columns] +\n",
        "    ([f\"{RAD_COL}__pref\"] if f\"{RAD_COL}__pref\" in df.columns else []) +\n",
        "    [f\"{c}__pref\" for c in W_COLS if f\"{c}__pref\" in df.columns] +\n",
        "    [f\"{c}__pref\" for c in AIR_COLS if f\"{c}__pref\" in df.columns]\n",
        ")\n",
        "\n",
        "ok_cols = [c for c in [\n",
        "    \"ok_sm0\",\"ok_sm1\",\"ok_st0\",\"ok_st1\",\n",
        "    \"ok_rad\",\"ok_wmin\",\"ok_wmean\",\"ok_wmax\",\n",
        "    \"ok_temperature\",\"ok_humidity\",\"ok_pressure\",\n",
        "] if c in df.columns]\n",
        "\n",
        "# Keep *_clean for SM/ST/RAD/W so plotting can compare raw vs clean directly\n",
        "CLEAN_KEEP = [\n",
        "    \"ground_humidity_0_clean\",\"ground_humidity_1_clean\",\n",
        "    \"ground_temp_0_clean\",\"ground_temp_1_clean\",\n",
        "    \"global_radiation_clean\",\n",
        "    \"wind_speed_min_clean\",\"wind_speed_mean_clean\",\"wind_speed_max_clean\",\n",
        "]\n",
        "clean_present = [c for c in CLEAN_KEEP if c in df.columns]\n",
        "\n",
        "# Keep air mask reason if present (from 7b lenient)\n",
        "AIR_META_KEEP = [c for c in [\"humidity_mask_reason\"] if c in df.columns]\n",
        "\n",
        "# all QC flags\n",
        "qc_cols = [c for c in df.columns if c.startswith(\"qc_\")]\n",
        "\n",
        "# raw columns for traceability (only those that exist)\n",
        "raw_cols = [c for c in { *SM_COLS, *ST_COLS, RAD_COL, *W_COLS, *AIR_COLS } if c in df.columns]\n",
        "\n",
        "export_cols = id_cols + time_cols + pref_cols + ok_cols + qc_cols + clean_present + AIR_META_KEEP\n",
        "raw_cols = [c for c in raw_cols if c not in export_cols]\n",
        "export_cols += raw_cols\n",
        "\n",
        "# Deterministic order & write\n",
        "node_sort = NODE_COL if NODE_COL in df.columns else (NODE_ID_COL if NODE_ID_COL in df.columns else None)\n",
        "sort_keys = ([node_sort] if node_sort else []) + [TS_NS_COL, ROW_ID_COL]\n",
        "sort_keys = [k for k in sort_keys if k in df.columns]\n",
        "df.sort_values(sort_keys, inplace=True, ignore_index=True)\n",
        "out_df = df[export_cols].copy()\n",
        "\n",
        "# Clean object columns for CSV\n",
        "obj_cols = out_df.select_dtypes(include=\"object\").columns\n",
        "if len(obj_cols):\n",
        "    out_df[obj_cols] = out_df[obj_cols].replace({r'[\\r\\n]': ' ', '\"': '\"\"'}, regex=True)\n",
        "\n",
        "out_main = next_versioned_csv(OUT_DIR, \"analysis_ready\")\n",
        "\n",
        "# First attempt: minimal quoting\n",
        "out_df.to_csv(out_main, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
        "\n",
        "# Verify; if mismatch, rewrite with QUOTE_ALL\n",
        "_ok = True\n",
        "try:\n",
        "    _test = pd.read_csv(out_main, engine=\"c\", low_memory=False)\n",
        "    if list(_test.columns) != list(out_df.columns):\n",
        "        _ok = False\n",
        "except Exception as e:\n",
        "    print(\"[Step 9] Re-read failed:\", e)\n",
        "    _ok = False\n",
        "\n",
        "if not _ok:\n",
        "    print(\"[Step 9] Rewriting with QUOTE_ALL for robustness…\")\n",
        "    out_df.to_csv(out_main, index=False, encoding=\"utf-8\",\n",
        "                  quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n",
        "    _test = pd.read_csv(out_main, engine=\"python\", low_memory=False)\n",
        "    assert list(_test.columns) == list(out_df.columns), \"Post-rewrite verification failed.\"\n",
        "\n",
        "print(\"Wrote analysis-ready file:\", out_main)\n",
        "\n",
        "# Optional: Parquet copy\n",
        "try:\n",
        "    out_parq = Path(str(out_main)).with_suffix(\".parquet\")\n",
        "    out_df.to_parquet(out_parq, index=False)\n",
        "    print(\"Wrote Parquet copy:\", out_parq)\n",
        "except Exception as e:\n",
        "    print(\"Parquet write skipped:\", e)\n",
        "\n",
        "# ---- Small console summary\n",
        "print(\"\\nPreview of columns included:\")\n",
        "print(export_cols[:30], \"...\")\n",
        "print(f\"Total columns exported: {len(export_cols)}\")\n",
        "tmin = pd.to_datetime(pd.to_numeric(df[TS_NS_COL], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\").min()\n",
        "tmax = pd.to_datetime(pd.to_numeric(df[TS_NS_COL], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\").max()\n",
        "print(\"Time span (from ts_ns):\", tmin, \"→\", tmax)\n",
        "print(\"Rows in/out (should match input):\", n_in, \"→\", len(df))\n",
        "\n",
        "# ---- Guardrail: lenient humidity must hold if columns exist\n",
        "need = {\"humidity\",\"humidity__pref\",\"qc_air_selfcheck_hum\",\"humidity_mask_reason\"}\n",
        "if need.issubset(df.columns):\n",
        "    _raw    = pd.to_numeric(df[\"humidity\"], errors=\"coerce\")\n",
        "    _pref   = pd.to_numeric(df[\"humidity__pref\"], errors=\"coerce\")\n",
        "    _qc     = df[\"qc_air_selfcheck_hum\"].astype(str)\n",
        "    # FIX here too\n",
        "    _reason = df[\"humidity_mask_reason\"].fillna(\"\").astype(str).str.strip()\n",
        "    _need   = _qc.str.startswith(\"CRIT\", na=False) | (_reason != \"\")\n",
        "    viol = int(((_need & _raw.notna()) & _pref.notna()).sum())\n",
        "    assert viol == 0, f\"lenient humidity violated for {viol} rows\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rJBU0M91Jox",
        "outputId": "b410aa01-bbf5-428a-87b4-3e14452c63cc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pick] stage8_rad_wind_range_v*.csv -> af_clean_v01/stage8_rad_wind_range_v001.csv\n",
            "Stage 9 INPUT: af_clean_v01/stage8_rad_wind_range_v001.csv\n",
            "[pick] stage5pd_sm_pairdiverge_v*.csv -> af_clean_v01/stage5pd_sm_pairdiverge_v001.csv\n",
            "[Stage 9] Replaced soil-moisture clean/QC from 5PD: af_clean_v01/stage5pd_sm_pairdiverge_v001.csv\n",
            "[pick] stage3_sm_knownbroken_v*.csv -> af_clean_v01/stage3_sm_knownbroken_v001.csv\n",
            "[pick] stage6pd_soiltemp_pairdiverge_v*.csv -> af_clean_v01/stage6pd_soiltemp_pairdiverge_v008.csv\n",
            "[Stage 9] Replaced soil-temp clean/QC from 6PD: af_clean_v01/stage6pd_soiltemp_pairdiverge_v008.csv\n",
            "[pick] stage7c_air_pres_qc_v*.csv -> (no match)\n",
            "[pick] stage7b_air_hum_qc_v*.csv -> af_clean_v01/stage7b_air_hum_qc_v001.csv\n",
            "[pick] stage7a_air_temp_qc_v*.csv -> af_clean_v01/stage7a_air_temp_qc_v001.csv\n",
            "Wrote analysis-ready file: af_clean_v01/analysis_ready_v006.csv\n",
            "Wrote Parquet copy: af_clean_v01/analysis_ready_v006.parquet\n",
            "\n",
            "Preview of columns included:\n",
            "['row_id', 'node_id', 'node', 'station_type', 'ts_ns', 'timestamp_iso', 'timestamp', 'ground_humidity_0__pref', 'ground_humidity_1__pref', 'ground_temp_0__pref', 'ground_temp_1__pref', 'global_radiation__pref', 'wind_speed_min__pref', 'wind_speed_mean__pref', 'wind_speed_max__pref', 'temperature__pref', 'humidity__pref', 'pressure__pref', 'ok_sm0', 'ok_sm1', 'ok_st0', 'ok_st1', 'ok_rad', 'ok_wmin', 'ok_wmean', 'ok_wmax', 'ok_temperature', 'ok_humidity', 'ok_pressure', 'qc_air_selfcheck_temp'] ...\n",
            "Total columns exported: 105\n",
            "Time span (from ts_ns): 2023-10-30 12:18:43+00:00 → 2025-09-18 10:29:42+00:00\n",
            "Rows in/out (should match input): 483787 → 483787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Analysis-Ready (Step 9) — Sanity Audit v2 ===\n",
        "# Verifies *_clean / __pref vs QC rules, lenient humidity, ok_* consistency.\n",
        "import pandas as pd, numpy as np, glob, os, re\n",
        "\n",
        "# --- locate latest AR ---\n",
        "ARs = sorted(glob.glob(\"af_clean_v01/analysis_ready_v*.csv\"),\n",
        "             key=lambda p: int(re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(p)).group(1)) if re.search(r\"_v(\\d+)\\.csv$\", os.path.basename(p)) else 0)\n",
        "if not ARs:\n",
        "    ARs = sorted(glob.glob(\"analysis_ready_v*.csv\"))\n",
        "assert ARs, \"No analysis_ready_v*.csv found.\"\n",
        "AR = ARs[-1]\n",
        "df = pd.read_csv(AR, low_memory=False)\n",
        "print(f\"[AR] {AR}  rows={len(df):,}\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def pct(a, b):\n",
        "    b = int(b) or 1\n",
        "    return f\"{(100.0*int(a)/b):5.2f}%\"\n",
        "\n",
        "def pbool(s):\n",
        "    if isinstance(s, pd.Series):\n",
        "        if s.dtype == bool: return s.fillna(False)\n",
        "        return s.astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"y\",\"yes\"])\n",
        "    return pd.Series(False, index=range(len(df)))\n",
        "\n",
        "def passfail(name, ok, extra=\"\"):\n",
        "    print(f\"[{name:<28}] {'PASS' if ok else 'FAIL'} {extra}\")\n",
        "\n",
        "def exists(*cols):\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    return (len(missing) == 0, missing)\n",
        "\n",
        "# node/time basics\n",
        "node_col = \"node\" if \"node\" in df.columns else (\"node_id\" if \"node_id\" in df.columns else None)\n",
        "\n",
        "# --- core columns present? ---\n",
        "core_ok, core_missing = exists(\"row_id\",\"ts_ns\",\"timestamp_iso\",\"timestamp\",\"station_type\")\n",
        "passfail(\"core columns\", core_ok, f\"missing={core_missing}\" if core_missing else \"\")\n",
        "\n",
        "# --- row_id uniqueness ---\n",
        "passfail(\"row_id unique\", df[\"row_id\"].is_unique)\n",
        "\n",
        "# --- time monotonic within node (or globally if no node col) ---\n",
        "t = pd.to_datetime(pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\").astype(\"Int64\"), utc=True, errors=\"coerce\")\n",
        "if node_col:\n",
        "    bad_nodes = []\n",
        "    for n, g in df.assign(__t=t).groupby(df[node_col].astype(str), sort=False):\n",
        "        if not g[\"__t\"].is_monotonic_increasing:\n",
        "            bad_nodes.append(n)\n",
        "    passfail(\"time monotonic by node\", len(bad_nodes)==0, f\"nonmonotonic_nodes={bad_nodes[:5]}{'...' if len(bad_nodes)>5 else ''}\")\n",
        "else:\n",
        "    passfail(\"time monotonic\", t.is_monotonic_increasing)\n",
        "\n",
        "# ---------------- SM & ST consistency (anycrit → clean is NaN; clean NaN→anycrit) ----------------\n",
        "def check_probe(raw, clean, anycrit, label):\n",
        "    if not all(c in df.columns for c in [raw, clean, anycrit]):\n",
        "        passfail(label, True, \"skipped (cols missing)\")\n",
        "        return\n",
        "    r = pd.to_numeric(df[raw], errors=\"coerce\")\n",
        "    c = pd.to_numeric(df[clean], errors=\"coerce\")\n",
        "    ac = pbool(df[anycrit])\n",
        "    fail1 = int(((ac & r.notna()) & c.notna()).sum())     # anycrit but clean not NaN\n",
        "    fail2 = int(((r.notna() & c.isna()) & ~ac).sum())     # clean NaN but not anycrit\n",
        "    masked_share = pct((r.notna() & c.isna()).sum(), r.notna().sum())\n",
        "    passfail(label, (fail1==0 and fail2==0), f\"masked={masked_share}  f1={fail1} f2={fail2}\")\n",
        "\n",
        "for suf in (\"0\",\"1\"):\n",
        "    check_probe(f\"ground_humidity_{suf}\", f\"ground_humidity_{suf}_clean\", f\"qc_sm_anycrit_{suf}\", f\"SM{suf} anycrit→NaN\")\n",
        "    check_probe(f\"ground_temp_{suf}\",     f\"ground_temp_{suf}_clean\",     f\"qc_st_anycrit_{suf}\", f\"ST{suf} anycrit→NaN\")\n",
        "\n",
        "# ---------------- Radiation/Wind range rules ----------------\n",
        "# WARN → clamped to boundary; CRIT → NaN\n",
        "RAD_LOWER_OK, RAD_UPPER_OK = 0.0, 1200.0\n",
        "W_LOWER_OK,   W_UPPER_OK   = 0.0, 100.0\n",
        "\n",
        "def check_range(raw, clean, qc, low_ok, hi_ok, label):\n",
        "    if not all(c in df.columns for c in [raw, clean, qc]):\n",
        "        passfail(f\"{label} range qc\", True, \"skipped (cols missing)\")\n",
        "        return\n",
        "    v  = pd.to_numeric(df[raw], errors=\"coerce\")\n",
        "    cl = pd.to_numeric(df[clean], errors=\"coerce\")\n",
        "    q  = df[qc].astype(str)\n",
        "\n",
        "    crit = q.eq(\"CRIT_out_of_range\") & v.notna()\n",
        "    f1   = int((crit & cl.notna()).sum())  # CRIT must be NaN\n",
        "\n",
        "    warn = q.eq(\"WARN_boundary\") & v.notna() & cl.notna()\n",
        "    f2   = int((warn & ~(np.isclose(cl, low_ok) | np.isclose(cl, hi_ok))).sum())  # WARN must clamp\n",
        "\n",
        "    ok   = q.eq(\"OK\") & v.notna()\n",
        "    f3   = int((ok & cl.isna()).sum())  # OK should not be NaN if raw present (soft)\n",
        "\n",
        "    masked_share = pct((v.notna() & cl.isna()).sum(), v.notna().sum())\n",
        "    passfail(f\"{label} range qc\", (f1==0 and f2==0), f\"masked={masked_share}  crit→not-NaN={f1}  warn→not-clamped={f2}  ok→NaN={f3}\")\n",
        "\n",
        "check_range(\"global_radiation\",\"global_radiation_clean\",\"qc_rad_range\",\n",
        "            RAD_LOWER_OK, RAD_UPPER_OK, \"Radiation\")\n",
        "check_range(\"wind_speed_min\",\"wind_speed_min_clean\",\"qc_wind_min_range\",\n",
        "            W_LOWER_OK, W_UPPER_OK, \"Wind min\")\n",
        "check_range(\"wind_speed_mean\",\"wind_speed_mean_clean\",\"qc_wind_mean_range\",\n",
        "            W_LOWER_OK, W_UPPER_OK, \"Wind mean\")\n",
        "check_range(\"wind_speed_max\",\"wind_speed_max_clean\",\"qc_wind_max_range\",\n",
        "            W_LOWER_OK, W_UPPER_OK, \"Wind max\")\n",
        "\n",
        "# ---------------- Air variables ----------------\n",
        "def check_air_mask_temp_pressure(base, qc_col):\n",
        "    label = f\"{base} mask(WARN/CRIT)\"\n",
        "    if not all(c in df.columns for c in [base, qc_col]):\n",
        "        passfail(label, True, \"skipped (cols missing)\"); return\n",
        "    pref_col = f\"{base}__pref\"\n",
        "    if pref_col not in df.columns:\n",
        "        passfail(label, True, \"skipped (pref missing)\"); return\n",
        "    raw  = pd.to_numeric(df[base], errors=\"coerce\")\n",
        "    pref = pd.to_numeric(df[pref_col], errors=\"coerce\")\n",
        "    qc   = df[qc_col].astype(str)\n",
        "    bad  = qc.str.startswith((\"WARN\",\"CRIT\"), na=False)\n",
        "    f1 = int(((bad & raw.notna()) & pref.notna()).sum())      # should be masked but isn't\n",
        "    f2 = int(((raw.notna() & pref.isna()) & ~bad).sum())      # masked though QC is OK\n",
        "    passfail(label, f1==0 and f2==0, f\"viol_warncrit={f1}  masked_but_ok={f2}\")\n",
        "\n",
        "def check_air_mask_humidity_lenient():\n",
        "    label = \"humidity lenient mask\"\n",
        "    need = [\"humidity\",\"humidity__pref\",\"qc_air_selfcheck_hum\",\"humidity_mask_reason\"]\n",
        "    if not all(c in df.columns for c in need):\n",
        "        passfail(label, True, \"skipped (cols missing)\"); return\n",
        "    raw    = pd.to_numeric(df[\"humidity\"], errors=\"coerce\")\n",
        "    pref   = pd.to_numeric(df[\"humidity__pref\"], errors=\"coerce\")\n",
        "    qc     = df[\"qc_air_selfcheck_hum\"].astype(str)\n",
        "    reason = df[\"humidity_mask_reason\"].fillna(\"\").astype(str).str.strip()\n",
        "    crit = qc.str.startswith(\"CRIT\", na=False)\n",
        "    masked_needed = crit | (reason != \"\")\n",
        "    viol_needed = int(((masked_needed & raw.notna()) & pref.notna()).sum())\n",
        "    masked_but_not_needed = int(((raw.notna() & pref.isna()) & ~masked_needed).sum())\n",
        "    passfail(label, viol_needed==0 and masked_but_not_needed==0,\n",
        "             f\"viol_needed={viol_needed}  masked_but_not_needed={masked_but_not_needed}\")\n",
        "\n",
        "check_air_mask_temp_pressure(\"temperature\", \"qc_air_selfcheck_temp\")\n",
        "# pressure QC might not exist yet; safe skip if missing\n",
        "if \"pressure\" in df.columns:\n",
        "    check_air_mask_temp_pressure(\"pressure\", \"qc_air_crosscheck_pres\")\n",
        "check_air_mask_humidity_lenient()\n",
        "\n",
        "# ---------------- ok_* booleans ----------------\n",
        "def same_bool(a, b):\n",
        "    A = pbool(a) if not (isinstance(a, pd.Series) and a.dtype == bool) else a\n",
        "    B = pbool(b) if not (isinstance(b, pd.Series) and b.dtype == bool) else b\n",
        "    return (A.fillna(False) == B.fillna(False))\n",
        "\n",
        "def expect_ok_flag(flag_col, expect_series, label):\n",
        "    if flag_col not in df.columns:\n",
        "        passfail(label, True, \"skipped (flag missing)\"); return\n",
        "    ok = pbool(df[flag_col]) if df[flag_col].dtype != bool else df[flag_col]\n",
        "    cmp = same_bool(ok, expect_series)\n",
        "    bad = int((~cmp).sum())\n",
        "    passfail(label, bad==0, f\"mismatches={bad}\")\n",
        "\n",
        "# SM/ST ok_* (should be ~ !anycrit)\n",
        "for suf, flag in [(\"0\",\"ok_sm0\"),(\"1\",\"ok_sm1\")]:\n",
        "    if f\"qc_sm_anycrit_{suf}\" in df.columns and flag in df.columns:\n",
        "        expect_ok_flag(flag, ~pbool(df[f\"qc_sm_anycrit_{suf}\"]), f\"{flag} ~ !anycrit\")\n",
        "for suf, flag in [(\"0\",\"ok_st0\"),(\"1\",\"ok_st1\")]:\n",
        "    if f\"qc_st_anycrit_{suf}\" in df.columns and flag in df.columns:\n",
        "        expect_ok_flag(flag, ~pbool(df[f\"qc_st_anycrit_{suf}\"]), f\"{flag} ~ !anycrit\")\n",
        "\n",
        "# Radiation & wind ok_* from CRIT status\n",
        "if all(c in df.columns for c in [\"qc_rad_range\",\"ok_rad\"]):\n",
        "    expect_ok_flag(\"ok_rad\",  ~df[\"qc_rad_range\"].astype(str).eq(\"CRIT_out_of_range\"), \"ok_rad\")\n",
        "for w,label in [(\"min\",\"ok_wmin\"),(\"mean\",\"ok_wmean\"),(\"max\",\"ok_wmax\")]:\n",
        "    qc = f\"qc_wind_{w}_range\"\n",
        "    if all(c in df.columns for c in [qc,label]):\n",
        "        expect_ok_flag(label, ~df[qc].astype(str).eq(\"CRIT_out_of_range\"), label)\n",
        "\n",
        "# Air ok_* from __pref notna\n",
        "for base in [\"temperature\",\"humidity\",\"pressure\"]:\n",
        "    flag = f\"ok_{base}\"; pref = f\"{base}__pref\"\n",
        "    if all(c in df.columns for c in [flag,pref]):\n",
        "        expect_ok_flag(flag, df[pref].notna(), f\"{flag} == pref.notna()\")\n",
        "\n",
        "# -------------- masked-share summary --------------\n",
        "def masked_share(series_raw, series_pref):\n",
        "    r = pd.to_numeric(series_raw, errors=\"coerce\")\n",
        "    p = pd.to_numeric(series_pref, errors=\"coerce\")\n",
        "    den = int(r.notna().sum()) or 1\n",
        "    return (int((r.notna() & p.isna()).sum()), den, pct((r.notna() & p.isna()).sum(), den))\n",
        "\n",
        "rows = []\n",
        "pairs = [\n",
        "    (\"ground_humidity_0\",\"ground_humidity_0__pref\"),\n",
        "    (\"ground_humidity_1\",\"ground_humidity_1__pref\"),\n",
        "    (\"ground_temp_0\",\"ground_temp_0__pref\"),\n",
        "    (\"ground_temp_1\",\"ground_temp_1__pref\"),\n",
        "    (\"global_radiation\",\"global_radiation__pref\"),\n",
        "    (\"wind_speed_min\",\"wind_speed_min__pref\"),\n",
        "    (\"wind_speed_mean\",\"wind_speed_mean__pref\"),\n",
        "    (\"wind_speed_max\",\"wind_speed_max__pref\"),\n",
        "    (\"temperature\",\"temperature__pref\"),\n",
        "    (\"humidity\",\"humidity__pref\"),\n",
        "    (\"pressure\",\"pressure__pref\"),\n",
        "]\n",
        "for raw, pref in pairs:\n",
        "    if raw in df.columns and pref in df.columns:\n",
        "        nmask, den, shar = masked_share(df[raw], df[pref])\n",
        "        rows.append((pref, nmask, den, shar))\n",
        "\n",
        "if rows:\n",
        "    print(\"\\nMasked share (raw present → __pref NaN):\")\n",
        "    for name, nmask, den, share in rows:\n",
        "        print(f\"  {name:28s} {nmask:8d}/{den:8d} = {share}\")\n",
        "\n",
        "print(\"\\nSanity audit complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEuQEkwr1A4z",
        "outputId": "9ca8989d-059f-4c28-b192-a613aa6d8303"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AR] af_clean_v01/analysis_ready_v006.csv  rows=483,787\n",
            "[core columns                ] PASS \n",
            "[row_id unique               ] PASS \n",
            "[time monotonic by node      ] PASS nonmonotonic_nodes=[]\n",
            "[SM0 anycrit→NaN             ] PASS masked= 7.74%  f1=0 f2=0\n",
            "[ST0 anycrit→NaN             ] PASS masked=10.32%  f1=0 f2=0\n",
            "[SM1 anycrit→NaN             ] PASS masked= 4.66%  f1=0 f2=0\n",
            "[ST1 anycrit→NaN             ] PASS masked= 7.26%  f1=0 f2=0\n",
            "[Radiation range qc          ] PASS masked= 0.18%  crit→not-NaN=0  warn→not-clamped=0  ok→NaN=0\n",
            "[Wind min range qc           ] PASS masked= 0.00%  crit→not-NaN=0  warn→not-clamped=0  ok→NaN=0\n",
            "[Wind mean range qc          ] PASS masked= 0.00%  crit→not-NaN=0  warn→not-clamped=0  ok→NaN=0\n",
            "[Wind max range qc           ] PASS masked= 0.00%  crit→not-NaN=0  warn→not-clamped=0  ok→NaN=0\n",
            "[temperature mask(WARN/CRIT) ] PASS viol_warncrit=0  masked_but_ok=0\n",
            "[pressure mask(WARN/CRIT)    ] PASS skipped (cols missing)\n",
            "[humidity lenient mask       ] PASS viol_needed=0  masked_but_not_needed=0\n",
            "[ok_sm0 ~ !anycrit           ] PASS mismatches=0\n",
            "[ok_sm1 ~ !anycrit           ] PASS mismatches=0\n",
            "[ok_st0 ~ !anycrit           ] PASS mismatches=0\n",
            "[ok_st1 ~ !anycrit           ] PASS mismatches=0\n",
            "[ok_rad                      ] PASS mismatches=0\n",
            "[ok_wmin                     ] PASS mismatches=0\n",
            "[ok_wmean                    ] PASS mismatches=0\n",
            "[ok_wmax                     ] PASS mismatches=0\n",
            "[ok_temperature == pref.notna()] PASS mismatches=0\n",
            "[ok_humidity == pref.notna() ] PASS mismatches=0\n",
            "[ok_pressure == pref.notna() ] PASS mismatches=0\n",
            "\n",
            "Masked share (raw present → __pref NaN):\n",
            "  ground_humidity_0__pref         37434/  483787 =  7.74%\n",
            "  ground_humidity_1__pref         22545/  483787 =  4.66%\n",
            "  ground_temp_0__pref             49924/  483787 = 10.32%\n",
            "  ground_temp_1__pref             35125/  483787 =  7.26%\n",
            "  global_radiation__pref            873/  483787 =  0.18%\n",
            "  wind_speed_min__pref                0/  483787 =  0.00%\n",
            "  wind_speed_mean__pref               0/  483787 =  0.00%\n",
            "  wind_speed_max__pref                0/  483787 =  0.00%\n",
            "  temperature__pref               14629/  483787 =  3.02%\n",
            "  humidity__pref                 285345/  483776 = 58.98%\n",
            "  pressure__pref                      0/  483787 =  0.00%\n",
            "\n",
            "Sanity audit complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "\n",
        "AR = \"af_clean_v01/analysis_ready_v006.csv\"\n",
        "df = pd.read_csv(AR, low_memory=False)\n",
        "\n",
        "# 1) Why is humidity masked? (top reasons)\n",
        "m = pd.to_numeric(df[\"humidity\"], errors=\"coerce\").notna() & pd.to_numeric(df[\"humidity__pref\"], errors=\"coerce\").isna()\n",
        "reason = df.loc[m, \"humidity_mask_reason\"].fillna(\"\").astype(str).str.strip()\n",
        "print(\"Top humidity mask reasons:\")\n",
        "print(reason.value_counts().head(10))\n",
        "\n",
        "# 2) How much of the masked humidity occurs in daylight?\n",
        "rad = pd.to_numeric(df.get(\"global_radiation__pref\", df.get(\"global_radiation\", np.nan)), errors=\"coerce\")\n",
        "day = rad > 200\n",
        "print(\"Masked-in-daylight share:\", (m & day).sum() / (m.sum() or 1))\n",
        "\n",
        "# 3) Node/site hotspots (who’s driving the masking?)\n",
        "site_col = \"station_type\" if \"station_type\" in df.columns else (\"_site\" if \"_site\" in df.columns else None)\n",
        "node_col = \"node\" if \"node\" in df.columns else (\"node_id\" if \"node_id\" in df.columns else None)\n",
        "\n",
        "if node_col:\n",
        "    node_stats = (\n",
        "        df.assign(_raw_h = pd.to_numeric(df[\"humidity\"], errors=\"coerce\"),\n",
        "                  _pref_h = pd.to_numeric(df[\"humidity__pref\"], errors=\"coerce\"))\n",
        "          .groupby(node_col, as_index=False)\n",
        "          .apply(lambda g: pd.Series({\n",
        "              \"raw_present\": int(g[\"_raw_h\"].notna().sum()),\n",
        "              \"masked\": int((g[\"_raw_h\"].notna() & g[\"_pref_h\"].isna()).sum())\n",
        "          }))\n",
        "          .assign(mask_share=lambda x: x[\"masked\"]/(x[\"raw_present\"].replace(0, np.nan)))\n",
        "          .sort_values(\"mask_share\", ascending=False)\n",
        "    )\n",
        "    print(\"\\nNodes with highest humidity mask share:\")\n",
        "    print(node_stats.head(10))\n",
        "if site_col:\n",
        "    site_stats = (\n",
        "        df.assign(_raw_h = pd.to_numeric(df[\"humidity\"], errors=\"coerce\"),\n",
        "                  _pref_h = pd.to_numeric(df[\"humidity__pref\"], errors=\"coerce\"))\n",
        "          .groupby(site_col, as_index=False)\n",
        "          .apply(lambda g: pd.Series({\n",
        "              \"raw_present\": int(g[\"_raw_h\"].notna().sum()),\n",
        "              \"masked\": int((g[\"_raw_h\"].notna() & g[\"_pref_h\"].isna()).sum())\n",
        "          }))\n",
        "          .assign(mask_share=lambda x: x[\"masked\"]/(x[\"raw_present\"].replace(0, np.nan)))\n",
        "          .sort_values(\"mask_share\", ascending=False)\n",
        "    )\n",
        "    print(\"\\nMask share by site:\")\n",
        "    print(site_stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHHyQ7fs7nxT",
        "outputId": "de70841c-1d9c-4406-8602-9f4f49f6edee"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top humidity mask reasons:\n",
            "humidity_mask_reason\n",
            "MASK_sticky_implausible      230650\n",
            "MASK_flatline_implausible     52184\n",
            "CRIT_out_of_range              2511\n",
            "Name: count, dtype: int64\n",
            "Masked-in-daylight share: 0.22028421735092607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3074316072.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Nodes with highest humidity mask share:\n",
            "       node  raw_present  masked  mask_share\n",
            "6  node_186        52294   45441    0.868952\n",
            "8  node_189        69757   52395    0.751107\n",
            "4  node_183        52831   33145    0.627378\n",
            "7  node_187        56338   35215    0.625067\n",
            "3  node_182        50424   30846    0.611733\n",
            "1  node_179        54277   32374    0.596459\n",
            "5  node_184        49095   24567    0.500397\n",
            "2  node_181        53798   24692    0.458976\n",
            "0  node_176        44962    6670    0.148347\n",
            "\n",
            "Mask share by site:\n",
            "   station_type  raw_present  masked  mask_share\n",
            "2     reference        69757   52395    0.751107\n",
            "0  agroforestry       369057  226280    0.613130\n",
            "1    open_field        44962    6670    0.148347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3074316072.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n"
          ]
        }
      ]
    }
  ]
}